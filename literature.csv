number,papers
1,"A comprehensive review of machine learning techniques on diabetes detection Diabetes mellitus has been an increasing concern owing to its high morbidity, and the average age of individual affected by of individual affected by this disease has now decreased to mid-twenties. Given the high prevalence, it is necessary to address with this problem effectively. Many researchers and doctors have now developed detection techniques based on artificial intelligence to better approach problems that are missed due to human errors. Data mining techniques with algorithms such as - density-based spatial clustering of applications with noise and ordering points to identify the cluster structure, the use of machine vision systems to learn data on facial images, gain better features for model training, and diagnosis via presentation of iridocyclitis for detection of the disease through iris patterns have been deployed by various practitioners. Machine learning classifiers such as support vector machines, logistic regression, and decision trees, have been comparative discussed various authors. Deep learning models such as artificial neural networks and recurrent neural networks have been considered, with primary focus on long short-term memory and convolutional neural network architectures in comparison with other machine learning models. Various parameters such as the root-mean-square error, mean absolute errors, area under curves, and graphs with varying criteria are commonly used. In this study, challenges pertaining to data inadequacy and model deployment are discussed. The future scope of such methods has also been discussed, and new methods are expected to enhance the performance of existing models, allowing them to attain greater insight into the conditions on which the prevalence of the disease depends.
 Although there many methods and algorithms which have been proposed in the field of machine learning or deep learning, many challenges remain, as mentioned by the authors in their works. The first concern that comes to mind when building a model is data. Refs. [32, 33, 49] were some of the authors that referenced the problem. One of the biggest problems encountered during the survey of the papers was finding articles and papers that did not relate to the popular PIMA Indian dataset. This dataset was chosen to ensure that the models provided good results because of the establishment of the dataset. The datasets are either too small or inadequate, or they lack real-time data. Small datasets pose a problem of overfitting on the model, which shows higher accuracy, but they are not able to deal with newer testing data. Hence, the model is not feasible for real-time implementation. Some authors dealt with CGM data, which was real-time, but the model training with that data was not efficient. The datasets were also selected from particular regions that are not representative of a common system. Different regions have different people and lifestyles. Generally, researchers spend 80% of their time cleaning and managing data for model training. Hence, data complexity leads to higher cost and maintenance charges. The next step is feature selection. While some authors neglected some of the features, some grouped them for feasible training. Every dataset poses the problem of having appropriate features to cater to the needs of a single algorithm. After all the data are made available, the technical stacks are finalized. Many tools are available to construct machine learning models, but choosing a model to optimize performance is also necessary. The next challenge is debugging. This becomes easy if tools such as Jupyter Notebooks are used where the code is divided into cells. This becomes difficult when the model runs on automation batch processes. In addition, as there are only a few diabetes datasets available on the Internet; more public data should be available for research. More research should be performed using heart rate, as it requires less bandwidth, and its computational complexity is also low. They can also be used in cloud or mobile devices. HR signals should also be used to detect other cardiac diseases. In some cases, authors required a time-series dataset. Since they are not available across any online resource, it is difficult to replicate such work. Such special models require extensive tuning and large datasets for both training and testing.

The next part is the construction of an actual model. To achieve perfect accuracy, many parameters must be adjusted. Random states, kernel, number of trees, hyperparameter tuning, and various others are considered while creating a model. Selecting a correct algorithm with suitable hyperparameters should also be performed precisely. Some classification models will only train on a single parameter, which results in a decreased accuracy for the model in real-time detection. It is evident from the analysis of these schemes in all classes that most of them suffer from either a single data input parameter or the feature selection is not optimal. Along with such restrictions on parameters, few classification-based schemes are purely dependent on kinds of hardware devices, which increases the difficulty of availability and adaptability of these schemes.

A healthcare-based machine learning model is only useful if it can be used for the benefit of people. Here, the model deployment in practical applications is critical. Many authors have proposed deploying models on mobile platforms. In real-life implementations, only engineers with background and experience with cloud servers and DevOps can deploy models. In this ongoing process, many issues need to be considered, such as how frequently the predictions are required to be displayed or the number of applications that are required for model processing. Although considerable precautions were taken to ensure there were no discrepancies in the study, no study could claim to be perfect and there is always scope for improvement. The development of more inquisitive study providing deeper insights into aspects that enable the predictive power of models rather than only pre-defined parameters such as accuracy, precision, F1 score, ROC, and AUC would be beneficial in the future. For classification, to distinguish between diabetic and normal profiles, clustering-based schemes provide accurate results. However, most of the clustering algorithms struggle with plug-n-play problems, which means that they usually contain human intervention during classification and analysis, which involves the possibility of error.

Considering all the above challenges, it can still be considered that they can be overcome in the future. Scholars and clinicians will continue to work toward the construction of larger and better datasets and design more efficient models and algorithms for better classification and accuracy. Any of the diseases occurring on a wide scale, such as diabetes, can be controlled through artificial intelligence techniques and automation. One can create state-of-the-art efficient models based on studies that provide early detection of diabetes and can help people to further change their lifestyle. Because deep learning performs better on most datasets, it should be combined with different algorithms to achieve better accuracy and performance. Hybrid schemes play an important role in improving the performance of the models. Through early detection, patients can be treated much earlier to avoid further risks of heart problems in cases of diabetes. Any model that can be deployed on mobile platforms should cater to the masses for their help and be representative. An implication of this survey is that ML models that have yielded efficient results that can be utilized by future researchers to further polish and improve as well as create a pipeline or an ensemble of correct and efficient models to increase the chances of predicting the disease with even more probability. Such models can be further improvised to automate the system created so that it can deal with newer data without problems."
2,"A diabetes prediction model based on Boruta feature selection and ensemble learning Background and objective
As a common chronic disease, diabetes is called the “second killer” among modern diseases. Currently, there is no medical cure for diabetes. We can only rely on medication for auxiliary treatment. However, many diabetic patients still die each year. In addition, a considerable number of people do not pay attention to their physical health or opt out of treatment due to lack of money, which eventually leads to various complications. Therefore, diagnosing diabetes at an early stage and intervening early is necessary; thus, developing an early detection method for diabetes is essential.

Methods
In this study, a diabetes prediction model based on Boruta feature selection and ensemble learning is proposed. The model contains the use of Boruta feature selection, the extraction of salient features from datasets, the use of the K-Means++_algorithm for unsupervised clustering of data and stacking of an ensemble learning method for classification. It has been validated on a diabetes dataset.

Results
The experiments were performed on the PIMA Indian diabetes dataset. The model was evaluated by accuracy, precision and F1 index. The obtained results show that the accuracy rate of the model reaches 98% and achieves good results.

Conclusion
Compared with other diabetes prediction models, this model achieved better results, and the obtained results indicate that this model is superior to other models in diabetes prediction and has better performance. Evaluation indicator 
In this study, we use six evaluation metrics to evaluate the model, including accuracy, recall, precision, F1 index, kappa coefficient and MCC coefficient. The details are shown below.

The confusion matrix is a matrix used to summarize the classification results of the classifier. It consists of true-positive TP, false-positive FP, false-negative FN and true-negative TN. The evaluation metrics of the model are calculated by TP, FP, FN and TN, which enables the performance evaluation of the model.

Accuracy, recall, precision, F1 index, Kappa coefficient and MCC coefficient. The calculation formula is shown in Eqs. 5–10. where 
 represents the sum of the number of correctly classified samples in each category divided by the total number of samples. Suppose the number of real samples in each category is 
, the number of samples in each category predicted by the model is 
, and the total number of samples is n. The formula for 
 is 
.

In order to evaluate model performance in a balanced manner and to prevent specific data from influencing the results of performance evaluation, the following experiments were all conducted using 7–3 divided data (7–3) and tenfold cross_validation (10CV).

Performance evaluation
Comparison between the same studies
In this section, the model is evaluated using the abovementioned metrics and compared with other models. The dataset uses the PIMA Indian diabetes dataset. The comparison results are shown in Tables 6 and 7. In the comparison, data for indicators not given by other researchers are replaced by ""–"". In Tables 6–12 and 14–20, the bold text indicates the experimental results of the method with the best performance under the current experimental evaluation metric. In Table 6, the proposed model has a 10–30% higher assessment metric than all other models. In Table 7, mean values were taken for comparison, with the proposed model having 0.3–23.1% higher accuracy, 0.4–28.4% higher recall, 1–26.5% higher F1 index, 2–6.5% higher kappa index, 0.3–24.7% higher precision, and 2.2–6.6% higher MCC index. Therefore, the proposed model in this paper outperforms the existing prediction models in all evaluation metrics and has better performance.

Comparison with other combinatorial classifiers
Since the combinatorial classifier stacking is used in this experiment, in this section, the model proposed in this thesis is compared with the latest published combinatorial classifier and the well-known bagging and boosting classifiers (the combinatorial classifier has been implemented using the Python language). If the researchers mention the values of their experimental parameters, the same parameter values are used. If no specific parameter values are indicated, all default values will be taken for the implementation. The specific experimental results are shown in Tables 8 and 9, and the results are analyzed and discussed separately. In Table 8, it can be seen that the boosting classifier, with the exception of the precision metric, outperforms the proposed model in all other metrics, 0.2% higher accuracy, 0.8% higher recall, 1.3% higher F1 index, 1.7% higher kappa index and 1.9% higher MCC index. Otherwise, the proposed models are better than the rest, with 0.6–10.3% higher accuracy, 0.7–13.3% higher recall, 5.5–10.7% higher F1 index, 0.6–22% higher kappa index, 1–8.7% higher precision, and MCC index 0.4–19.6% higher.

Table 9 shows that both REL and ARS classifier evaluation metrics are slightly better than the proposed model. These include 0.5% higher accuracy, 0.3–0.5% higher F1 index, 0.5–0.8% higher kappa index, 2.3% higher precision, and 0.1–1.6% higher MCC index. Otherwise, the proposed models are better than the rest, with 5.7–9.8% higher accuracy, 0.8–14.1% higher recall, 6.5–12.1% higher F1 index, 0.1–23.8% higher kappa index, and 1–8.4% higher precision, and the MCC index is 0.3–21.8% higher.

Comparison in the original dataset
In this section, the PIMA Indian diabetes dataset used is not preprocessed and includes noisy data such as missing values and extreme values. I will use the original PIMA Indian Diabetes dataset to evaluate the model proposed in this paper and to compare it with the models proposed by other researchers. The specific experimental results are shown in Tables 10 and 11. The difficulty of artificial intelligence technology in predicting whether a person is a diabetic population is how to improve the accuracy of the prediction results. In this paper, we propose a new method for predicting diabetes based on Boruta feature selection and ensemble learning, which mainly consists of extracting relevant features of the dataset using the Boruta feature selection algorithm, discovering some potential K patterns in the data using the K-Means++_algorithm, supervising the classification of the data using stacking, and optimizing the parameters using grid search to find the optimal values of the parameters. We used the PIMA Indian diabetes dataset for our experiments and achieved 98% accuracy using tenfold cross validation. In addition, comparing this model with other models, this model performs better. To validate the performance of the model on other datasets, we evaluated the model using the early diabetes risk prediction dataset, all with good results. Therefore, the model has strong applicability and reliability. It is successful in the early prediction of diabetic disease.

Although the model works better, there are two aspects of the two datasets used in this experiment. On the one hand, the sample size of the dataset is small, and the attribute values and noise in the data are lower than in the real data. On the other hand, there is an imbalance in the sample size ratio between diabetic and nondiabetic populations in the dataset. When the model is trained, it will be biased to the category with a high sample size, so that the category with a low sample size is not adequately trained, resulting in lower model performance. For future work, first, it is necessary to cooperate with hospitals and use the hospital data as training data, while the ratio of diabetic population and nondiabetic population data samples should be kept equal. Second, the number of datasets will be expanded to ensure sufficient training and testing. We will also work on other chronic diseases, such as heart disease and kidney disease."
3,"A review on current advances in machine learning based diabetes prediction Diabetes is a metabolic disorder comprising of high glucose level in blood over a prolonged period in the body as it is not capable of using it properly. The severe complications associated with diabetes include diabetic ketoacidosis, nonketotic hypersmolar coma, cardiovascular disease, stroke, chronic renal failure, retinal damage and foot ulcers. There is a huge increase in the number of patients with diabetes globally and it is considered a major health problem worldwide. Early diagnosis of diabetes is helpful for treatment and reduces the chance of severe complications associated with it. Machine learning algorithms (such as ANN, SVM, Naive Bayes, PLS-DA and deep learning) and data mining techniques are used for detecting interesting patterns for diagnosing and treatment of disease. Current computational methods for diabetes diagnosis have some limitations and are not tested on different datasets or peoples from different countries which limits the practical use of prediction methods. This paper is an effort to summarize the majority of the literature concerned with machine learning and data mining techniques applied for the prediction of diabetes and associated challenges. This report would be helpful for better prediction of disease and improve in understanding the pattern of diabetes. Consequently, the report would be helpful for treatment and reduce risk of other complications of diabetes.
 Diabetes is one of the major health problems (with no cure) across the world which leads to various other severe complications. According to world health organization, in 2014 there were 422 million people diagnosed with diabetes and half of the population was undiagnosed. Early detection is the major positive factor for the treatment of diabetes and reducing other related complications. Thereby, considering the importance of diabetes, a number of computational methods have been developed for prediction of diabetes and are also associated with complications [39]. These diabetes prediction methods are based on data mining and machine learning methods. There are several databases of diabetes available; they have different datasets and sizes. These datasets are of different locations, PIDD [28] dataset is the most popularly used dataset for machine learning based diabetes prediction. The National Institute of Diabetes and Digestive and Kidney Disease (NIDDK) have studied the datasets comprising of population near Phoenix, Arizona, USA since 1965. This dataset constitute all the female patients of at least 21 years old of Pima Indian Heritage. The other popular diabetes datasets which were used in various studies are: dataset taken from 1999 to 2014 National Health and Nutrition Examination Survey (NHANES) [30] and data taken from Arvind Eye Hospital and Postgraduate Institute of Ophthalmology, Cuddalore Road Thavalakuppam Junction, Pondicherry [39]. The computational method has a potential to predict diabetes in the early stage. In the past various methods have been developed using machine learning algorithms which can help to rapidly and efficiently diagnose the disease. In the past researchers had used algorithms like ANN [35], PNN [39], SVM, Bayesian method [37], multilayer perceptron [37], back propagation algorithm [34], modified-particle swarm optimization [5], LS-SVM [5], Apriori algorithm [34], fuzzy-c mean clustering [36], etc. for development of methods for diabetes prediction, and had got fairly good accuracy in experiment (Table 1). The researchers have also used fuzzy sets to decrease the computational time to process the datasets and increase the efficiency of prediction models [31], [36], [38].
Application of new and rare machine learning methods is important but without the knowledge of the current issues and bottle neck in diabetes prediction the construtive progress cannot be made. As in recent study implementation of leftover and rare machine learning methods were not able to achieve better accuracy than previous studies [57].

The broad view in the trend for diabetes prediction revealed that the diabetes prediction was initially based on the simpler neural network based ML methods which progressively evolved and more advanced machine learning algorithms such as deep learning (CNN) have been implemented to improve the accuracy as well robustness of the prediction [46].

It is also observed that initially one ML algorithm was used at a time for the prediction but gradually the hybrid and combined models based on more than one ML algorithms were developed to achieve better accuracy (Table 1). As most of the studies have used PIDD therefore to minimize the role of datasets in trend of the diabetes prediction research a data independent ML algorithm is the current need of the hour. The graph was plotted between the algorithm/method and accuracy values from the studies which were based on PIDD and the studies which provided accuracy values of their models in test set (Fig. 3). It is clear from graph (in Fig. 1) that the most of the hybrid/combined models had high accuracies when compared to single ML methods (Fig. 3). These high accuracies (>95%) were achieved by the combination of methods such as SVM + ANN and LS-SVM-MPSO (Table 1). Despite the successful development and achieving more than 95% accuracy no diabetes prediction model claims to be used in diabetes prediction for global population. Diabetes is the global problem in which the life style, race and environment are the important factors which influence the disease. The major challenge in incorporating different datasets of diabetes in ML model to develop global prediction model is the features present in datasets are not same in different datasets [49]. The reason for less reliability of diabetes prediction methods is that most of these methods were only tested and trained on a single dataset. In view of the global nature of diabetes these methods should be trained, validated and tested on the different population. Different available data fusion methods can be used for fusion of different datasets in order to build model from different datasets. In all past studies, only one method has used two different datasets for training/testing and applied data fusion algorithm to merge the datasets to build prediction model [49]. Ideal machine learning methods should be tested and trained on different representative population all around the globe. However, few studies had used different datasets but it can be concluded that using more than one datasets in ML models seems to have inverse effect on the accuracy of the model. Two machine learning based study which had used the different dataset, i.e. Negi and Jaiswal [49], and Mirshahvalad and Zanjani [50] had the accuracies around 75% only. Even, in the second study the datasets used were similar and no method for data fusion was used/required in the study. So, it was observed that the using more than one datasets (merging the datasets) in model building/testing results in decrease in the accuracy of the models (Fig. 3).

To build multiple datasets based prediction model different datasets must be fused/merged through data fusion methods before the training for model building. Most common algorithms of data fusion that are categorized under three categories are data association, state estimation, and decision fusion [40]. These techniques are divided based on the criteria: the relationship between the input sources and these relationships are complementary, redundant, and cooperative. Based on the nature and type of input and output common data fusion system was given by Dasarathy [41]. So, input source of datasets is influential for the selection of fusion method. Although, simpler methods can also be developed using common feature between different datasets or missing features in one dataset can be filled with zero/mean values to fuse or combine the datasets [49]. However, merging the datasets is challenging to achieve the better accuracy but it is required to make practical use and reliability of the prediction model of diabetes. Further, combined and hybrid machine learning models have potential to achieve better/high accuracy on the challenging datasets and proposed to be developed in further research.

The current review suggests that machine learning methods can be more reliable if they will be trained, validated and tested on global population or at least on represented dataset from all the available diabetes datasets. The different features present in different datasets exert challenge in combining the datasets which require data fusion before model building. Finally state of art machine learning algorithms such as SVM, ANN and deep learning must be used to build prediction model for comparison the performance of each algorithm alone and combination to identify the best method for the detection of diabetes."
4,"Advancing diabetes prediction with a progressive self-transfer learning framework for discrete time series data. Although diabetes mellitus is a complex and pervasive disease, most studies to date have focused on individual features, rather than considering the complexities of multivariate, multi-instance, and time-series data. In this study, we developed a novel diabetes prediction model that incorporates these complex data types. We applied advanced techniques of data imputation (bidirectional recurrent imputation for time series; BRITS) and feature selection (the least absolute shrinkage and selection operator; LASSO). Additionally, we utilized self-supervised algorithms and transfer learning to address the common issues with medical datasets, such as irregular data collection and sparsity. We also proposed a novel approach for discrete time-series data preprocessing, utilizing both shifting and rolling time windows and modifying time resolution. Our study evaluated the performance of a progressive self-transfer network for predicting diabetes, which demonstrated a significant improvement in metrics compared to non-progressive and single self-transfer prediction tasks, particularly in AUC, recall, and F1 score. These findings suggest that the proposed approach can mitigate accumulated errors and reflect temporal information, making it an effective tool for accurate diagnosis and disease management. In summary, our study highlights the importance of considering the complexities of multivariate, multi-instance, and time-series data in diabetes prediction. Given the complexity of the disease and unknown interactions between related factors, the accurate prediction of diabetes development is crucial. In this study, we proposed a progressive self-transfer network that incorporates time series data preprocessing methods, shifting and rolling window and modifying time resolution, to reflect feature representations in multivariate and multi-instance time series analysis. The proposed method also accounts for dynamic temporal patterns, including temporal imbalance of the label, which is common in medical data. The gradual improvement of the metrics performances shown in Fig. 2 indicates that the progressive self-transfer network followed by ensemble method efficiently integrates and employs information added over time, enabling each downstream classifier to interpret the dataset from their own perspective. The results demonstrate that the proposed method can effectively recognize previously unseen data patterns and transfer the acquired knowledge as background information to the sequential tasks. Therefore, the model can now detect more patients earlier than before, enabling early diagnosis and intervention.

Furthermore, our study contributes to the field by utilizing deep learning methods for time series prediction tasks on the KoGES dataset. To the best of our knowledge, there have been limited studies on this dataset using deep learning techniques. Previous studies have used conventional algorithms and primarily focused on identifying the association of a single or a few factors with the development of diabetes. In contrast, our approach considers multiple relevant features to predict diabetes development, providing a more comprehensive understanding of the disease. As such, our study offers a significant contribution to the field of diabetes prediction using time series analysis and deep learning methods.

In order to optimize the LSTM model used in this research, we conducted a series of experiments adjusting various parameters. These experiments were performed across all stages, including the non-progressive self-transfer networks, single progressive self-transfer networks, and their ensemble applications. We tested LSTM models with different numbers of layers, and found that the model with five layers performed slightly better than the four-layer model in some metrics, but with a significant increase in standard deviations. We also manually adjusted the dropout rate and input unit size. Additionally, we optimized the methods used to modify the time series data, including the initial window size and the expansion of time intervals. Since we had a limited number of discrete time steps, we chose to double or triple the time resolution, which allowed us to finalize the structure of the input and output data.

It is important to note that our study may have potential bias or human error issues, which can arise in discriminative supervised models. One potential source of human error is in the labeling process, which can be influenced by misreported survey responses or biases introduced during clinical measurements. Moreover, our study did not discriminate between type 1 diabetes mellitus (T1DM) and T2DM, which limits our understanding of the participants’ diabetes mellitus development. To address these limitations, future studies could focus on disease-specific data collection to improve the reliability of the labels and allow the discrimination between T1DM and T2DM. Additionally, incorporating genetic information such as single nucleotide polymorphisms (SNPs) could enhance the model’s background knowledge and enable personalized patient care. SNPs, in combination with lifestyle habits, could serve as key factors in diabetes development and support more effective patient interventions."
5,"An effective correlation-based data modeling framework for automatic diabetes prediction using machine and deep learning techniques. The rising risk of diabetes, particularly in emerging countries, highlights the importance of early detection. Manual prediction can be a challenging task, leading to the need for automatic approaches. The major challenge with biomedical datasets is data scarcity. Biomedical data is often difficult to obtain in large quantities, which can limit the ability to train deep learning models effectively. Biomedical data can be noisy and inconsistent, which can make it difficult to train accurate models. To overcome the above-mentioned challenges, this work presents a new framework for data modeling that is based on correlation measures between features and can be used to process data effectively for predicting diabetes. The standard, publicly available Pima Indians Medical Diabetes (PIMA) dataset is utilized to verify the effectiveness of the proposed techniques. Experiments using the PIMA dataset showed that the proposed data modeling method improved the accuracy of machine learning models by an average of 9%, with deep convolutional neural network models achieving an accuracy of 96.13%. Overall, this study demonstrates the effectiveness of the proposed strategy in the early and reliable prediction of diabetes.
 In this section, the simulated outcomes of the data modeling strategy for diabetes classification have been compared against each other and also with other recent similar studies. It has been observed that the proposed data modeling approach yields significant improvement with the application of either ML or DL models. With the proposed data modeling technique, ML models, specifically the Random Forest classifier, have exhibited increased performance metrics, with an accuracy of 82.82%. A simple deep CNN algorithm proposed for DL models achieved classification accuracies of 88.38% and 96.13% with and without data modeling, respectively. The detailed comparison with recent existing methods is reported in Table 10. As previously mentioned, a significant portion of the human population is affected by diabetes. If left unchecked, it will pose a grave threat to the global community. Therefore, in our proposed research, we designed a robust diabetic prediction model by combining a data modeling approach with ML and DL algorithms. Moreover, the significance of pre-processing has been examined, and it has been determined that it plays a crucial role in accurate and reliable prediction. However, the suggested research primarily focused on establishing a data modeling framework with the goal of providing more relevant data to the learning algorithm’s input in order to improve accurate diabetes prediction among individuals. PIMA Indian Diabetes (PID) data from the UCI machine learning repository database was used in the experiment. During each test, both the original input dataset and the suggested redesigned dataset were used to validate the performance of the classification algorithms. Compared to the original dataset, it has been observed that the input dataset using the data modeling technique significantly improves the performance of the recommended ML models. Furthermore, the proposed data modeling framework was also applied to a seven-layered deep CNN model and achieved promising accuracy of 96.13% for early prediction of diabetes. Overall, the proposed data modeling strategy enhanced the accuracy of all the suggested ML and DL models by an average of 10%. In the future, we plan to create a comprehensive system in the form of a website or mobile application that uses the proposed data modeling approach to assist healthcare professionals in the early detection of diabetes.
"
6,"Analysis of clinical predictors of kidney diseases in type 2 diabetes patients based on machine learning Background: The heterogeneity of Type 2 Diabetes Mellitus (T2DM) complicated with renal diseases has not been fully understood in clinical practice. The purpose of the study was to propose potential predictive factors to identify diabetic kidney disease (DKD), nondiabetic kidney disease (NDKD), and DKD superimposed on NDKD (DKD + NDKD) in T2DM patients noninvasively and accurately.

Methods: Two hundred forty-one eligible patients confirmed by renal biopsy were enrolled in this retrospective, analytical study. The features composed of clinical and biochemical data prior to renal biopsy were extracted from patients' electronic medical records. Machine learning algorithms were used to distinguish among different kidney diseases pairwise. Feature variables selected in the developed model were evaluated.

Results: Logistic regression model achieved an accuracy of 0.8306 ± 0.0057 for DKD and NDKD classification. Hematocrit, diabetic retinopathy (DR), hematuria, platelet distribution width and history of hypertension were identified as important risk factors. Then SVM model allowed us to differentiate NDKD from DKD + NDKD with accuracy 0.8686 ± 0.052 where hematuria, diabetes duration, international normalized ratio (INR), D-Dimer, high-density lipoprotein cholesterol were the top risk factors. Finally, the logistic regression model indicated that DD-dimer, hematuria, INR, systolic pressure, DR were likely to be predictive factors to identify DKD with DKD + NDKD.

Conclusion: Predictive factors were successfully identified among different renal diseases in type 2 diabetes patients via machine learning methods. More attention should be paid on the coagulation factors in the DKD + NDKD patients, which might indicate a hypercoagulable state and an increased risk of thrombosis.
 An accurate noninvasive method to differentiate three types of kidney diseases in T2DM patients can improve the ability of clinical diagnosis and identify patients who did not undergo renal biopsy. It will also reduce the rate of misclassification and misdiagnosis of that population. In this retrospective study, we applied machine learning algorithm to analyze T2DM patients with renal damage and identify distinct features among them. Our models performed better than previously developed ones with high precision values of pairs of groups DKD vs NDKD (91.3%) and NDKD vs DKD_+_NDKD (91.03%). Although the performance of our model for distinguishing DKD from DKD_+_NDKD was not ideal, the features such as D-Dimer and INR found in the study could serve as meaningful indicators.

Lasso Linear Regression was used for classifying DKD and NDKD and further compared with models from Chen’s team [7, 8]. The comparison was based on the similar enrolled criteria and conducted in a Chinese people, which would increase the reliability and applicability of the results. The reason for the better performance of our model for differentiating DKD and NDKD could be closely related to the choice of weights of multivariate factors by machine learning algorithm. Subsequently, we successfully established a model to identify NDKD with DKD_+_NDKD using Kernel SVM after analyzing more than 18,000 medical data. This model adequately reflected the advantage of SVM in dealing with small samples and nonlinear feature interactions. Finally, we attempted to construct a model to predict DKD_+_NDKD and DKD, the precision value of which was 68.22%. Compared with these results, the performance of DKD and DKD_+_NDKD model was not as accurate, the reason for which might be explained by the smaller sample size of the DKD_+_NDKD group. We plan to collect more cases of DKD_+_NDKD to improve the model’s performance in a follow-up study.

HCT, DR and hematuria were three most important characteristic variables to differentiate DKD from NDKD, and the others were PDW and history of hypertension. Our results showed that only DR was a positive risk factor in predicting DKD. The predictive value of DR alone had been reported in two meta-analysis. The results showed that DR was a strong predictor in diagnosing or screening for DKD with a pooled sensitivity of 0.65/0.67 and specificity of 0.75/0.78, respectively. Meanwhile, they emphasized the high diagnosis value of proliferative DR in predicting DKD [20, 21]. A recent review also indicated that patients with DR would increase the risk of DKD by 31% [22]. All the evidences indicated that DR was a key element applied in non-invasive clinical diagnosis. In addition, we have shown that HCT, hematuria, PDW and history of hypertension are implicated as risk factors for developing NDKD. The indicator of hematuria and history of hypertension are consistent to the findings in previous studies [7, 9, 10, 15, 23]. Hct is considered as a determinant factor of oxygen supply and blood viscosity and can provide physicians useful information about red blood cell volume [24]. In DKD patients, long-term hyperglycemia leads to renal interstitial hypoxia, aggravates inflammatory response, prevents erythropoietin (EPO) production, resulting in anemia [25]. In contrast, NDKD patients are less closely associated with anemia. PDW reflects platelet activation and is related to microthrombi formation. Patients with DKD exhibit hemostatic abnormalities, which can aggravate renal ischemia and hypoxia under the hypercoagulable state [26]. Our study confirmed that Hct and PDW were valuable predictors in differentiating DKD and NDKD.

We reported that hematuria, duration of diabetes, INR, D-Dimer, HDL-C, BMG, DR, TG, TP, MPO, MCV and LEU as predictors for identifying NDKD and DKD_+_NDKD. Among these variables, hematuria, HDL-C, TG and MCV were identified as risk factors for NDKD. These findings indicated that microscopic hematuria and higher level of plasma lipids could be two representative clinical manifestations in NDKD patients in contrast to the DKD_+_NDKD. So, controlling the modifiable factor of lipid as early as possible might benefit patients with NDKD. The risk factors for the DKD_+_NDKD mainly reflected in hyperglycemia conditions and coagulation function, such as duration of diabetes and D-dimer. Previous studies suggested that there existed an increased D-dimer level and hypercoagulable state in DKD patients [27]. Hypercoagulation had a tendency to increase thrombosis through upregulation of coagulation factors [28]. So, we inferred that the higher level of D-dimer in DKD_+_NDKD patients might exhibit thrombotic tendencies under a double damage to the kidney. Therefore, with regard to the DKD_+_NDKD, it might be necessary to raise the patients’ awareness of blood glucose control and improve the ability of health management in this population. Moreover, there was high prevalence of NDKD and the DKD_+_NDKD (79.25%) in patients underwent kidney biopsy, which again reminded us of the importance of identifying different patterns of kidney diseases in T2DM.

Differentiating DKD_+_NDKD from DKD was a challenging problem in clinical diagnosis. Current studies were limited to describe the incidence and types of pathology in the DKD_+_NDKD patients [5, 7, 10, 12, 13, 29]. Based on the successful construction of the two prediction models, we conducted a tentative study on this group and found that D-dimer and INR were two important parameters for predicting DKD_+_NDKD when compared with DKD. In the present study, the level of D-dimer in the DKD_+_NDKD was twice more than those of the other groups (shown in Table 2). Previous studies observed higher D-dimer level in DKD patients, which indicated that renal injury was associated with disorder of the coagulation system and alerted the progression of the disease via coagulation-protease-dependent signal [27, 30]. Our findings revealed that it would be beneficial for monitoring coagulation function early in the DKD_+_NDKD patients.

Based on the comprehensive analysis of the selected features among three groups of patients, we found that hematuria as a common variable included in all three models. T2DM patients were more likely to develop NDKD when hematuria level was high, which was used as an indication for renal biopsy [19, 31]. In the present study, hematuria was indeed an important feature of NDKD, and its relative importance gradually reduced from the first place to the third in three models. The prevalence of hematuria in NDKD (55%) was higher than in the other groups, which is consistent with a previous study [4]. However, there were opposite views that hematuria might be a clinical manifestation of DKD [32]. The discordance might ascribe to the difference in the selection of subjects and definition of hematuria. Recently, a meta-analysis showed that the predictive value of hematuria in NDKD was low with the pooled sensitivity of 0.42 and specificity of 0.72, and further revealed that urinary dysmorphic erythrocytes might be more effective in predicting NDKD [31].

DR was the second common risk factor in each group. Presence or absence of DR as predictors for DKD or NDKD has been widely studied. On the other hand, it was pointed out that the severity of DR might not be correlated with the presence of DKD [33]. This indicated that DR alone might be insufficient as a predictor of DKD, and it should be combined with other indicators to improve predictive accuracy in T2DM patients with kidney damage. Interestingly, our findings also showed that the importance of DR as a risk factor varied significantly among the three groups of patients. The importance of DR ranked second in distinguishing DKD and NDKD, seventh in NDKD and DKD_+_NDKD, and fifth in DKD and DKD_+_NDKD. This suggested that DR was more likely to be related to DM-associated kidney diseases.

Another important indicator for separating DKD from NDKD patients was anemia. It was reported that anemia occurred earlier and more severe in DKD patients due to reduction of erythropoietin production and other pathophysiology mechanisms [34]. Currently, one study from histopathological perspective revealed that severe interstitial fibrosis and tubular atrophy (IF/TA) independent of global glomerulosclerosis caused damage to erythropoietin production, resulting in earlier concurrency of anemia in DKD patients [35]. Therefore, it was worthwhile to further elucidate the mechanism of early anemia and renal diseases progression in T2DM patients.

The study collected the entirety of patients’ electronic medical record at the time of kidney biopsy and carried out a comprehensive analysis of the diagnosis of different kidney diseases. Secondly, machine learning techniques applied to large volume and multiple patterns of clinical data could better identify meaningful parameters without using explicit instructions and lead to stable and accurate predictive models. Finally, data with missing variables to some extent could still be useful in research without affecting final results. Nevertheless, there existed several limitations to this study. Firstly, it was a retrospective study in a single center and the recruited patients were limited in one provincial of China, which would decrease the extensive applicability of the results. Secondly, the machine learning techniques may lead to some findings of unknown or unmeaningful factors, which need future study to illustrate their significance. Moreover, the goal to find and establish a differential diagnosis of DKD and the DKD_+_NDKD has not been realized and needed to be followed up in a future study.

In conclusion, we successfully developed models for distinguishing NDKD from DKD and DKD_+_NDKD from NDKD in patients with T2DM using machine learning methods. Several meaningful risk factors were identified among these three groups, which will benefit patients with contraindications for renal biopsy. Furthermore, our results also suggested that more attention should be paid to coagulation function in DKD_+_NDKD patients."
7,"Application of supervised machine learning algorithms for classification and prediction of type-2 diabetes disease status in Afar regional state, Northeastern Ethiopia 2021 Ethiopia has been challenged by the growing magnitude of diabetes in general and type-2 diabetes in particular. Knowledge extraction from stored dataset can be an important base for better decision on diabetes rapid diagnosis, suggestive on prediction for early intervention. Thus, this study was addressed these problem by application of supervised machine learning algorithms for classification and prediction of type 2 diabetes disease status and might provide context-specific information to program planners and policy makers so that, priority will be given to the more affected groups. To apply supervised machine learning algorithms; compare these algorithms and select the best algorithm based on their performance for classification and prediction of type-2 diabetic disease status (positive or negative) in public hospitals of Afar regional state, Northeastern Ethiopia. This study was conducted at Afar regional state from February to June of 2021. Decision tree; pruned J 48, Artificial neural network, K-nearest neighbor, Support vector machine, Binary logistic regression, Random forest, and Naïve Bayes supervised machine learning algorithms were applied using secondary data from the medical database record review. A total of 2239 sample Dataset diagnosed for diabetes from 2012 to April 22/2020 (1523 with type-2 diabetes and 716 without type-2 diabetes) was checked for its completeness prior to analysis. For all algorithms, WEKA3.7 tool was used for analysis purposes. Moreover, all algorithms were compared based on their correctly classification rate, kappa statistics, confusion matrix, area under the curve, sensitivity, and specificity. From the seven major supervised machine learning algorithms, the best classification and prediction results were obtained from random forest [correctly classified rate (93.8%), kappa statistics (0.85), sensitivity (0.98), area under the curve (0.97) and confusion matrix (out of 454 actual positive prediction for 446)] which was followed by decision tree pruned J 48 [correctly classified rate (91.8%), kappa statistics (0.80), sensitivity (0.96), area under the curve (0.91) and confusion matrices (out of 454 actual positive prediction for 438)] and k-nearest neighbor [correctly classified rate (89.8%), kappa statistics (0.76), sensitivity (0.92), area under the curve (0.88) and confusion matrices (out of 454 actual positive prediction for 421)]. Random forest, Decision tree pruned J48 and k-nearest neighbor algorithms have better classification and prediction performance for classifying and predicting type-2 diabetes disease status. Therefore, based on this performance, random forest algorithm can be judged as suggestive and supportive for clinicians at the time of type-2 diabetes diagnosis.
 The study identified the prediction and classification accuracy of RF [correctly classified rate (93.8%), kappa statistics (0.85), sensitivity (0.98), AUC (0.97) and confusion matrix (out of 454 actual positive predict as positive for 446 of them)], DT pruned J48 [correctly classified rate (91.8%), kappa statistics (0.80), sensitivity (0.96), AUC (0.91) and confusion matrices (out of 454 actual positive predict as positive for 438 of them)],K-NN [correctly classified rate (89.8%), kappa statistics (0.76), sensitivity (0.92), AUC (0.88) and confusion matrices (out of 454 actual positive predict as positive for 421 of them)].

Furthermore, prediction and classification accuracy were obtained from SVM [correctly classified rate (85.5%), kappa statistics (0.66), sensitivity (0.91), AUC (0.88) and confusion matrix (out of 454 actual positive predict as positive for 414 of them)], logistic regression [correctly classified rate (87.2%), kappa statistics (0.69), sensitivity (0.93), AUC (0.90) and confusion matrices (out of 454 actual positive predict as positive for 424 of them)], ANN [correctly classified rate (88.8%), kappa statistics (0.74), sensitivity (0.92), AUC (0.90) and confusion matrices (out of 454 actual positive predict as positive for 417 of them)] and naïve Bayes [correctly classified rate (86%), kappa statistics (0.68), sensitivity (0.89), AUC (0.88) and confusion matrices (out of 454 actual positive predict as positive for 407 of them)].

From the seven major supervised machine learning algorithms, the best classification and prediction result for type-2 diabetes disease status was obtained from RF classification and prediction algorithm 93.8%ofcorrectly classification capacity. In the WEKA tool, the percentage of accurately classified instances is called the accuracy of the classifying model. RF algorithm also shows an excellent prediction capacity at 100% accuracy on other studies conducted by Tejashri and his colleagues in 2015 at Wagholi, Pune, India19 and the study conducted by Naqvi and his colleagues in 2018 found that RF (accuracy of 89.3%)as the best technique for prediction of diabetes20.

On the other hand, the best accuracy of this study obtained from random forest (RF) (93.8%) is higher than the accuracy which was obtained from the previous study (85.9%) by Iyer and his collageus. This might be due to the number of input variables used because our study used seven input variables while the other study used only four variables (Plasma glucose concentration, BMI, DPF, and Age) as an input variable for classification and prediction of diabetes disease status13.

Based on their accuracy, this algorithm (RF) was followed by DT-pr-une-d J48 and k-nearest neighbor (K-NN) algorithms at 91.8% and 89.8%, respectively. Another study carried out by Roobini and Lakshmi on 2018 showed that DT pruned J48 and K-NN accuracy of 91.72% and 91.14% respectively, which is consistent in accuracy with our research study3.

Basically Pruned J48 Tree is fast DT learning and it builds a DT based on the information gain or reducing the variance. It examines normalized information gain (difference in entropy) that results from choosing an attribute as a split point. The highest normalized IG is used at the root of the tree. The DT pruned J48 generate a tree contained 18 number of leaves with 35 size of tree that gets an accuracy of 91.8%. This help to extract certain rules and reduce the risk of over fitting on the training data. From this, variable age was obtained as a root node with the highest information gain (870.0/148.0) when age is_>=_36 year to be more likely tested positive for type-2 diabetes others as a leaves. This result is consistent with the result obtained from a study conducted by Amatul and his colleagues in 2017 from Indian diabetes dataset for prediction and classification of type-2 diabetes disease status which showed a diagnostic result of positive for type-2 diabetes disease was at the age of 40 years old21.

In addition, other rules which can be extracted from this schematic representation showed that age_>=_36 years which is the root node, DBP_>_81, FBSL_>_115, RBSL_>_211, sex_=_female and BMI_>_25 more likely to be predicted as tested positive for type 2 diabetes. Another study entitled as “Diagnosis of diabetes using classification mining techniques” which was conducted by Aiswarya and his colleagues22 showed that plasma glucose concentration as a root node. This difference may be due to this study considers both fasting and random glucose levels separately but the other study used in combination as a single variable.

The other alternatives that have been used in this study for performance comparison were kappa statistics. From this kappa statistics that compares observed accuracy with expected accuracy (random chance) out of the seven supervised machine learning algorithms, it was higher in RF algorithm which was 0.85 and followed by DT pruned J48 (0.80) and K-NN (0.76).The value of kappa statistics of the second best algorithms (DT_=_0.80) is almost double of the value of kappa statistics of another study DT J48 algorithm which was 0.4713. This inconsistent result might be due to the effect of pruning because pruning reduces the occurrence of events due to chance.

The study conducted at United Arab Emirates in 2015 was found the higher kappa statistics result from naïve Bayes algorithm 0.50 followed by J48 DT which was 0.4715. Normally, the value of kappa statistics ranges from __1 up to_+_1, meaning the value 1 indicates perfect classification -1 indicates wrong classification, i.e., all the classifications are occurred due to chance. Based on this range, the result of this study obtained from random forest, DT, pruned J48, and K-NN for classification of type-2 diabetes disease status lies on excellent classification and prediction capacity.

As can be seen from the result table of Table 7, in this study, the performance of the seven supervised machine learning algorithms was compared to their classification performance based on specificity, sensitivity, and area under ROC curve (Table 7). For the testing dataset, the comparative analysis results demonstrated that the random forest DT pruned J48 and K-NN algorithms showed the best result of sensitivity of 98%, 96%and 92% respectively. On the base of specificity, the best result was observed in random forest, K-NN, and DT pruned J48 algorithms with 93%, 92%, and 91%, respectively. On the other hand, their area under the ROC curve showed that almost similar results ranging from (0.88–0.91) except the maximum ROC curve result obtained from random forest which is 0.97. Surprisingly, in all aspects of performance comparison criteria, random forest algorithm came out to be the best algorithm with a classification and prediction capacity based on its excellent performance of sensitivity, specificity, and ROC curve. A similar result was obtained from another study conducted at Pakistan in 2018 by Naqvi and his colleagues with the highest sensitivity of random forest at 96.2%20.

In addition, a review paper reviewed by Chui and his colleagues in 2017 showed that RF algorithm had an excellent capacity on classification and prediction of type-2 diabetes at a ROC curve of 0.98, which was, exactly similar with our study23.

Another research work conducted on Classification of Diabetes patient by using Data Mining Techniques in 2018 by Nidhi and his colleagues24 showed that a better classification performance by DT J48 at 65.3% of sensitivity. This result is even lower than the true sensitivity of our result obtained from DT J48 algorithm which was 96% of sensitivity. This difference in prediction and classification performance may be due to the difference in sample size used for data mining and the attributes used to classify and predict type-2 diabetes disease status. In the previous study, they have been conducted on 768 instances with eight attributes where as in our research study we used 2239 samples with eight attributes. This indicated that an increase in sample size will lead to better classification and prediction capability of these supervised machine learning algorithms’11.

From the result of confusion matrix showed on Table 8 which was conducted on 672 samples of testing dataset contained 454 actual positive and 218 actual negative for type-2 diabetes. On the basis of these evaluation criteria, the seven supervised machine learning algorithms, random forest and DT pruned J48 algorithms were revealed the highest prediction capacity on forecasting of true positive, i.e. out of 454 actual positive samples they predict 446 and 438 of them as positive respectively. On the other hand, the lowest prediction performance was obtained from the SVM algorithm out of 454 actual positive participants, it predicts to be tested positive only for 414 of them. This study result is consistent with the study conducted in Wagholi, Pune, India, in 2015 entitled as Data Mining Approach for Diagnosing Type-2 Diabetes status19.

Strength and limitation of the study
This study was carried out using an appropriate methodological approach with sufficient amount of sample size for data mining. Methodologically, the study covered seven supervised machine learning algorithms (ANN, K-NN, Naïve Bayes, DT, pruned J48, logistic regression, RF, and SVM). The best three classifier and predictor models were selected for prediction of the upcoming new client for type-2 diabetes diagnosis. As a result, these points are considered as the strength of the study.

On the other hand, this study was having the following limitations: firstly, since the study was conducted on secondary data, some of the important variables for classification and prediction of type-2 diabetic disease like glycated hemoglobin level were missed. The reason behind the removal of this important predictor variable is the occurrence of very few (only 2) diagnostic results recorded for this variable due to its expensiveness to perform the test. Secondly, while some of the record values of certain instances are missed; has been replaced with mean which affects the accuracy of prediction and classification performance, so that this also considered as limitation.
"
8,"Artificial Intelligence and Machine Learning in Endocrinology and Metabolism: The Dawn of a New Era The rapid growth of technology in the past couple of decades has paved the way for development of novel techniques that can solve scientific questions at a rate that is far beyond the capability of humans. One such example is the field of Artificial Intelligence (AI) and Machine Learning (ML). AI is a discipline that deals with the study and design of intelligent agents, that is, devices that intricately perceive their environment and take actions that maximize the chances of achieving their goals (1). AI, in a way, mimics the structure and operating methodologies of a human brain (2). AI has two forms of application: physical and virtual (3). The physical component is mainly represented by robots. Derived from a Czech word robota, meaning “forced labor,” the physical robotic forms were conceptualized by inventors such as Leonardo Da Vinci (3). This component has been widely used in the field of endocrinology, such as robot-assisted surgery of adrenal or prostate cancer. Examples of virtual applications of AI are electronic medical records (EMR), where specific algorithms are used to identify subjects, and harness health related data (3).

ML is a field of AI that deals with the development of models and intricate networks that enable computer systems to improve their performance on a specific task progressively (4). ML algorithms can be: (i) unsupervised (spontaneous pattern detection), (ii) supervised (building algorithms based on prior examples), or (iii) reinforcement learning (utilization of reward/punishment techniques to obtain the desired result) (3). A common use of ML in daily life includes flagging spam in an e-mail, autonomous driving and selecting the best route for daily commute. In the field of medicine, AI/ML technology can have substantial impact at three levels: physicians, by improving the diagnostic accuracy and assisting with therapeutic and surgical interventions; health systems, by enabling improved workflow and reduction in errors; patients, through tailoring of diagnostic, and treatment modalities based on the unique phenotypic and genetic features of individual patients (5). In this review, we focus on the virtual components of AI and ML and provide some examples for the utility of AI/ML in endocrinology and metabolism.

From early ML tools like logistic regression which found their utility in medicine several decades ago, AI/ML methods have become far more multifaceted and have revolutionized the field of medicine through their ability to compute and analyze vast and complex array of datasets which would not be feasible solely with trained human skillsets (2). Several AI/ML methods have proven their utility in the diagnosis and management of various endocrinopathies. Gradient forest analysis, a ML technique, was applied in a study to identify factors contributing to variation in all-cause mortality among subjects in the Action to Control Cardiovascular Risk in Diabetes (ACCORD) trial (6). This technique detected four risk groups based on hemoglobin glycosylation index (HGI), BMI, and age. The lowest risk group (with HGI < 0.44, BMI < 30 kg/m2, and age <61 years) experienced reduced absolute mortality risk of 2.3%, while the highest risk group (HGI > 0.44) experienced a 3.7% increase in absolute mortality risk attributable to intensive glycemic therapy. These mortality variations in the intensive treatment group were previously not detected by older, univariate subgroup analyses (6). Another study developed a prototype support vector regression model that outperformed diabetologists in predicting blood glucose levels at 30 and 60 min from a given time in patients with type 1 diabetes, and predicted about one quarter of hypoglycemic events 30 min ahead of the actual event (7). Another emerging field based on AI technology that could potentially have a wider scope in the future is “pre-emptive medicine” (20). Pre-emptive medicine is a novel concept proposed in Japan, which aims at delaying the onset, or even preventing the occurrence of chronic diseases, such as diabetes, hypertension, cancer, or dementia by using a combination of AI techniques, genomic analysis and environmental interaction data (20). The above examples reinforce the promising role of AI/ML in diagnosis and management of endocrine disorders which, in several instances, can outperform skilled physicians, minimize resource use and allocation, and yield tangible benefits by supporting physicians and accelerating clinical decision-making (7, 16). Despite the substantial evidence for the ability of AI/ML to deliver cost-effective healthcare and improve patient outcomes, medicine has trailed behind other scientific fields in implementing these techniques into practice (21). Potential hurdles include the longitudinal nature of variations in human disease, inadequacies in the quality and reliability, heterogeneity of healthcare data, personal data confidentiality, need for informed consent from patients, requirement of supportive policies and efficient business models, unpredictable reimbursement, and increasing necessity for data sharing (21, 22). The so-called “digital biomarkers” that are obtained through big data analyses performed using AI/ML techniques are not readily interpretable clinically, in the sense, even if a certain newer AI/ML algorithm has been shown to be superior to older techniques in certain population cohorts; its implementation in clinical practice across more diverse populations might not necessarily result in better diagnosis or outcome; and could potentially even lead to over-diagnosis and over-treatment in certain patient cohorts (23). Ethical issues, including misuse of AI/ML to manipulate quality metrics to make unscrupulous profit, potential in-built discriminatory biases toward under-represented populations, and physician over-dependence on AI/ML pose some of the foreseeable challenges in this field (24). Although AI/ML can theoretically “replace” physicians with regards to performing certain diagnostic, therapeutic, or surgical tasks, these technologies, almost certainly, will never be able to provide the emotional, social, and ethical support that a physician can offer the patient, and will never replace the unique bond of a doctor-patient relationship (25, 26).

Then the question arises: What aspects of patient care can physicians focus on in the era of AI/ML technology? With the ever-growing complexity and quantity of knowledge in the medical field, it is almost impossible for physicians to mentally organize and retain all of this data (27). Therefore, the medical fraternity should focus on strengthening the following aspects in order to re-define the role of physicians in the era of AI/ML: (1) Medical school curricula must shift their focus from information acquisition to knowledge management and communication skills, (2) Physicians must be trained to manage and collaborate with AI/ML applications, (3) Emphasis must be placed on training physicians to interpret AI/ML output data and to effectively utilize these results in clinical decision making, and (4) Reinforcing cultivation of empathy and compassion among physicians (27). Eventually, physicians and AI technology need to develop a mutually supportive relationship than a competitive one. While physicians can provide appropriate feedback for AI/ML techniques and tools to improve, these tools in turn can facilitate physicians in solving uncertain clinical scenarios (28). This can result in mutual identification of key clinical or algorithmic biases, which can be then tackled by the combined efforts of physicians and AI/ML through better data collection and through model improvements (28).

In conclusion, utilization of AI/ML will enable diagnosing endocrine disorders with higher accuracy, potentially avoid unnecessary investigations, and reduce healthcare expenditures and facilitate better digital storage of vast patient data, be it individual profiles or aggregated data for epidemiological research and planning, and these benefits may one day transform clinical endocrine practice. Today, AI technologies like artificial pancreas for management of diabetes have already become a reality (29). Major efforts are required from academia and the information technology industry to push for further development of AI/ML technology in endocrinology. Endocrinologists are well suited to play a vital role in the advancement of AI/ML. However, this has not been the focus of training programs for endocrine subspecialties, which do not provide the necessary education for trainees to feel confident in the use of these technologies for diagnosis or research. There is certainly a need to spread awareness, acquire funding, introduce these concepts into training programs, and encourage further research in this new, exciting branch of endocrinology and metabolism—a deep future awaits!
"
9,"Artificial intelligence based prediction models for individuals at risk of multiple diabetic complications: A systematic review of the literature. AIM: The aim of this review is to examine the effectiveness of artificial intelligence in predicting multimorbid diabetes-related complications.

BACKGROUND: In diabetic patients, several complications are often present, which have a significant impact on the quality of life; therefore, it is crucial to predict the level of risk for diabetes and its complications.

EVALUATION: International databases PubMed, CINAHL, MEDLINE and Scopus were searched using the terms artificial intelligence, diabetes mellitus and prediction of complications to identify studies on the effectiveness of artificial intelligence for predicting multimorbid diabetes-related complications. The results were organized by outcomes to allow more efficient comparison.

KEY ISSUES: Based on the inclusion/exclusion criteria, 11 articles were included in the final analysis. The most frequently predicted complications were diabetic neuropathy (n = 7). Authors included from two to a maximum of 14 complications. The most commonly used prediction models were penalized regression, random forest and Naive Bayes model neural network.

CONCLUSION: The use of artificial intelligence can predict the risks of diabetes complications with greater precision based on available multidimensional datasets and provides an important tool for nurses working in preventive health care.

IMPLICATIONS FOR NURSING MANAGEMENT: Using artificial intelligence contributes to a better quality of care, better autonomy of patients in diabetes management and reduction of complications, costs of medical care and mortality. When patients have additional chronic comorbid conditions, there is an almost exponential increase in the cost of care related to health care services, medicines and hospital admissions (McPhail, 2016). Complications resulting from type 2 diabetes, such as nephropathy, neuropathy, blindness, cardiovascular disease and amputations reduce their quality of life and increase mortality. With advances in the care and treatment of type 2 diabetes and its complications, people with diabetes can live with their condition for longer (Deshpande et al., 2008; Liu et al., 2010). It is important to detect the development of complications early enough, as rapid action can prevent or delay the onset of chronic complications (Marshall & Flyvbjerg, 2006). AI plays an important role in predicting complications using basic clinical and biochemical patient data, but predicting the occurrence of different complications is a challenging task due to different risk factors, unbalanced data and rapid changes (Singla et al., 2019). Therefore, there is an increasing emphasis on the use of appropriate AI techniques to predict prognosis (Singla et al., 2019; Yousefi & Tucker, 2020). Consequently, accurate prediction helps to target nursing interventions better (Ljubic et al., 2020). AI supports nurses in clinical decision-making and other tasks that are not directly related to the patient (Seibert et al., 2021).

Nurses, as the largest part of all workers involved in health systems, will benefit enormously from AI (Shang, 2021). The role of nurses is to be actively involved in decision-making regarding the implementation of AI in the health care system and to ensure that they ensure that these changes are implemented in accordance with the ethical principles and values of nursing (Buchanan et al., 2020). With the introduction of technology, nurses' experience, knowledge and skills will be transformed into learning new ways of thinking and processing information (Robert, 2019). Our literature review also found that nurses are rarely involved in the interdisciplinary team that carries out the implementation. In most cases, it was individual research carried out by the researchers. It would be important to involve health care providers as they can influence the actual implementation of AI in clinical practice.

IT skills training should be offered to nursing students and those already working in a clinical setting (Risling, 2017). It is important that they understand the potential of AI and its impact on health care (Fritz & Dermody, 2019). It is also important that nurses are empowered by technological change and that they are not just passively involved in it (Ng et al., 2021).

In practice, there is still a lack of models or frameworks for implementing AI in everyday health care practices (Svedberg et al., 2022). Yet, there are individual gaps in the literature on AI in nursing, with implications for clinical practice (Shang, 2021). The content of research in the field is very diverse, so it is important to develop guidelines on research reporting and technology implementation (von Gerich et al., 2021). This is also the problem we encountered in our literature review, and it is also the biggest limitation. The included research reported different methodological approaches and, above all, reported different results that cannot be synthesized due to inequalities. Despite the heterogeneity of the studies, most of them were based on data from patients diagnosed with diabetes, extracted from various electronic records. It is also very difficult to compare individual studies with each other, as they included a wide range of population sizes (minimum 129, maximum 287,438)."
10,"Artificial Intelligence for Diabetes Management and Decision Support: Literature Review Background: Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis.

Objective: The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges.

Methods: A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review.

Results: We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results.

Conclusions: We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients' quality of life. By systematically examining high-quality articles in the PubMed database, we identified a series of studies with the goal of evaluating the latest efforts of AI-enabled solutions for diabetes management. The topics we reviewed suggest that prediction and prevention are currently being revitalized and reinforced by AI applications, whereas “safety and failure detection” has been less extensively reviewed, constituting fewer than 6% of the studies we encountered. Similarly, few investigations have delved into the application of AI techniques to early detection of critical issues such as exercise, meals, infusion set failures, and so forth. Exploiting the latest AI techniques to improve the safety of both AP systems and open-loop tools has the potential to dramatically improve performance. By contrast, research on closed-loop systems, representing 31 out of 141 of the reviewed studies (22%), has been the most productive area for AI applications. Most of these efforts addressed fuzzy techniques, but the application of other methodologies has begun to attract increasing interest. In our opinion, researchers in this field should continue to take advantage of the latest improvements in AI and to combine them with development of the AP. A considerable number of the reviewed studies, 41 out of 141, investigated BG, either to develop models that enable accurate predictions of BG concentrations (27 studies) or to detect possible BG events (14 studies). Multiple studies reported accurate prediction and detection tools that promise to improve management resources for current and future therapies. These tools include bolus advisors, as well as both lifestyle and patient stratification.

Our findings show the increasing importance of AI methods for diabetes management. We think these methods will encourage further research into the use of AI methods to extract knowledge from diabetic data. In general, the most striking advances in the application of AI techniques come from data-driven methods that learn from large datasets. The ability to collect information from individual diabetic patients has led to a shift in diabetes management systems; accordingly, systems that lack access to valuable data will face substantial hurdles. Diabetes management is geared toward tailored management of therapies, at the level of smaller strata of patients or even individuals. Thus, management protocols provided to diabetic patients should be tailored to address their needs at various points during their illness. Furthermore, the availability of genetic data, such as that provided by metabolomics analysis, has also empowered the application of AI methods to personalization of diabetes management.

The increased availability of digitized health data from diabetic populations, along with the emerging applications of AI and research trends such as the AP and personalized medicine, suggests that we are moving toward a new paradigm for management of diabetes. This new outlook proposes to achieve custom delivery of diabetes care while tailoring professional practices, medical decisions, and treatments to individual patients. On the other hand, the inclusion of intelligent algorithms in decision making has ethical implications that should be addressed by physicians and scientists. The ethical risks associated with the release of personal data should also be investigated. For example, the increasingly frequent use of health apps and the potential use of tools based on AI by insurance companies could lead to discrimination or the exclusion (or both) of some citizens from health services.

A large number of studies have already been published on the application of AI to diabetes in a broad range of management domains. Our dive into PubMed demonstrates an acceleration in the pace of research on AI-powered tools designed to predict and prevent the complications associated with diabetes. Although the available technologies and methods for diabetes management are growing exponentially in terms of quantity and quality, the potential of AI to boost effective and accurate management of diabetes has already been demonstrated in both open- and closed-loop therapies. Research in this field should continue and should seek to discover the opportunities and advantages of applying AI methodologies in diabetes management that differentiate these strategies from other classical approaches."
11,"Artificial Intelligence for Predicting and Diagnosing Complications of Diabetes. Artificial intelligence can use real-world data to create models capable of making predictions and medical diagnosis for diabetes and its complications. The aim of this commentary article is to provide a general perspective and present recent advances on how artificial intelligence can be applied to improve the prediction and diagnosis of six significant complications of diabetes including (1) gestational diabetes, (2) hypoglycemia in the hospital, (3) diabetic retinopathy, (4) diabetic foot ulcers, (5) diabetic peripheral neuropathy, and (6) diabetic nephropathy. It is highly evident that AI can be incorporated into the process of predicting and diagnosing the progression of major complications associated with diabetes. Furthermore, advances in AI technology can be integrated into the personalized management of diabetes and its complications, leading to better treatment plans and improved patient outcomes. An important input for any AI software intended to predict, diagnose, treat, or prevent virtually every complication of diabetes will be CGM data. Activity monitors and apps to track nutrition may also prove to be useful.97 Donated data from the OpenAPS Data Commons,98 which include hundreds of individuals and thousands of days of data, have enabled more accurate blood glucose forecasting.99 Challenges to creating trustworthy AI for diagnosis and treatment include the need for standardized aggregation of clinical data, maintenance of patient privacy, de-emphasis of outlier data and noise, and the use of advanced statistical learning methods and ML algorithms. The value of an AI model would be degraded if contributing data sets had different sample sizes or dissimilar methods for feature extraction. Furthermore, data set can be modeled with AI in a variety of configurations.
Artificial intelligence can support a precision medicine paradigm for diabetes. This will be possible if multiple types of genetic, genomic, physiological, biomarker, environmental, and behavioral data can be collected, assembled, and analyzed with methods that ordinarily require human intelligence (artificial intelligence) or with methods that can identify patterns without being specifically programmed to find them (machine learning).
Six complications of diabetes are particularly common, debilitating, and costly for society, including (1) gestational diabetes, (2) hypoglycemia in the hospital, (3) retinopathy, (4) foot ulcers, (5) neuropathy, and (6) nephropathy. The use of larger data sets from real world data sources like EHRs will help improve the predictive accuracy of AI-powered software. The next decade promises to be one of great advances in precision medicine for predicting and diagnosing complications of diabetes powered by AI."
12,"Artificial Intelligence in Current Diabetes Management and Prediction Purpose of review: Artificial intelligence (AI) can make advanced inferences based on a large amount of data. The mainstream technologies of the AI boom in 2021 are machine learning (ML) and deep learning, which have made significant progress due to the increase in computational resources accompanied by the dramatic improvement in computer performance. In this review, we introduce AI/ML-based medical devices and prediction models regarding diabetes.

Recent findings: In the field of diabetes, several AI-/ML-based medical devices and regarding automatic retinal screening, clinical diagnosis support, and patient self-management tool have already been approved by the US Food and Drug Administration. As for new-onset diabetes prediction using ML methods, its performance is not superior to conventional risk stratification models that use statistical approaches so far. Despite the current situation, it is expected that the predictive performance of AI will soon be maximized by a large amount of organized data and abundant computational resources, which will contribute to a dramatic improvement in the accuracy of disease prediction models for diabetes. Next, we discuss the use of AI in medicine for diabetes, specifically in medical devices. The first AI-based medical device, BodyGuardian, was cleared by the US Food and Drug Administration (FDA) in 2012 when approval was given to a patch-like electrocardiogram equipped with an AI-based arrhythmia detection algorithm. Since then, the regulations on programmed medical devices, including AI, have advanced in various countries, including the USA, Europe, China, and Japan. Thanks to the outstanding development of deep learning technology and advancements in clinical applications these days, the number of approved AI-based medical devices has dramatically increased in both the USA and Europe in the past few years [4].

Currently, there are dozens of FDA-cleared AI-based medical devices using AI/machine learning technology. While most of these approvals are linked to radiology, cardiology, and oncology, three AI-based medical devices are related to diabetes management [5•]. In Japan, 12 types of AI-based medical devices have been approved as of 2020. However, all of them are for image analysis concerning radiology and diagnostic imaging, and there are no such medical devices approved for diabetes care.

Efforts towards the clinical application of AI in the diagnosis and treatment of diabetes are mainly categorized into four areas: (1) automatic retinal screening, (2) clinical diagnosis support, (3) patient self-management tools, and (4) risk stratification [6]. The first category is automatic retinal screening, an AI technology that automatically interprets the presence or absence of diabetic retinopathy—an important complication of diabetes—from fundus images. An example of this technology is the IDx-DR device manufactured by Digital Diagnostics Inc., approved by the FDA in 2018 for its high diagnostic performance by clinical trials [7]. Using this AI device, patients can be diagnosed with diabetic retinopathy or not without professional judgment from an ophthalmologist. Then, primary physicians can choose to have the patients with their fundus images see an ophthalmologist or re-examine the IDx-DR device 12 months later. This device facilitates the screening and diagnosis of diabetic retinopathy, especially in rural communities where patients have difficulties accessing an ophthalmologist.

The second category is clinical diagnostic support. Currently, AI technologies that mimic the “hidden tips of treatments by a specialist,” such as fine-tuning insulin dose, are being developed rather than just a support system for diabetes diagnosis itself. One example is Advisor Pro, manufactured by DreaMed Diabetes, Ltd., which the FDA approved in 2018. This system sends information obtained by continuous glucose monitoring (CGM) and self-monitoring of blood glucose (SMBG) to a cloud server and uses AI to determine and propose the necessity for insulin dose adjustments remotely. Then, physicians can review the proposals and notify patients. We introduce one of the clinical trials that evaluated the efficacy of this AI technology published in 2020 [8]. In this non-inferiority study, 108 patients with type 1 diabetes were randomly allocated to either an AI-managed group that received insulin treatments using the AI system or a manually managed group that received insulin treatments by a diabetes specialist. The results demonstrated that the targeted blood glucose concentration maintenance and hypoglycemia rates were non-inferior in the AI-guided group compared with the specialist manual managed group. In the future, there will be more situations like this where AI-based medical devices replace diabetes specialists in terms of fine-tuning insulin therapy.

The third category is the patient self-management tool. Self-management tool is familiar with some diabetes patients because they have already self-checked various biometric data such as actively measuring blood glucose levels through SMBG. With the patient self-management tools, the AI technology interprets their biometric data and alert like a diabetologist to improve the patient’s blood glucose control. The Guardian Connect System, manufactured by Medtronic, is an example of an AI system with this functionality. This system is based on CGM, has an accompanying smartphone application, and was certified by the FDA in 2018. It is characterized by using the AI to predict a hypoglycemic attack 1 h in advance based on the CGM data and alerts the patient. According to the product data, the accuracy of the alert is 98.5%, only 30 min before the onset of hypoglycemia. In this system, the AI issues alert for hypoglycemia to the patients from their biometric data, which are sometimes difficult to understand. Then, the patient can take, e.g., glucose tablets to prevent hypoglycemia and associated complications.

Go to:
Prediction of New-Onset Diabetes Using AI
Finally, the fourth category of AI usage in the diagnosis and treatment of diabetes is prediction and risk stratification. This category could be a part of preemptive medicine, accurately identifying individuals that are highly likely to develop a specific disease from the general population at the pre-illness stage. Thus, this technology would eventually eliminate the incidence of diabetes by implementing medical intervention for these people at a very early stage. Predicting the onset of diabetes does not happen with the advent of machine learning technology. To date, lots of diabetes onset prediction models have been created using statistics with known risk factors of diabetes in large cohorts. Abbasi et al. reported the usefulness of statistical models like logistic regression, Cox proportional hazard model, or Weibull distribution analysis to predict the onset of diabetes in non-diabetic individuals within 5 to 10 years [9]. In this report, the accuracy of prediction for new-onset diabetes within 5 to 10 years was around 0.74 to 0.94 in the C-index [9]. Despite the variance of predictive performance because of different baseline characteristics in each cohort, this result may show a relatively high level of predictive performance just by the conventional statistical models.

However, machine learning could be a promising tool that can maximize predictive performance than conventional statistics models. Table _Table11 demonstrates the studies predicting new-onset diabetes mellitus by machine learning models. Zou et al. [10] reported that the accuracy of new-onset DM prediction for hospitalized patients was around 0.81 using random forest. Choi et al. [11] also denoted that the area under the curve (AUC) of new-onset DM within 5 years for hospitalized patients was 0.78, but it derived from machine learning-based logistic regression. Other reports using population-based cohorts or electronic health records (EHR) denoted that new-onset DM prediction performance was around 0.84 to 0.87 in terms of AUC [12–14]. Moreover, Ravaut et al. [15] recently addressed that they could detect new-onset DM within 5 years with the performance of AUC 0.8026 using over 2 million general population with DM prevalence of just 1%. We also developed a machine learning-based prediction model to identify the diabetes signatures before the onset of diabetes using one of the machine learning algorithms, the gradient-boosting decision trees method. We recruited 509,153 annual health checkup records of 139,225 participants from 2008 to 2018 at Kanazawa city, Ishikawa, Japan. Of those, 65,505 participants without DM were included for the analysis. We identified 4,696 new-onset diabetes patients (7.2%) during the study period. Our trained model predicted the future incidence of diabetes with the area under the curve (AUC) and overall accuracy of 0.71 (95% confidence interval (CI), 0.69 to 0.72) and 94.9% (CI, 94.5–95.2), respectively [16].
"
13,"Artificial Intelligence in Efficient Diabetes Care Diabetes is a chronic disease that is not easily curable but can be managed efficiently. Artificial Intelligence is a powerful tool that may help in diabetes prediction, continuous glucose monitoring, Insulin injection guidance, and other areas of diabetes care. Diabetes, if not appropriately managed, leads to secondary complications like retinopathy, nephropathy, and neuropathy. Artificial intelligence helps minimize the risk of these complications through software and Artificial Intelligence-based devices. Artificial Intelligence can also help physicians in the early diagnosis and management of diabetes while reducing medical errors. Here we review the advancement of Artificial Intelligence in diabetes management.
 TYPE 1 DIABETES MANAGEMENT
There is a huge amount of literature on AI/ML approach being used in type 1 diabetes. There are algorithms that have been used to detect composition of food based on images of food thereby helping in carb counting.[16] Prediction of future blood glucose values and anticipating impending hypoglycemic or hyperglycemic event has been the focus of research in numerous publications.[17] Major work is also being done on developing bolus calculators to automate the process of calculating premeal insulin dose prediction.[18]

From the perspective of the applicability of these approaches in India, there are two major lacunae. Firstly, most of this research is carried out among people using the insulin pumps and CGMs. As use of these modalities in India is limited due to economic issues, usability of this research in India is also limited. Secondly, different researchers have focused on individual areas of type 1 diabetes management and there is still no single application/technology available that can solve management of type 1 diabetes including carbohydrate counting, calculating insulin-carbohydrate ratios, and also predicting insulin dose for each meal for each patient, especially on multiple subcutaneous daily injections.

LIMITATIONS AND THE WAY FORWARD
AI/ML is as good as the data used to generate this intelligence. Our country is sometimes called as “country with no records”, however, this may not be exactly true but it does underline the general scenario of lack of record-keeping as an essential part of medical practice in India. A huge burden of disease can be transformed into an opportunity, if entire data is harnessed in a usable form and AI/ML is used to generate insights and solutions specific to our population. A concerted and collective effort is needed by the government and large associations, like, endocrine society of India to initiate data collections and research.
"
14,"Artificial intelligence with temporal features outperforms machine learning in predicting diabetes Diabetes mellitus type 2 is increasingly being called a modern preventable pandemic, as even with excellent available treatments, the rate of complications of diabetes is rapidly increasing. Predicting diabetes and identifying it in its early stages could make it easier to prevent, allowing enough time to implement therapies before it gets out of control. Leveraging longitudinal electronic medical record (EMR) data with deep learning has great potential for diabetes prediction. This paper examines the predictive competency of deep learning models in contrast to state-of-the-art machine learning models to incorporate the time dimension of risk. The proposed research investigates a variety of deep learning models and features for predicting diabetes. Model performance was appraised and compared in relation to predominant features, risk factors, training data density and visit history. The framework was implemented on the longitudinal EMR records of over 19K patients extracted from the Canadian Primary Care Sentinel Surveillance Network (CPCSSN). Empirical findings demonstrate that deep learning models consistently outperform other state-of-the-art competitors with prediction accuracy of above 91%, without overfitting. Fasting blood sugar, hemoglobin A1c and body mass index are the key predictors of future onset of diabetes. Overweight, middle aged patients and patients with hypertension are more vulnerable to developing diabetes, consistent with what is already known. Model performance improves as training data density or the visit history of a patient increases. This study confirms the ability of the LSTM deep learning model to incorporate the time dimension of risk in its predictive capabilities. Early prediction of diabetes onset is important for all health care systems, as diabetes is now considered a modern preventable pandemic. Leveraging longitudinal EMR data with deep learning can detect individuals at high risk of developing diabetes for early intervention that could delay or even prevent the onset of diabetes. State of the art machine learning algorithms which are reported on extensively in the literature for predictive analysis cannot capture long term sequences and temporal relations. It is worth noting that, of all the examined state of the art machine and deep learning models, deep learning models (LSTM, CNN and CNN-LSTM) outperform the baseline machine learning models (Table 3) due to their distinctive potential to extract temporal relations. In contrast to widely used machine learning models, LS+TM has greater potential to extract complex information from time series data due to their hierarchical structure. Moreover, the feedback connection in LSTM helps to capture the sequential information in data and to forecast the future based on past data. The gating structure of LSTM controls the flow of information into the cell and provides a memory for long term dependencies in time series data. It is thus that deep learning models like LSTM can better utilize temporal features of EMR data than traditional machine learning models and could be used to enhance other clinical predictive tasks. Furthermore, of all the base line machine learning models, SVM also has considerable predictive competency. There are two main benefits of using deep learning models for the prediction of diabetes. The first is the ability to take into account the temporal nature of risk, which accumulates over time to predict diabetes with a higher accuracy. The second is the ability of the model to work when limited data may be available. The proposed method shows a less than 5% decrease in accuracy when the size of the training data is decreased from 90% to 5%. This has implications for predicting diabetes with higher accuracy in situations when data is limited. Limitations of the study include lack of socio-economic data, family history, dietary habits, physical activity, sleep patterns, psychosocial stress levels and microbiome data, which are known factors in the development of obesity and diabetes."
15,"Artificial Intelligence: The Future for Diabetes Care Artificial intelligence (AI) is a fast-growing field and its applications to diabetes, a global pandemic, can reform the approach to diagnosis and management of this chronic condition. Principles of machine learning have been used to build algorithms to support predictive models for the risk of developing diabetes or its consequent complications. Digital therapeutics have proven to be an established intervention for lifestyle therapy in the management of diabetes. Patients are increasingly being empowered for self-management of diabetes, and both patients and health care professionals are benefitting from clinical decision support. AI allows a continuous and burden-free remote monitoring of the patient's symptoms and biomarkers. Further, social media and online communities enhance patient engagement in diabetes care. Technical advances have helped to optimize resource use in diabetes. Together, these intelligent technical reforms have produced better glycemic control with reductions in fasting and postprandial glucose levels, glucose excursions, and glycosylated hemoglobin. AI will introduce a paradigm shift in diabetes care from conventional management strategies to building targeted data-driven precision care.
 Applications
Automated retinal screening. Deep learning algorithms have been developed to automate the diagnosis of diabetic retinopathy.22 AI-based screening of retina is a feasible, accurate, and well-accepted method for the detection and monitoring of diabetic retinopathy. A high sensitivity and specificity of 92.3% and 93.7%, respectively, have been reported for automated screening of the retina. Patient satisfaction for automated screening is also high with 96% patients reported as being satisfied or very satisfied with this method.23 Convolutional neural networks (CNN) have been trained on limited data sets to generate lesion-specific probability maps for hemorrhages, microaneurysms, exudates, neovascularization, and normal appearance in the retina.24

Clinical decision support. Supervised machine learning-based clinical decision support tools have been developed to predict short- and long-term HbA1c response after insulin initiation in patients with type 2 diabetes mellitus. These tools also help to identify clinical variables that can influence a patient's HbA1c response. The elastic net regularization-based generalized linear model based on baseline HbA1c and estimated glomerular filtration rate is reported to reliably predict the HbA1c response after insulin initiation. Areas under the curve (AUC) of 0.80 (95% confidence interval [CI] 0.78–0.83) and 0.81 (95% CI 0.79–0.84), respectively, are reported for short. and long.term HbA1c response.25 Machine learning has been used to develop an intuitive approach for customizing interventions in medication adherence and predicting the risk of hospitalization in diabetes. In a retrospective cohort study (n = 33,130), machine learning yielded adherence thresholds of 46% to 94% as most discriminating for risk of all-cause hospitalization. This study confirmed the variability of predictive adherence thresholds according to patient characteristics and complexity of medications.26

Predictive population risk stratification. Healthcare recommendation system (HRS) using machine learning helped to predict the risk for a disease, including diabetes, by analyzing patient's lifestyle, physical health factors, mental health factors, and their social network activities. Data from 68,994 healthy people and patients with diabetes has been used as a training data set for using decision tree, random forest, and neural networks to predict diabetes with high accuracy (accuracy = 0.8084 with all attributes).27

Predictive models have been built to leverage big data analytics for building estimates of possibility of development of complications in patients with diabetes. Many such models have been developed to predict the development of both long-term (eg, retinal, cardiovascular, and renal) and short-term (ie, hypoglycemia) complications of diabetes.28 Trained to interpret images of feet, mobile apps have been introduced to follow-up with patients with diabetes for development of diabetic foot ulcers.29

Machine learning has been used to develop decision tree models for prediction of development of type 2 diabetes mellitus in pregnant women with gestational diabetes. The discriminative power of this prediction method was 83.0% in the training set and 76.9% in an independent testing set, making it superior to conventional monitoring of fasting glucose levels.30

Genomics. Advanced molecular phenotyping, genomics, epigenetic alterations, and development of digital biomarkers is a new advance in the diagnosis and management of disease conditions.31 These can be applied to diabetes where huge data sets are generated owing to the heterogenous nature and chronic course of the disease. Microbiome data has been used to build a repository of microbial marker genes that can be used to predict the possibility of development of diabetes and guide the treatment in patients with confirmed diabetes.32 Genome-wide association studies have identified more than 400 signals that could potentially establish the genetic susceptibility to diabetes.33 Convolutional neural networks models have been trained on multiple genome-wide mapping and regulatory epigenomic annotations available for pancreatic islets to predict regulatory variants for refining the signals associated with diabetes.34

Patient self-management tools. Self-management is the key to the treatment of diabetes. With the advent of AI, patients are empowered to manage their own diabetes, generate data for their own parameters, and be their own experts for health.

Increased awareness: Digital platforms allow a targeted education of patients with diabetes. Awareness and knowledge about eating habits and activity patterns are now available through web-based programs and mobile phone and smartphone apps.35 This has been particularly useful in the management of diabetes in pregnant women. A web-based intervention was reported to increase the knowledge of gestational diabetes and proved to be a good adjunct to the management of diabetes in 21 women.36

Self-treatment: AI allows patients with diabetes to take daily decisions for diet and activity. Apps have been used to allow patients to assess the quality and calorie value of food intake. Accountability for diabetes care is enhanced when patients capture a picture of their own food and assess what they eat.37

Digital therapeutics has been evaluated in the management of diabetes. In a 12-week interventional study, 118 adults with type 2 diabetes mellitus received a digital intervention (FareWell) via an app and a digitally delivered specialized human support in the form of coaching every 2 weeks via telephone. The aim of the intervention was to evaluate a sustainable shift to plant-based diet and regular exercise. All patients had HbA1c >6.5% at baseline and 28% patients achieved HbA1c <6.5% at the end of the study. At 12 weeks, >86% participants were still using the app, and a total of 57% achieved a composite outcome of reducing HbA1c, reducing diabetic medication use, or both. Patients showed good acceptance for the app, with 92% reporting greater confidence in the management of their diabetes compared to that prior to participating in the study.38

The One Drop Mobile app was designed to help schedule medication reminders, view statistics, set goals, track health outcomes, and get data-driven insights in patients with type 1 and type 2 diabetes. Total of 1288 patients reported a 1.07% to 1.27% absolute reduction in HbA1c during a median 4 months of using the app. The use of One Drop Mobile app for tracking self-care was associated with improved HbA1c in patients with diabetes.39

Other applications. Telehealth has revolutionized the management of diabetes. Remote monitoring reduces the time spent in follow-up visits and allows a more real-time monitoring of the glycemic status as well as the overall health of the patient. AI has the ability to replace 50%-70% of routine follow-up clinical consultations with virtual engagements and remote monitoring.12 Short message service (SMS) text messaging are being tested for improvement in medication adherence in a randomized control trail in over 800 patients with type 2 diabetes mellitus in sub-Saharan Africa.40

Other devices. Diet and exercise are the initial and effective strategies to prevent type 2 diabetes mellitus in high-risk individuals.41 Various apps have been designed that provide customized dietary plans and schedules and suggest alterations in food intake to suit an individual's lifestyle. Daily activity levels can be tracked by wearables that record step counts and time and intensity of other activities.42 Wearable devices are effective facilitators of changes in behavior toward health.43 These devices enable tracking of daily activity and can motivate an individual to include a targeted activity into routine to prevent chronic diseases, including type 2 diabetes mellitus.

Several apps are also designed to analyze the image of food and provide details of the nutrient and calorie value of food. These apps can help to keep a check on body weight and prevent obesity, an established predecessor to type 2 diabetes mellitus.42 Web-based programs provide knowledge about diet and physical activity and patients can log in their daily intake and activity data and gain continuous feedback.35

End Users
The end users of technical advances in diabetes care include health care professionals in hospitals, diabetes management centers, and research institutes. Electronic health records (EHR) allow consistent and homogenous data capture and increased access to data. EHRs are being tapped as data repositories to train and develop algorithms for the prediction, detection, and management of diabetes.44 Patients represent a significant end user of AI-based advances for the management of diabetes and have embraced technical advances with fervor. Technical advances have simplified the management of diabetes and enabled patients to efficiently operate and execute the required management strategies. Better glycemic control is reported with the use of mobile apps in the management of type 2 diabetes mellitus. In a systematic review of 14 studies (n = 1360), the use of mobile apps resulted in a mean reduction in HbA1c of 0.49% (95% Cl 0.30, 0.68; I2 = 10%) when compared with controls.45 In another recent meta-analysis of 21 studies (n = 1550), mean HbA1c reductions of 0.49% (95% CI, 0.04-0.94; I2 =_84%) and 0.57% (95% CI, 0.32-0.82; I2 =_77%) were reported for type 1 and type 2 diabetes, respectively.46

Limitations of Artificial Intelligence
The application of AI in diabetes care has several limitations.

Human factors. Factors influencing the use of AI in diabetes care have been evaluated in some studies. In a meta-analysis of 14 randomized control trials, younger patients were reported to attain greater benefits from mobile apps for diabetes care and the effect size was enhanced with health care professional feedback.45 AI can pose a risk of de-skilling physicians by introducing dependence. This may introduce a vicious cycle of inadequate accuracy because AI in itself requires periodic refinements by experts.9

Technical factors. Barriers for the use of AI in diabetes care include cost, access, and implementation. With a growing array of devices and apps, interoperability is reported as a common potential barrier to their use in diabetes management.12

Limitations of data. Paucity of supporting data to build logical and accurate algorithms is a common challenge in diabetes care. Data sets will need to be more mature and structured to inform digital applications to construct impactful solutions. Concerns about security and data protection and regulatory concerns are also limiting the seamless adoption of technology in diabetes care.

Limitations of design. Current models and applications of AI in diabetes care have been validated using retrospective data sets. Prospective validation of these technical advances holds promise for automating diabetes care.9 Endpoints in clinical studies will need to be redefined to include the digital biomarkers and data from apps and monitors and activity trackers.

Summary
AI is attracting attention for the management of diabetes. AI enables us to rethink diabetes and redefine the strategies for prevention and management of diabetes.

AI supports the development of prediction models to estimate the risk of diabetes and its related complications. This will help to bring in an element of personalized care in the management of diabetes. Patients are now being empowered to manage their own health and physicians can provide a timely and targeted intervention through technical platforms. These advances save time and cost because data can be collected remotely and virtual management is replacing the routine visits to a clinic.

AI has introduced a quantum change in diabetes care and will continue to evolve. Going further, broader experience generated from the continuous use of AI will help to standardize the functionality and utility in diabetes care.
"
16,"Clinical Decision Support System for Diabetic Patients by Predicting Type 2 Diabetes Using Machine Learning Algorithms Diabetes is one of the most serious chronic diseases that result in high blood sugar levels. Early prediction can significantly diminish the potential jeopardy and severity of diabetes. In this study, different machine learning (ML) algorithms were applied to predict whether an unknown sample had diabetes or not. However, the main significance of this research was to provide a clinical decision support system (CDSS) by predicting type 2 diabetes using different ML algorithms. For the research purpose, the publicly available Pima Indian Diabetes (PID) dataset was used. Data preprocessing, K-fold cross-validation, hyperparameter tuning, and various ML classifiers such as K-nearest neighbor (KNN), decision tree (DT), random forest (RF), Naïve Bayes (NB), support vector machine (SVM), and histogram-based gradient boosting (HBGB) were used. Several scaling methods were also used to improve the accuracy of the result. For further research, a rule-based approach was used to escalate the effectiveness of the system. After that, the accuracy of DT and HBGB was above 90%. Based on this result, the CDSS was implemented where users can give the required input parameters through a web-based user interface to get decision support with some analytical results for the individual patient. The CDSS, which was implemented, will be beneficial for physicians and patients to make decisions about diabetes diagnosis and offer real-time analysis-based suggestions to improve medical quality. For future work, if daily data of a diabetic patient can be put together, then a better clinical support system can be implemented for daily decision support for patients worldwide.
 In this research, several ML approaches were applied for the classification of the PID dataset. The accuracy of the algorithms which were applied to the raw dataset is shown in Table 2.
It can be seen that (from Table 1) the dataset contains zero values in some attributes, such as BMI, glucose, BP, and insulin. But in real life, this cannot be possible. So, the irrelevant zero values are replaced by the mean value of the individual column values. After that, different scaling methods are applied to the dataset to improve the accuracy, and the results are shown in Table 3.
After applying various scaling methods, the comparison of different ML classifier models is evaluated. From the information in Table 3, the KNN model provides the highest accuracy among all other classifiers. The accuracy was 84.02% for the MinMax scaling technique when the value of k was 11, and the p value was 2 for the KNN algorithm. The lowest accuracy of 73.96% was observed using the DT classification method. On the other hand, it can be seen that HBGB has no impact on the applications of scaling methods. However, different scaling approaches definitely had distinct effects on the classification algorithms, which is helpful in augmenting the model accuracy through a trial-and-error technique. In the final phase, the rule-based approaches were implemented [30–32] to the dataset. This time the zero values were replaced by the median value of the corresponding column. The rules are given in Table 4.
After applying rules, the accuracy was significantly improved compared to the last phase, which is shown in Table 5.
Table 5 and Figure 4 represent that HBGB provided the highest accuracy, and the other performance metrics, such as precision (Figure 5), sensitivity or recall (Figure 6), and F1 score (Figure 7) are also very prominent for HBGB compared to other algorithms used in this study. For this method, the 5-fold cross-validation technique [27] and hyperparameter tuning technique were also applied. For the hyperparameter, the best max iter value was 100, and the best learning rate was found at 0.04. It is also seen that the DT model provides good accuracy as well. On the other hand, SVM provided the lowest accuracy in this method. As HBGB had shown the highest performance, this algorithm was selected to predict diabetes for our prediction system. Important features in Figure 8 are also determined to make the analytics graph based on these features [33].
In this research, an expert system is presented to help physicians as well as patients to make decisions about diabetes diagnosis and to offer real-time analysis-based suggestions to improve medical quality. The time-consuming identification process leads to a patient's appointment at a diagnostic center and consultation with a doctor. For predicting diabetes, several ML classification techniques were applied with different scaling methods. A rule-based approach with the HBGB classifier provides the highest accuracy of 92.21%. Since the best result for our prediction system was obtained from HBGB, this algorithm can be used for the proposed CDSS. This proposed system will also be very beneficial for the nondiabetic patient as it will show a comparative analysis of different parameters that are directly responsible for diabetes disease. For the automation of diabetes analysis, the work can be expanded and enhanced. In future work, feature selection techniques can be implemented to check the effects on accuracy improvement with various subsets of features. It is also planned to collect data from many regions across the globe in the future to create a more accurate and broader predictive model for diabetes decisions. If daily data of diabetes patients can be put together, then a clinical support system can be implemented for daily decision support for patients worldwide.
"
17,"Comparative study on risk prediction model of type 2 diabetes based on machine learning theory: a cross-sectional study. Objectives To compare the prediction effects of six models based on machine learning theories, which can provide a methodological reference for predicting the risk of type 2 diabetes mellitus (T2DM).

Setting and participants This study was based on the monitoring data of chronic disease risk factors in Dongguan residents from 2016 to 2018. The multistage cluster random sampling method was adopted at each monitoring site, and 4157 people were finally selected. In the initial population, we excluded individuals with more than 20% missing data and eventually included 4106 subjects.

Design K nearest neighbour algorithm and synthetic minority oversampling technique were used to process the data. Single factor analysis was used for preliminary selection of variables. The 10-fold cross-validation was used to optimise the parameters of some models. The accuracy, precision, recall and area under receiver operating characteristic curve (AUC) were used to evaluate the prediction effect of models, and Delong test was used to analyse the differences of AUC values of each model.

Results After balancing data, the sample size increased to 8013, of which 4023 are patients with T2DM and 3990 in control group. The comparison results of the six models showed that back propagation neural network model has the best prediction effect with 93.7% accuracy, 94.6% accuracy, 92.8% recall and the AUC value of 0.977, followed by logistic model, support vector machine model, CART decision tree model and C4.5 decision tree model. Deep neural network has the worst prediction performance, with 84.5% accuracy, 86.1% precision, 82.9% recall and the AUC value of 0.845.

Conclusions In this study, six types of risk prediction models for T2DM were constructed, and the predictive effects of these models were compared based on various indicators. The results showed that back propagation neural network based on the selected data set had the best prediction effect. According to the 2021 survey results of the International Diabetes Association, the number of patients with diabetes in China has reached 140.9 million, making China the country with the largest number of patients with diabetes in the world.2 Global diabetes-related health expenditures were estimated at US$966 billion in 2021, and are projected to reach US$1054 billion by 2045.2 More and more researchers have realised that the early diagnosis or prediction of DM through model is of great significance for the prevention of T2DM, the improvement of life quality of patients with T2DM and the prevention of related complications.

In recent years, machine learning techniques have been widely used to predict the risk of T2DM. Logistic regression, as a classical classification model, has the advantages of low calculation cost, easy implementation and understanding, and is suitable for solving the binary classification problem of relatively simple datasets. For researchers in the field of clinical medicine, most of them are still inclined to use regression-based methods for disease correlation analysis in order to obtain an intuitive and clear result under limited conditions. Logistic regression is essentially a linear classifier. If the relationship between data is linearly separable, this traditional method will be more suitable. However, it has poor effect on data processing of multiple categories of related features, which may oversimplify the complex relationship between factors with non-linear interaction, leading to potential loss of important relevant information. However, diagnosis models based on machine learning, such as BP neural network, SVM, decision tree and DNN, are often more suitable for classification tasks with multidimensions and large sample sizes. They have better predictive performance in solving non-linear problems and better fitting ability for complex datasets, which shows the importance of selecting appropriate models according to the characteristics of datasets.

Hong12 built a diabetes prediction model based on BP neural network, SVM and integrated learning, and the results showed that the prediction effect of BP neural network was better than that of SVM. Gao13 used BP neural network and logistic regression to construct the prediction model of T2DM complications, and the study found that the prediction effect of BP neural network was higher than that of logistic regression. Liu et al14 constructed logistic model, BP neural network model and decision tree model to analyse the risk factors of T2DM, and the results showed that the prediction effects of the three models from high to low were BP neural network, logistic regression model and decision tree model, respectively. Dwivedi15 used six algorithms of classification trees, SVM, ANN, NB, logistic and KNN to predict diabetes, and the results showed that the prediction effect of logistic regression model was better than that of SVM. The above conclusions are consistent with the results of this study. However, the BP neural network model also has certain limitations, as it cannot determine the direction of variables and cannot determine whether variables are protective or risk factors. Although the predictive performance of the logistic model is weaker than that of the BP neural network, it has strong interpretability for the results and can reflect the relationship between various factors and T2DM.

As a simple and easy to use non-parametric classifier, decision tree model has high computational speed and robustness. The SVM model overcomes the problem of dimensionality disaster and non-linear divisibility by using kernel function method, and has strong generalisation ability, which can deal with high dimensional problems. The results of this study showed that the prediction effect of SVM and CART was similar, and the difference was not statistically significant. However, Faruque et al16 and Kandhasamy and Balamurali17 found that the prediction effect of C4.5 decision tree model was significantly higher than that of SVM model. In practical application, compared with SVM model, CART decision tree model and C4.5 decision tree model can present the variables included in the model more intuitively. Cheruku et al18 built a prediction model for T2DM based on PIDD dataset by using a variety of machine learning methods. The results showed that the prediction effect of C4.5 decision tree model was better than that of CART decision tree model, and the accuracy was 74.2% and 70.7%, respectively. Althunayan et al19 compared the performance of nine algorithms such as Naive Bayes, C4.5, CART and random forest in the prediction of T2DM, and the results showed that the accuracy of random forest was the highest, and the accuracy of C4.5 algorithm was much higher than that of CART algorithm. Meng et al20 used ANN, logistic regression and C4.5 data mining technology to predict diabetes, and finally concluded that C4.5 machine learning technology is more effective and accurate than other methods. The results of this study showed that the prediction effect of CART decision tree model is slightly better than that of C4.5 decision tree model. The CART decision tree model is a binary tree, and compared with C4.5 decision tree model, its operation is faster and the model formed is more concise. Therefore, CART is more suitable for the processing of large sample data.

DNN is a kind of multilayer unsupervised neural network, which can represent complex non-linear problems more carefully and efficiently. Islam Ayon and Milon Islam21 used DNN method to build a prediction model for T2DM based on PIDD dataset, and the accuracy of the model reached 98.35%. Mohapatra et al22 applied DNN to the prediction of T2DM, and the accuracy of the algorithm was as high as 97.11%. The results of this study showed that compared with the other five T2DM prediction models, DNN had the worst prediction effect, with the accuracy of only 84.5% and the AUC value of 0.845. The results of this study are different from those of previous related studies, which may be caused by the differences in sample size, data quality, included characteristic variables, definition of related variables and construction techniques of different datasets. In a word, the quality of the model is affected by data characteristics, training time, parameter complexity and other factors, so the most appropriate model should be selected according to specific problems.

In addition, screening and early diagnosis and treatment of high-risk T2DM groups are effective measures to reduce their mortality. Based on the results of various model construction, it is recommended to implement targeted T2DM screening for people aged 34 and above, as well as comprehensive smoking and alcohol cessation. For people aged 54 and above, comprehensive screening should be carried out. Early intervention measures should be implemented for the identified high-risk T2DM population, and high attention should be paid to risk factors that can be changed by behaviour, such as controlling sugar and oil, reducing the intake of red and processed meat, reducing the frequency of potato consumption, supplementing dietary fibre, etc, and strengthening the detection of fasting and postprandial blood glucose levels, so as to reduce the incidence of T2DM.

In conclusion, the T2DM disease risk prediction model based on BP neural network has the best prediction in this study, which can efficiently identify patients with T2DM. As a simple prediction model, it can be used in clinical practice and has certain guiding significance and clinical application value. The logistic regression model is second only to BP neural network in predicting effect, but it is highly explanatory to the results and can intuitively show the relationship between variables in the model, which has better operability and feasibility in clinical application. Although the decision tree model can intuitively present the classification process, it belongs to the black box algorithm just like BP neural network and DNN, and the specific relationship between independent variables and dependent variables is unknown, so it is often necessary to adopt other means to evaluate the importance of each variable in the model. Therefore, each model has its own advantages and disadvantages, and the results obtained can complement each other, suggesting that when applying classifiers in clinical decision-making, we can select appropriate classifiers according to research purposes, so as to obtain the highest value in practice.

This study still has certain limitations: (1) This prediction model has only been verified internally and lacks external verification. It will be verified in larger population samples in different regions in the future. (2) Due to the condition limitation, there was only one fasting glucose reading per survey subject, and some subjects may have been misclassified as diabetic due to random error. (3) Due to the limitation of dataset, the failure to include common disease risk factors such as genetic inheritance and self-care status (physical activity, sleeping time, etc) into the model may affect the accuracy of prediction to some extent. (4) SMOTE was used to process the unbalanced data to reduce the risk of model overfitting. However, this algorithm cannot overcome the data distribution problem of unbalanced datasets and is prone to produce distribution marginalisation problem."
18,"Comparisons of the prediction models for undiagnosed diabetes between machine learning versus traditional statistical methods. We compared the prediction performance of machine learning-based undiagnosed diabetes prediction models with that of traditional statistics-based prediction models. We used the 2014–2020 Korean National Health and Nutrition Examination Survey (KNHANES) (N_=_32,827). The KNHANES 2014–2018 data were used as training and internal validation sets and the 2019–2020 data as external validation sets. The receiver operating characteristic curve area under the curve (AUC) was used to compare the prediction performance of the machine learning-based and the traditional statistics-based prediction models. Using sex, age, resting heart rate, and waist circumference as features, the machine learning-based model showed a higher AUC (0.788 vs. 0.740) than that of the traditional statistical-based prediction model. Using sex, age, waist circumference, family history of diabetes, hypertension, alcohol consumption, and smoking status as features, the machine learning-based prediction model showed a higher AUC (0.802 vs. 0.759) than the traditional statistical-based prediction model. The machine learning-based prediction model using features for maximum prediction performance showed a higher AUC (0.819 vs. 0.765) than the traditional statistical-based prediction model. Machine learning-based prediction models using anthropometric and lifestyle measurements may outperform the traditional statistics-based prediction models in predicting undiagnosed diabetes.
 We compared the prediction performance of the ML-based prediction models with that of the TS-based diabetes prediction models with an external validation set in a large representative sample of Korean adults, using self-reported clinical data. Our findings suggest that ML-based diabetes prediction models, regardless of the number of features used in developing models, were superior to TS-based prediction models5,6 using the diabetes risk score method8. When the feature selection method was employed in our ML-based model, the AUC was 0.819, which was better than the highest AUC (0.765) among TS-based models6.

Some assumptions explain why the ML-based diabetes prediction models used in this study were superior to the TS-based prediction models. First, the ML methods we used in our study were bagging34 and boosting35 algorithms22, which developed multiple prediction models, aggregated to determine the final prediction result. Since the final prediction result is determined by voting for various prediction results, an unbiased prediction result can be obtained23,36. Compared with a single prediction model, these methods result in a more accurate prediction34,35,36,37,38,39. Second, when compared to the ML-based approach, the TS-based approach5,6,8 is challenging for researchers to develop prediction models by considering all possible cases that may result from multiple features and algorithms. In contrast, an ML-based method can select the optimal features to maximize the prediction performance using feature selection algorithms40. In addition, by using hyperparameter tuners such as Optuna41 and Hyperopt42, it is possible to determine how many single prediction models are combined to develop a final prediction model to maximize prediction performance while avoiding overfitting. Our findings suggest that diabetes prediction models developed by the ML-based method may be more time-efficient, cost-effective, and superior to the previous TS-based method.

For these reasons, there is growing evidence for the application of the ML-based approach and artificial neural network, a type of ML, to develop prediction models for diabetes11,12,14,43,44. However, these prediction models11,12,43 may be less accessible because they were developed using blood lipid variables (e.g., fasting glucose, HbA1c, triglyceride, and total cholesterol). In addition, another study14 using XGBoost, an algorithm similar to our approach, reported a high AUC score of 0.92. However, this prediction model14 may be at high risk of overfitting45 given that the prediction model was developed without using the ‘external validation set’. In addition, the prediction performance of this model14 was not assured, given that there was no verified result for unseen data. On the other hand, our ML-based prediction model developed using non-invasive data may be more accessible. Furthermore, the external validity of our prediction model was tested from the external validation set and we used the SHAP analysis to determine the predictive power of each predictor (feature) and to generate explainable models, while the previous artificial neural network prediction model for undiagnosed diabetes44, deemed a black-box model, using non-invasive data (e.g., age, WC, body mass index, sex, smoking status, hypertension, and family history of diabetes) did not validate their model through the application of SHAP analysis.

In addition, the aforementioned prediction models only mentioned the prediction performance and did not explain the importance or effect of the features that contributed to the prediction results. Therefore, it was impossible to interpret the prediction models used in these studies. To address this limitation, the ML-based prediction model of this study calculated the contribution and effect of each feature using SHAP and presented it to interpret its prediction results. Additionally, the sensitivity of our prediction model using age, WC, and RHR was 83.3%, which may be sufficiently valid.

This study has several limitations. First, given the nature of the cross-sectional study design, we could not determine causality between the features and undiagnosed diabetes. Thus, future studies on diabetes prediction models should employ longitudinal cohort data to examine the temporal relationships between features and incident diabetes. Additionally, RHR is highly affected by sleep quality, smoking status, alcohol consumption, and/or major health characteristics; therefore, interpretation should be made with caution. Lastly, findings cannot be generalized to wider populations given that our study examined Korean data only. Thus, additional research with racially/ethnically diverse population data is needed to confirm our preliminary findings.

In conclusion, our study suggests that ML-based undiagnosed type 2 diabetes prediction models may improve the prediction performance of TS-based prediction models and methods. The continuous increase in the number of diagnosed and undiagnosed diabetes epidemics is a major public health concern. The study findings will inform public health researchers and healthcare professionals to apply efficient new diabetes prediction models for the prevention of diabetes and its adverse health consequences. A clear next step in future research is to identify our preliminary findings in a different setting of data with wider populations in order to better generalize the findings."
19,"Data collaboration analysis in predicting diabetes from a small amount of health checkup data. Recent studies showed that machine learning models such as gradient-boosting decision tree (GBDT) can predict diabetes with high accuracy from big data. In this study, we asked whether highly accurate prediction of diabetes is possible even from small data by expanding the amount of data through data collaboration (DC) analysis, a modern framework for integrating and analyzing data accumulated at multiple institutions while ensuring confidentiality. To this end, we focused on data from two institutions: health checkup data of 1502 citizens accumulated in Tsukuba City and health history data of 1399 patients collected at the University of Tsukuba Hospital. When using only the health checkup data, the ROC-AUC and Recall for logistic regression (LR) were 0.858 ± 0.014 and 0.970 ± 0.019, respectively, while those for GBDT were 0.856 ± 0.014 and 0.983 ± 0.016, respectively. When using also the health history data through DC analysis, these values for LR improved to 0.875 ± 0.013 and 0.993 ± 0.009, respectively, while those for GBDT deteriorated because of the low compatibility with a method used for confidential data sharing (although DC analysis brought improvements). Even in a situation where health checkup data of only 324 citizens are available, the ROC-AUC and Recall for LR were 0.767 ± 0.025 and 0.867 ± 0.04, respectively, thanks to DC analysis, indicating an 11% and 12% improvement. Thus, we concluded that the answer to the above question was “Yes” for LR but “No” for GBDT for the data set tested in this study.
 In this study, we used DC analysis to analyze the data held by different institutions to build a model for predicting the future of diabetes. Consequently, the prediction accuracy of diabetes was improved by expanding the number of samples through DC analysis for LR but not for GBDT. For LR, the averages were improved using DC analysis for almost all metrics in all datasets when compared to the individual (raw data) analysis as summarized in Table 3a as well as when compared to the middle regression analysis as summarized in Table 3b. In addition, for AUC and Recall in particular, more significant improvement was observed when the data set was “Quarter” or “Half” data set.

However, as mentioned above and seen from Table 3a, GBDT was inferior to the individual (raw data) analysis for most data sets and associated metrics. There are three reasons for this. First, no parameter tuning was performed on the machine learning methods in this experiment. Specifically, there is no limit on the depth, the learning rate is set to 0.1, and the number of estimators is set to 100. More sophisticated parameter tuning, such as limiting the depth of GBDT, is a promising future direction. Second, the combination of GBDT and the data sets used in this study may be incompatible with the intermediate representation method. Recall that the intermediate representation method, PCA in this study, is employed to process raw data for confidential data sharing in DC analysis. Since the middle regression analysis is the counterpart of the individual analysis for the processed data, the performance of the middle regression analysis basically gets worse than the individual analysis (for raw data) as can be seen from Fig. 2. On the other hand, the performance of DC analysis basically gets better than the middle regression analysis thanks to the data sharing as can be seen from Table 3b at least for AUC and Recall even for GBDT. The point is how much the middle regression analysis gets worse than the individual analysis. We see from Fig. 2 that the gap between performances of the individual (raw data) analysis and the middle regression analysis is slight for LR but significant for GBDT. We note that this problem might be specific to the data set used in this study; for example, Imakura et al.23 showed that DC analysis improves the performance of decision tree using 12 data sets including artificial one and those from UCI machine learning repository. We are still not sure when the aforementioned gap between the individual (raw data) analysis and the middle regression analysis can get small theoretically nor empirically. However, in practice, what we need to do is to observe the gap in the individual analysis, before getting into DC analysis, by changing the method for intermediate representation to find out the best one. However, promising methods, including SVD (Singular value decomposition) and LPP (Locality preserving projection) as well as PCA, perform all similarly in the preliminary experiment of this study. Searching for effective methods for intermediate representation is a future problem. Third, in terms of sample size, Seto et al. concluded that GBDT showed better AUC than LR when the sample size exceeded 10,000. However, since the largest data set in this experiment is only approximately 1500, GBDT may show a higher AUC than LR if this values increases.

One limitation of this study was the question of labeling. When we built the model to predict diabetes within 3 years, we defined the target patients as those not diagnosed with diabetes at baseline and had FPG and HbA1c levels at least once within 3 years. If the number of patients was limited to those with these two values in all 3 years, the number would be 164 in the city health checkup data and 724 in the hospital health history data, and if we built the model as in this analysis, there would be no data for 2016 and 2018, only for 2017."
20,"Data-driven modeling and prediction of blood glucose dynamics: Machine learning applications in type 1 diabetes Background: Diabetes mellitus (DM) is a metabolic disorder that causes abnormal blood glucose (BG) regulation that might result in short and long-term health complications and even death if not properly managed. Currently, there is no cure for diabetes. However, self-management of the disease, especially keeping BG in the recommended range, is central to the treatment. This includes actively tracking BG levels and managing physical activity, diet, and insulin intake. The recent advancements in diabetes technologies and self-management applications have made it easier for patients to have more access to relevant data. In this regard, the development of an artificial pancreas (a closed-loop system), personalized decision systems, and BG event alarms are becoming more apparent than ever. Techniques such as predicting BG (modeling of a personalized profile), and modeling BG dynamics are central to the development of these diabetes management technologies. The increased availability of sufficient patient historical data has paved the way for the introduction of machine learning and its application for intelligent and improved systems for diabetes management. The capability of machine learning to solve complex tasks with dynamic environment and knowledge has contributed to its success in diabetes research.

Motivation: Recently, machine learning and data mining have become popular, with their expanding application in diabetes research and within BG prediction services in particular. Despite the increasing and expanding popularity of machine learning applications in BG prediction services, updated reviews that map and materialize the current trends in modeling options and strategies are lacking within the context of BG prediction (modeling of personalized profile) in type 1 diabetes.

Objective: The objective of this review is to develop a compact guide regarding modeling options and strategies of machine learning and a hybrid system focusing on the prediction of BG dynamics in type 1 diabetes. The review covers machine learning approaches pertinent to the controller of an artificial pancreas (closed-loop systems), modeling of personalized profiles, personalized decision support systems, and BG alarm event applications. Generally, the review will identify, assess, analyze, and discuss the current trends of machine learning applications within these contexts.

Method: A rigorous literature review was conducted between August 2017 and February 2018 through various online databases, including Google Scholar, PubMed, ScienceDirect, and others. Additionally, peer-reviewed journals and articles were considered. Relevant studies were first identified by reviewing the title, keywords, and abstracts as preliminary filters with our selection criteria, and then we reviewed the full texts of the articles that were found relevant. Information from the selected literature was extracted based on predefined categories, which were based on previous research and further elaborated through brainstorming among the authors.

Results: The initial search was done by analyzing the title, abstract, and keywords. A total of 624 papers were retrieved from DBLP Computer Science (25), Diabetes Technology and Therapeutics (31), Google Scholar (193), IEEE (267), Journal of Diabetes Science and Technology (31), PubMed/Medline (27), and ScienceDirect (50). After removing duplicates from the list, 417 records remained. Then, we independently assessed and screened the articles based on the inclusion and exclusion criteria, which eliminated another 204 papers, leaving 213 relevant papers. After a full-text assessment, 55 articles were left, which were critically analyzed. The inter-rater agreement was measured using a Cohen Kappa test, and disagreements were resolved through discussion.

Conclusion: Due to the complexity of BG dynamics, it remains difficult to achieve a universal model that produces an accurate prediction in every circumstance (i.e., hypo/eu/hyperglycemia events). Recently, machine learning techniques have received wider attention and increased popularity in diabetes research in general and BG prediction in particular, coupled with the ever-growing availability of a self-collected health data. The state-of-the-art demonstrates that various machine learning techniques have been tested to predict BG, such as recurrent neural networks, feed-forward neural networks, support vector machines, self-organizing maps, the Gaussian process, genetic algorithm and programs, deep neural networks, and others, using various group of input parameters and training algorithms. The main limitation of the current approaches is the lack of a well-defined approach to estimate carbohydrate intake, which is mainly done manually by individual users and is prone to an error that can severely affect the predictive performance. Moreover, a universal approach has not been established to estimate and quantify the approximate effect of physical activities, stress, and infections on the BG level. No researchers have assessed model predictive performance during stress and infection incidences in a free-living condition, which should be considered in future studies. Furthermore, a little has been done regarding model portability that can capture inter- and intra-variability among patients. It seems that the effect of time lags between the CGM readings and the actual BG levels is not well covered. However, in general, we foresee that these developments might foster the advancement of next-generation BG prediction algorithms, which will make a great contribution in the effort to develop the long-awaited, so-called artificial pancreas (a closed-loop system). 5.1. Principal findings
Recently, machine learning has received wider attentions for modeling and the prediction of BG dynamics. Regardless of its popularity, no recent reviews have analyzed and reflected on the current development. As far as our knowledge is concerned, no review has focused mainly on modeling and prediction of BG dynamics using machine learning techniques. Therefore, the purpose of this review is to assess and analyze the state-of-the-art machine learning applications in BG prediction pertinent to the controller of an artificial pancreas (a closed-loop system), modeling of a personalized profile, personalized decision support systems, and BG alarm event applications. It serves as a compact guide regarding modeling options and strategies of machine learning applications and their hybrid system in type 1 diabetes. However, some recent reviews have been conducted on diabetes in general and on BG prediction in particular [[11], [12], [13]]. For instance, Oviedo et al. [12] recently conducted a methodological review on BG prediction, focusing mainly on a closed-loop system (an artificial pancreas), which assessed a variety of approaches, including physiological models, data-driven models, and hybrid approaches. Kavakiotis et al. [11] also performed a systematic review of the applications of machine learning and data mining techniques in diabetes research in the context of diabetes prediction and diagnosis, diabetes complications, genetic background and environment, and healthcare and management.

BG dynamics are affected by numerous factors, such as history of BG values, insulin medication, physical activity, and dietary intake. Moreover, they are also affected by other factors, such as an individual’s body mass index, stress level, amount of sleeping time, presence of illness, some medications, smoking habit, periods (menstruation), alcoholism, allergies, and altitude. In principle, a successful BG predictor is expected to incorporate as much information as possible to effectively track and predict BG levels. However, due to the complexity of BG dynamics, it remains difficult to achieve an accurate prediction in every circumstance. Most of the available BG prediction algorithms have their own limitations, working better in some specific circumstances. The reported BG prediction algorithms have explored various classes of machine learning, input parameters, and training algorithms. Most of these studies have neglected the effect of physical activity on BG dynamics, and only a few studies have considered the effect of patient uncontrollable parameters, such as stress, illness, and others. Generally, the reported BG prediction algorithms can be categorized under different scenarios, such as real time (online) versus offline, the age group (children, adult, and old), BG regions (hypo/eu/hyperglycemia events), the time of day (diurnal vs. nocturnal), generalizability (generic vs. specific), free-living versus non-free-living conditions, and the evaluation approach (in vivo vs in silico). Accordingly, most researchers have considered separate age groups, which are typically related with the dynamics and active lifestyles adopted by each group. Few attempts have been made to develop real-time algorithms that perform under free-living conditions. Moreover, most of the reported algorithms perform better in either of these BG regions (hypo/eu/hyperglycemia events). Furthermore, most of the algorithms rely on in silico evaluations, which further put the clinical significance in question. Also, the lack of a well-defined approach to estimate carbohydrate intake is an issue; it is mainly done manually by the individual users and is prone to an error that can severely affect the predictive performance. The lack of a universal approach to estimate and quantify the approximate effect of physical activities, stress, and infection incidence on the BG level is another challenge. For instance, regarding physical activity integration, a wide variety of approaches have been proposed, such as using a scale and level code to quantify the degree and duration of physical activity, the sum of energy expenditures during an interval of time, physical activity caloric use based on standard table, MET, exercise compartmental models, a proprietary algorithm to estimate the physical activity energy expenditure by combining different physiological signals such as transversal acceleration (a measure of movement), heat flux (the average heat dissipated or absorbed by the arm), longitudinal acceleration (measure of movement), skin and near-body temperature, and galvanic skin response (electrical conductivity between two points on the arm). It seems that almost all the studies have followed quite different approaches, and this poses a challenge in regarding one’s approach as universal. In addition, also few studies have been done regarding model portability that can capture the inter- and intra-variation among the patients. It also seems that the effect of time lags between the CGM reading and the actual BG levels is not well covered. Generally, the review indicates the lack of a one-fits-all algorithm that performs better under totally free-living conditions.

Any successful BG prediction algorithm should at least consider both the patient’s controllable parameters (BG, insulin, diet, physical activity, and others) and patient uncontrollable parameters (stress, infections, medications, and others). Moreover, it is also necessary to consider any relevant contextual information, such as intra- and inter-variability among the patient’s changes in lifestyles, the time of day (diurnal vs nocturnal) and others. Future researchers need to reflect on a longer prediction horizon (giving more response time) with reasonable clinical accuracy, approaches to improve time lags from the CGM, real-time capability under free living conditions, and a thorough validation using real patients (clinical trials) with ample subjects over a longer period of time. Moreover, the predictor should give proper weights and penalties for errors in hypoglycemia, euglycemia, and hyperglycemia regions. It is also necessary to consider a proper way to estimate the amount and effect of dietary consumption and physical activity during integration with the machine learning model. Stress and infections have a prominent effect on BG dynamics, which could in turn affect the predictor performances. To this end, it is necessary to test and assess the effects of a change in either lifestyle or physiology (infections) on the predictor performance using subjects who are monitored within different periods. Furthermore, the effect of different CGM devices on the quantitative performance of the prediction algorithms should be explored along with the associated time lag.

5.2. Summary of existing efforts (Machine learning techniques)
5.2.1. Artificial neural network (ANN)
An artificial neural network (ANN) is a computational model consisting of various processing elements known as neurons and a scaled connection between them called weights [94]. Various forms of artificial neural networks are used, but the network topology could be generally categorized as feed-forward networks (SLP, MLP, and radial basis function) and recurrent/feedback networks (Elman net, Kohonen’s SOM, and Hopfield Networks). The feed- forward network is the most common topology, where it consists of a connection between different neurons that are directed only in one direction (forward) from the earlier stage to the next level. Recurrent or feedback network topology involves at least one feedback loop in the architecture [94]. Both of these network topologies have been successfully employed in modeling and for the prediction of BG levels in type 1 diabetes patients. Regarding the feed-forward network, for example, Allam et al. and others [18,44,45] have developed a feed-forward neural network from CGM data using the back propagation Levenberg-Marquardt optimization training algorithm. Pappada et al. and others [46,47,65] have also proposed time-lagged feed-forward neural networks trained through a back-propagation gradient descent algorithm, which is capable of storing previous values of data within the network. Zainuddin et al. [29] have proposed a wavelet neural network, integrating different wavelet families as an activation function for modeling BG dynamics trained through pseudo-inverse with fixed parameter initialization. Zarkogianni et al. [27] developed a seven-layer neuro-fuzzy network using wavelets as an activation function and Gaussian function as a membership function trained through a gradient-based algorithm with an adaptive learning rate. Compared to these shallow networks, Mhaskar et al. [52] proposed a semi-supervised deep learning neural network with a judge predictor based on the function approximation on data-defined manifolds, using diffusion polynomials. Baghdadi et al. [19] implemented a radial basis function network using Gaussian function in the hidden layer neuron. Georga et al. [57] investigated the applicability of an extreme learning machine (ELM), specifically an online sequential ELM (OS-ELM) and online sequential ELM kernels (KOS-ELM) for training single hidden-layer feed-forward neural networks. In a surgical care setting, Pappada et al. [48] trained a feed-forward network from CGM data for bedside monitoring using a back-propagation training algorithm. Apart from the feed-forward network, recurrent or feedback networks have been utilized in BG prediction; that is, recurrent neural networks, autoregressive neural networks and self-organizing maps. For example, Daskalaki et al. [22] developed an online adaptive ANN-based model using a fully connected, multilayered ANN with two feedback loops trained through a teacher-forced, real-time, recurrent algorithm. Sandham et al. [38,42] and Robertson et al. [40] have used an Elman recurrent network trained through a backpropagation gradient descent algorithm with momentum and an adaptive learning rate and Levenberg-Marquardt algorithm, respectively. Alanis et al. [73,79] developed an autoregressive version of a neural network called neural network autoregressive external input (NNARX), which is trained through an extended Kalman filter (EKF) algorithm. Chernetsov et al. [21] performed a comparative analysis of three recurrent or feedback networks: the layer recurrent network (LRN), Elman net, and nonlinear autoregressive network (NARX-net). They investigated the effect of different learning algorithms, network architectures, prediction horizons, data sample sizes, and tapped delay line lengths on the performance of the network. Moreover, Zarkogianni et al. [26] conducted a comparative analysis of four machine learning techniques in the modeling of BG dynamics: a feed-forward neural network (FNN) trained through a backpropagation algorithm, a self-organizing map (SOM) achieved by applying a vector quantization method, a neuro-fuzzy network using wavelets as activation functions (WFNN), and a linear regression model (LRM). They used CGM data and also explored the effect of physical activity data collected from a SenseWear Armband. The study demonstrated the superiority of SOM and its ability to capture both the complexity of the dynamics and also the inter- and intra-variations among the patients [26].

5.2.2. Support vector machines (SVM), kernel function (KF), and gaussian process regression
Support vector machines have been widely exploited across a wide range of applications, such as pattern identification and recognition, categorization or classification, regression, and prediction [95]. Support vector regression (SVR) is the most widely used class of SVMs in BG prediction and modeling. In this regard, for example, Reymann et al. [41] investigated the applicability of BG prediction using a mobile platform based on SVR, with radial basis function (RBF) as a kernel. Moreover, Li et al. [54] tried to use pooled patient data to capture patient similarities, which led to the development of a personalized BG prediction model using smartphone-collected data based on SVR. Georga et al. [58] investigated the potential performance enhancement from using a feature ranking algorithm, random forests (RF), and RReliefF algorithms, where the predictor is based on SVR-exploiting Gaussian radial basis function (RBF) as a kernel.

As a solution to the artificial neural network requirement of larger training data and much more information to learn, Naumova et al. developed a novel fully adaptive regularized learning (FARL) approach using meta-learning to choose the kernels and regularization parameters in kernel-based regularization learning algorithms [35].

Gaussian process regression is a useful nonparametric regression tool that has been widely adopted in various applications, such as a vital-sign “early warning system,” patient physiological monitoring, disease prediction, and the discovery of biomarkers in microarray gene expression data. Tomczak et al. [32] investigated the applicability of Gaussian process regression in BG prediction coping with categorical inputs. The input consisted of the data, time, code (categorical), and BG level (numeric). The categorical code was used to describe the type of measurement (e.g., insulin dose, meal intake, physical exercise, pre-prandial BG measurement, and others). The covariance function was proposed to deal with the categorical inputs [32].

5.2.3. Genetic programming and genetic algorithms
An evolutionary algorithm (EA) is a biologically inspired approach to problem solving [96]. The two most used variants of EA in BG prediction and modeling approaches are genetic programming (GP) and genetic algorithms (GA). Hidalgo et al. [76,77] used a genetic programming-based symbolic regression known as grammatical evolution to develop an individualized model of BG dynamics. Moreover, Contreras et al. [71] used the grammatical evolution approach to develop a standalone BG prediction model. Furthermore, Hidalgo et al. [69] assessed the performance of different predictors, genetic programming, random forests, k-nearest neighbors, and grammatical evolution along with a new enhanced modeling algorithm, a variant of grammatical evolution that uses optimized grammar, and a variant of tree-based genetic programming that uses a three-compartment model for carbohydrate and insulin dynamics.

5.2.4. Random Forest (RF)
Random forests or random decision forests are an ensemble approach of learning for classification and regression applications, which learns by constructing a multitude of decision tress generating the mode of the class or the mean of prediction. In this regard, for example, Xao et al. [30] developed a random forest regression and support vector regression-based BG predictors and assessed the performance improvement gained through the selection of an optimal feature representative using a combined approach of feature importance scores of ensemble learning and a sequential backward selection (SBS) algorithm. Furthermore, Georga et al. [59] used a random forest regression to predict BG levels with a multivariate dataset containing a subcutaneous glucose profile, plasma insulin concentration, intestinal absorption of meal-derived glucose, and daily energy expenditure.

5.2.5. Hybrid approach
Hybridization involves combining two or more different approaches, either at the preprocessing, feature extraction, or learning stage when looking for improved performance. The majority of the BG prediction models involve the hybridization of physiological (compartmental) models along with different machine learning techniques. Regarding support vector regression, for example, Plis et al. [43] combined support vector regression along with a physiological model, where the latter generates informative input features to be used to train the SVR model. Furthermore, Georga et al. [[60], [61], [62], [63]] combined support vector regression with compartmental models, which are used to quantify the absorption of subcutaneously administered insulin, glucose from the gut following a meal, and the effects of exercise on plasma glucose and insulin dynamics. Regarding the hybridization of an artificial neural network with other approaches, some researchers have reported success in this direction. For example, Mougiakakou et al. [50] combined an artificial neural network with a compartmental model, where the latter is used to estimate the effect of food on BG levels and the influence of injected insulin on plasma insulin concentration; this output along with the previous BG measurements were used to train the ANN model. Mougiakakou et al. [51] further investigated the combination of a recurrent neural network along with three compartmental models, which estimated the effect of short-acting (SA) insulin intake on blood insulin concentration, intermediate-acting (IA) insulin intake on blood insulin concentration, and, carbohydrate intake on BG absorption from the gut. Zecchin et al. [23,24] combined an artificial neural network and a physiological model to exploit meal information to be used along with the CGM data. Moreover, Zecchin et al. [15,25] further explored the applicability of a jump neural network, which is feed by a meal physiological model and CGM data, and compared their result with a previously proposed artificial neural network [23,24]. Briegel et al. [74] explored a nonlinear state space model for modeling an individual BG dynamic using a compartmental model and an artificial neural network. Furthermore, Otto et al. [49] developed a hybrid model combining an artificial neural network and fuzzy logic, where the fuzzy logic was used to approximate food, insulin, and the level of exercise. Several researchers have attempted to hybridize genetic programming along with physiological models. For example, Contreras et al. [70] developed a hybrid model using a genetic programming-based algorithm known as grammatical evolution and a physiological model. Self-organizing maps (SOMs) have been used to develop a hybrid model along with a physiological model. For example, Zarkogianni et al. [28] used the physiological model to simulate the subcutaneous insulin kinetics and glucose absorption from the gut into the blood, which are in turn fed into the SOM. Jankovic et al. [56] developed a two-layer (prediction and correction layer) online adaptive personalized BG prediction model. The prediction layer consisted of an autoregressive model with external input (ARX) and an artificial neural network, which made the first estimates and then the output was further optimized in the second (correction) layer through an extreme learning machine (ELM).

5.2.6. Ensemble approach – merging different predictors for performance improvement
Due to the complexity of BG dynamics, it remains difficult to achieve an accurate prediction in every circumstance (i.e., hypo/eu/hyperglycemia events). One prediction model can have a better prediction power in either of these circumstances, and the other model can achieve better predictive power where the first model fails to accurately predict. Therefore, it is natural to look for opportunities to exploit the strengths from these different predictors to achieve better predictive power in most of these circumstances, which has led to ensemble approaches. An ensemble approach is generally favored when one is interested in merging two or more different predictors for improved performance. Various approaches have been taken to ensemble predictors (e.g., heuristic algorithms; bagging, boosting, and weighted majorities; the Bayesian model averaging approach, and online versions of these) [33]. The main differences between these approaches are how the weights are determined to achieve the best possible predictive power. In this regard, for example, Wang et al. [31] proposed a novel approach that is able to combine several prediction algorithms, where the adaptive weight of each algorithm is determined through an inversely proportional relationship to its sum of the squared prediction errors. The proposed approach was tested using an autoregressive (AR) model, an extreme learning machine, and support vector regression and achieved a satisfactory result [31]. Moreover, Stahl et al. [34] proposed a novel Bayesian approach to merge multiple predictors by using recursive weighting for a single prediction through a regularized optimization technique. Stahl et al. [33] further investigated a novel merging approach that combines elements from switching and averaging techniques to form a soft switcher in a Bayesian framework. Botwey et al. [66] investigated three different data fusion techniques to merge two predictors, an autoregressive model with output correction, cARX, and a recurrent neural network, RNN, based on the Dempster-Shafer evidential theory (DST), genetic algorithms (GA), and genetic programming (GP). Moreover, Daskalaki et al. [68] merged an autoregressive approach with an output correction module (cARX) model, and recurrent neural network (RNN) models, where the fusion is implemented using a linear combination of the two models’ output and the balancing factor (weight) is determined through a customized cost function."
21,"Deep Learning for Diabetes: A Systematic Review Diabetes is a chronic metabolic disorder that affects an estimated 463 million people worldwide. Aiming to improve the treatment of people with diabetes, digital health has been widely adopted in recent years and generated a huge amount of data that could be used for further management of this chronic disease. Taking advantage of this, approaches that use artificial intelligence and specifically deep learning, an emerging type of machine learning, have been widely adopted with promising results. In this paper, we present a comprehensive review of the applications of deep learning within the field of diabetes. We conducted a systematic literature search and identified three main areas that use this approach: diagnosis of diabetes, glucose management, and diagnosis of diabetes-related complications. The search resulted in the selection of 40 original research articles, of which we have summarized the key information about the employed learning models, development process, main outcomes, and baseline methods for performance evaluation. Among the analyzed literature, it is to be noted that various deep learning techniques and frameworks have achieved state-of-the-art performance in many diabetes-related tasks by outperforming conventional machine learning approaches. Meanwhile, we identify some limitations in the current literature, such as a lack of data availability and model interpretability. The rapid developments in deep learning and the increase in available data offer the possibility to meet these challenges in the near future and allow the widespread deployment of this technology in clinical settings.
 A. Limitations and Challenges
Although deep learning has improved the state of art in several areas of diabetes, the applications in healthcare systems need to be robust, reliable, and convincing to avoid safety issues and provide effective therapeutic aids. In this context, there remain several limitations and challenges for deep learning to be further introduced in actual clinical settings. Table IV summarizes the five common limitations identified from the selected articles: data volume, data variability, data quality, feature processing, and interpretability. In real-world scenarios, the data collected from people with diabetes are prone to be imperfect, due to human errors and sensor artifacts. The process to collect real data is sometimes expensive and time-consuming. Due to data privacy policies, sharing data-sets among research teams is sometimes difficult. These factors lead to many studies employing a reduced, sometimes insufficient, amount of data. Another challenge that arises due to the complexity of glucose dynamics is how to process the available data in order to characterize people with diabetes. Also, deep learning models lack transparency. From the perspective of clinicians, why the models produce the output for a certain input case is important, particularly for some critical decision-making applications. The complicated structures in DNN layers can effectively learn the patterns from non-linear signals but reduce the interpretability of the model. Therefore, it is crucial to consider the trade-off between performance and interpretability when investigating deep learning for diabetes. Finally, the efficiency of training deep learning models is expected to be enhanced through new algorithmic and hardware developments [52], [104].
B. Opportunities and Future Work
The list of challenges introduced in Section V-A not only applies to the field of diabetes but also is valid in other health domains. Deep learning is a hotspot in the era of AI, and it is worth noting that most of the selected papers are publications from the recent two years, as shown in Fig. 4, which indicates that this is an emerging technology. Hence, there is a large space to improve the current applications for diabetes.

First, the digital records and vital signs are increasingly collected by the multi-modal systems with wearables and smartphone applications. Most of these data are conveniently uploaded to centralized systems or cloud repositories. With the popularization of the Internet of things and 5 G networks, data volumes and variability of data sources are expected to significantly increase in many healthcare applications, and in particular, in diabetes care. As the data volume expands, many low-quality data samples can be filtered out and removed from training sets, and the advances in wearables (e.g. CGM) can effectively reduce the measurement errors. Deep learning is well adapted to cope with such an increase in data availability. Several publicly available datasets are outlined in Section IV, and more datasets will be shared in the communities after proper post-processing and de-anonymization. In order to deploy deep learning in an ambulatory setting, the frameworks mentioned in Section II can be easily ported to mobile devices by using tools such as TensorFlow Lite [74], [77].

To interpret deep learning technologies in healthcare, many recent attempts in the AI domain have been made to enhance model transparency and understand model functionality. In particular, a unified framework, the SHapley Additive exPlanations (SHAP), was proposed to explain the input features that contribute to the final output, which has been validated on many data-driven applications in healthcare domains [116]. This is also an effective method to select input features by ranking their importance. In Table III, an article also employed SHAP analysis to attribute the descriptors for the CNN outcomes [101]. Another effective technique to interpret the learned features of CNN layers is t-distributed stochastic neighbor embedding (t-SNE) [117], which was used to visualize the clusters of heartbeat data according to glucose levels in [90]. The use of t-SNE can also be generalized to other CNN applications, such as DR detection, to qualitatively analyze the extracted feature maps. Moreover, a recent study also verified the conformance of neural network models in terms of glucose-insulin dynamics [118]. Similar approaches can be used to analyze the performance of DNNs and further enhance interpretability.

Instead of solely using data-driven models, integrating the expert knowledge in the learning process can help to better understand the underlying mechanisms of a health condition such as diabetes. Specifically, there are two feasible methods. One is to incorporate the physiological parameters as the input feature of the models, and the other is to use expert knowledge as a guide during the training process. Expert knowledge is also essential to craft safety constrains and calculate the confidence of the model outputs.

Many selected articles mentioned that their studies require to be further validated in real-world scenarios [79], [80], [95], [101], [105]. In this regard, a team from Google took a step forward. They conducted a human-centered study in 11 clinics, applying deep learning to diabetic eye diseases [119]. The results indicate that some socio-environmental factors need to be addressed before the widespread deployment of such automated systems."
22,"Detection of diabetic patients in people with normal fasting glucose using machine learning Background
Diabetes mellitus (DM) is a chronic metabolic disease that could produce severe complications threatening life. Its early detection is thus quite important for the timely prevention and treatment. Normally, fasting blood glucose (FBG) by physical examination is used for large-scale screening of DM; however, some people with normal fasting glucose (NFG) actually have suffered from diabetes but are missed by the examination. This study aimed to investigate whether common physical examination indexes for diabetes can be used to identify the diabetes individuals from the populations with NFG.

Methods
The physical examination data from over 60,000 individuals with NFG in three Chinese cohorts were used. The diabetes patients were defined by HbA1c_≥_48 mmol/mol (6.5%). We constructed the models using multiple machine learning methods, including logistic regression, random forest, deep neural network, and support vector machine, and selected the optimal one on the validation set. A framework using permutation feature importance algorithm was devised to discover the personalized risk factors.

Results
The prediction model constructed by logistic regression achieved the best performance with an AUC, sensitivity, and specificity of 0.899, 85.0%, and 81.1% on the validation set and 0.872, 77.9%, and 81.0% on the test set, respectively. Following feature selection, the final classifier only requiring 13 features, named as DRING (diabetes risk of individuals with normal fasting glucose), exhibited reliable performance on two newly recruited independent datasets, with the AUC of 0.964 and 0.899, the balanced accuracy of 84.2% and 81.1%, the sensitivity of 100% and 76.2%, and the specificity of 68.3% and 86.0%, respectively. The feature importance ranking analysis revealed that BMI, age, sex, absolute lymphocyte count, and mean corpuscular volume are important factors for the risk stratification of diabetes. With a case, the framework for identifying personalized risk factors revealed FBG, age, and BMI as significant hazard factors that contribute to an increased incidence of diabetes. DRING webserver is available for ease of application (http://www.cuilab.cn/dring).

Conclusions
DRING was demonstrated to perform well on identifying the diabetes individuals among populations with NFG, which could aid in early diagnosis and interventions for those individuals who are most likely missed. Considering that there exists a small proportion of diabetes patients with normal fasting glucose level who could be missed during screening, we assessed the ability of common risk factors to detect the missed diabetic patients in population with normal fasting glucose (NFG) and thus constructed DRING, a machine learning model for predicting the diabetic individuals with NFG based on physical examination data of over 60,000 samples derived from three Chinese cohorts. DRING showed more than 0.9 of AUC and 75% of sensitivity on newly recruited independent test set. For enhancing the interpretability of DRING, potential characteristics related to diabetes except for fasting blood glucose contains BMI, age, ALC, and sex by feature importance ranking analysis. Moreover, the analysis for exploring the personalized risk factors of diabetes was proved to be practical with a case study. Two models and the framework of screening personalized risk factors were integrated into DRING webserver, which requires only 13 features as input to predict the risk of diabetes. To accommodate different requirements for specificity levels, the webserver offers users a range of choices to customize the specificity levels.

With the increase of data volume, machine learning techniques have the potential to revolutionize diabetes screening by enabling more accurate risk stratification and timely interventions. The performance of ML-based methods is influenced by the availability of the number of samples and features. For instance, the combination of ML techniques and electronic health record data could enhance the effectiveness of diabetes screening and improving patient outcomes. One of the challenging issues in utilizing ML techniques is selecting the most suitable method to achieve optimal performance on a given dataset. Nearly all machine learning techniques have been applied on the diabetes risk prediction [16], while no single method that consistently outperforms other methods across diverse datasets. Dinh et al. reported that the model of predicting diabetes with eXtreme Gradient Boost (XGBoost) performed best than those of RF, SVM, and LR based on the National Health and Nutrition Examination Survey (NHANES) dataset [30]. With the Pima Indian Diabetes Database (PIDD), which is a widely used dataset in diabetes recognition with machine learning, a study constructed 24 classifiers such as decision tree, LR, discriminant analysis, k-nearest neighbors, and ensemble learners and found the best accuracy score of 77.9% was produced by the LR model [31]. Jahangir and his colleagues devised automatic multilayer perceptron model achieving an accuracy of 88.7% on PIDD [32], while they did not train other machine learning models using processed data. The difference in performance between these two studies may not solely depend on the ability of machine learning models but also the dataset partitioning and preprocessing could impact the results. A recent study conducted a systematic analysis among 71 studies of clinical prediction models and concluded that no evidence of superior performance of other machine learning methods over LR [33]. Our results have also shown that LR model obtains a higher AUC and balanced accuracy in identifying the diabetes from population with NFG. This could be attributed to the fact that complex models are not suitable when using a limited number of features. The factors, such as the size of the dataset, interpretability, and the balance of precision and complexity, collectively determine the optimal choice of models.

There is typically no specific guideline that needs to be strictly followed when employing artificial intelligence models for diabetes screening, except for the initial step of sample definition, that is, defining positive and negative samples. The definition of normal fasting glucose level varies among different guidelines, for example, the American Diabetes Association (ADA) uses FBG of 5.6 mmol/L as the definition of normal fasting glucose, while the threshold is set to 6.1 mmol/L according to the WHO or International Diabetes Federation (IDF). Here, we screened the samples with NFG by the criterion of WHO and found the fasting blood glucose (FBG) level for most diabetes patients is approximate to 6.0 (Fig. 1A). In our cohorts, it was observed that the number of diabetes patients sharply decreased when the threshold for NFG was lowered. Specifically, less than half of the individuals were classified as diabetes when NFG level was set at 5.69 (Additional file 1: Fig. S5). Accordingly, we recommended the FBG of 5.69 as the alarming line of diabetes for Chinese, that is, a person should notice the sugar intake and go for a thorough diabetes-focused examination in case his fasting glucose is more than 5.69. Although it was questioned when the justification for lowering the threshold of normal fasting glucose recommended by ADA [34], the warning line of normal fasting glucose needs timely adjustment for people from different races or regions considering the significant difference in diet, lifestyle, and environment.

The risk factors are diverse for incident diabetes. We employed clinical basic information and blood routine test indicators to train the prediction model. In contrast, most other methods for prediction the risk of diabetes rely on the basic information including diabetes family history and blood pressure combined with biochemical markers such as triglycerides and total cholesterol [15, 35, 36] and rarely include the features of blood routine test. Except for the well-known factors such as BMI and age [35, 37], the analysis of feature importance ranking showed that absolute lymphocyte count (ALC), mean corpuscular volume (MCV), white blood cell count (WBC), and neutrophil (NEU) also are important for identifying the diabetes patients (Fig. 5 and Additional file 1: Fig. S4). Twig et al. had revealed that WBC was an independent risk factor for incident diabetes in young men [38]. The inclusion of more diabetes-associated variables and higher resolution data is likely to improve the accuracy of the predictive model. Quincy et al. presented a diabetes risk stratification model integrating physiological, biochemical, and genomics data and achieved superior testing accuracies [39]; however, its practicality is weakened due to the difficulty in data acquisition.

The strength of this study lies in that it is the first prediction model specially designed to identify diabetes patients who are at high risk of being missed according to our knowledge, which serves as a valuable supplement to existing diabetes risk prediction models. The model has also been integrated into the online tool, facilitating its potential clinical application. However, a key limitation of this study is that it is challenging to assert the generalization of the prediction model on global population. Diabetes is influenced by various factors such as race and environment, although our method was validated and tested on multiple cohorts, all of which were Chinese populations; thus, its performance in other populations remains uncertain. The generalization of the current method needs to be assessed through more external validation datasets, especially those involving other ethnic populations. Secondly, there exist various prediction models for diabetic risk assessment at present, but it is still incomparable between DRING and other methods because DRING is extensively used for distinguishing the diabetes patients with NFG and healthy individuals. Third, the precision of current method is relatively low, which is markedly impacted by the severely imbalanced distribution of diabetes and normal individuals. The ratio of positive to negative samples is over 1:100. Thus, even with the inclusion of over 60,000 samples in our study, the number of diabetes samples is only around 600. It is undoubtable that this scenario aligns with the real-world condition represented by infrequent cases of diabetes patients with NFG and major healthy individuals with NFG. In the future, the model of predicting diabetes risk for the population with NFG introducing more crucial features such as waist-to-hip ratio, blood pressure, and common biochemical indicators might enhance its precision. Last but not the least, conducting a comprehensive health technology assessment is necessary to promote our method serving as a decision-making support system in diabetes diagnosis."
23,"Development and Validation of a Machine Learning Model Using Administrative Health Data to Predict Onset of Type 2 Diabetes Importance: Systems-level barriers to diabetes care could be improved with population health planning tools that accurately discriminate between high- and low-risk groups to guide investments and targeted interventions.

Objective: To develop and validate a population-level machine learning model for predicting type 2 diabetes 5 years before diabetes onset using administrative health data.

Design, setting, and participants: This decision analytical model study used linked administrative health data from the diverse, single-payer health system in Ontario, Canada, between January 1, 2006, and December 31, 2016. A gradient boosting decision tree model was trained on data from 1 657 395 patients, validated on 243 442 patients, and tested on 236 506 patients. Costs associated with each patient were estimated using a validated costing algorithm. Data were analyzed from January 1, 2006, to December 31, 2016.

Exposures: A random sample of 2 137 343 residents of Ontario without type 2 diabetes was obtained at study start time. More than 300 features from data sets capturing demographic information, laboratory measurements, drug benefits, health care system interactions, social determinants of health, and ambulatory care and hospitalization records were compiled over 2-year patient medical histories to generate quarterly predictions.

Main outcomes and measures: Discrimination was assessed using the area under the receiver operating characteristic curve statistic, and calibration was assessed visually using calibration plots. Feature contribution was assessed with Shapley values. Costs were estimated in 2020 US dollars.

Results: This study trained a gradient boosting decision tree model on data from 1 657 395 patients (12 900 257 instances; 6 666 662 women [51.7%]). The developed model achieved a test area under the curve of 80.26 (range, 80.21-80.29), demonstrated good calibration, and was robust to sex, immigration status, area-level marginalization with regard to material deprivation and race/ethnicity, and low contact with the health care system. The top 5% of patients predicted as high risk by the model represented 26% of the total annual diabetes cost in Ontario.

Conclusions and relevance: In this decision analytical model study, a machine learning model approach accurately predicted the incidence of diabetes in the population using routinely collected health administrative data. These results suggest that the model could be used to inform decision-making for population health planning and diabetes prevention. This decision analytical model study found that accurate prediction of type 2 diabetes onset at the population level 5 years in advance was possible solely from routinely collected administrative health data for the purposes of public health planning and health resource allocation. It was not our goal for this model to be applied in the context of individual patient care. Our model was trained and validated on more than 2 million patients, which, to our knowledge, is one of the largest cohorts for predicting diabetes incidence. Our model showed consistent calibration across sex, immigration status, racial/ethnic and material deprivation, and a low to moderate number of events in the health care history of the patient. The cohort was representative of the whole population of Ontario, which is itself among the most diverse in the world.50 The model was well calibrated, and its discrimination, although with a slightly different end goal, was competitive with results reported in the literature for other machine learning–based studies that used more granular clinical data from electronic medical records without any modifications to the original test set distribution.23,24,25

Assessing risk in populations is the basis of health system planning and a critical element of diabetes prevention.51,52 When managing risk in populations, there are critical questions regarding the most efficient usage of resources, and without a comprehensive estimate of risk in populations, strategies can be costly and ineffective. Furthermore, it is widely recognized that the prevention of diabetes is not only influenced by factors at the individual level but must be complemented by whole population approaches, such as food policies and environmental changes.6 The use of machine learning methods for predicting risk in populations offers an important opportunity to inform resource and policy-level decisions that can change diabetes risk trajectories as well as allow for more efficient targeting of resources within a health system.

The growing burden of diabetes is a challenge faced by other jurisdictions across the globe.1,2,3 Continuous risk assessment using the multi-instance approach we proposed could reduce this cost through the targeting of preventive health measures, even more so given the fact that our model did not require additional data collection. Such an approach could be feasible in countries such as the UK, Australia, New Zealand, and the Scandinavian countries, which have large, administrative databases suitable for linkage.53,54,55,56,57 Furthermore, this approach could also be deployed in populations covered under a singular health insurance system, such as Medicare or private insurers.58

Our features not only captured each patient’s medical history but also included the social and demographic determinants of health, which are important predictors of a patient’s overall risk of developing diabetes and are often missing in clinical data sources.59,60,61 Moreover, the calibration of our machine learning model across demographic subgroups suggests that it may be possible to apply it to target-specific population segments with preventive measures (Table 2 and Figure 3). Diabetes prevention strategies can be targeted toward those above a certain risk threshold.62 Our model results suggest that older patients from the most marginalized neighborhoods in terms of race/ethnicity and material deprivation were at the highest risk and may therefore benefit the most from preventive measures. Given the growing costs associated with the diabetes cohort, our work suggests a quantitative financial incentive toward the direction of preventive measures that consider those at greatest risk, including from a socioeconomic perspective.59 Because our machine learning model included social determinants of health that are known to contribute to diabetes risk, our population-wide approach to risk assessment may represent a tool for addressing health disparities.59,63,64

Strengths and Limitations
Our study approach had several strengths. Owing to the nature of administrative data, such an approach could be applied to other chronic diseases. In 2009, 24.3% of Ontarians were found to be affected by multiple comorbidities.65 Accurately forecasting other prominent chronic conditions, such as hypertension, could lead to potential considerable reductions in health care costs while also improving the health and well-being of the population. Similar work to create risk prediction models has been done in a primary prevention cohort from New Zealand to determine 5-year cardiovascular disease risk, and research from the UK reinforces that reducing cardiovascular event risk by even 1% would result in both large cost savings and improved population health.54,66 Moreover, we included a detailed calibration assessment, both overall and within key population subgroups, which suggests that our model did not only have strong discrimination but was well calibrated in a diverse population.67 Finally, the choice of using a gradient boosting machine model permitted the usage of Shapley values to enhance explainability.68 Our proposed approach also had some important limitations. First, there was the potential for misclassification of patients with type 1 diabetes given limitations with the algorithm used in label construction of type 1 and type 2 diabetes.69,70 Of the roughly 2% to 3% of individuals aged 20 years or younger who tested positive for diabetes, we are uncertain how many were actually diagnosed with type 1 diabetes. However, we chose not to exclude younger patients in our cohort owing to the rising incidence of type 2 diabetes in youths and young adults.71,72 Second, the input administrative health data were highly heterogeneous: only 23.4% of patients had at least 1 laboratory value, and only the patients older than 65 years had a prescription history. We believe that more consistency and fewer missing values in the input data would improve the model’s discrimination. Third, administrative data often does not capture certain features known to be highly predictive of diabetes onset, such as body mass index; however, we achieved competitive performance when our machine learning model was compared to those trained on richer sources of data while allowing for applicability at the population level. Fourth, although we can interpret the model’s decisions and the way it splits variables to separate patients into risk score categories, the model strictly captured correlations in the data and not causal pathways. Finally, our model would need to be further validated through prospective studies before deployment."
24,"Diabetes Detection Models in Mexican Patients by Combining Machine Learning Algorithms and Feature Selection Techniques for Clinical and Paraclinical Attributes: A Comparative Evaluation. The development of medical diagnostic models to support healthcare professionals has witnessed remarkable growth in recent years. Among the prevalent health conditions affecting the global population, diabetes stands out as a significant concern. In the domain of diabetes diagnosis, machine learning algorithms have been widely explored for generating disease detection models, leveraging diverse datasets primarily derived from clinical studies. The performance of these models heavily relies on the selection of the classifier algorithm and the quality of the dataset. Therefore, optimizing the input data by selecting relevant features becomes essential for accurate classification. This research presents a comprehensive investigation into diabetes detection models by integrating two feature selection techniques: the Akaike information criterion and genetic algorithms. These techniques are combined with six prominent classifier algorithms, including support vector machine, random forest, k-nearest neighbor, gradient boosting, extra trees, and naive Bayes. By leveraging clinical and paraclinical features, the generated models are evaluated and compared to existing approaches. The results demonstrate superior performance, surpassing accuracies of 94%. Furthermore, the use of feature selection techniques allows for working with a reduced dataset. The significance of feature selection is underscored in this study, showcasing its pivotal role in enhancing the performance of diabetes detection models. By judiciously selecting relevant features, this approach contributes to the advancement of medical diagnostic capabilities and empowers healthcare professionals in making informed decisions regarding diabetes diagnosis and treatment. The objective of this study is to compare the performance of six machine learning algorithms in combination with two feature selection techniques for generating classification models of diabetic and nondiabetic patients using clinical and paraclinical features. The implemented classifier algorithms include SVM, RF, kNN, GB, ET, and NB, while the feature selection techniques utilized are the Akaike information criterion and genetic algorithms. Initially, classification models were created using the complete feature set of the dataset and subsequently compared with models generated using feature subsets obtained through the feature selection techniques. A total of 18 classification models were generated, and their performance was compared. Based on the results obtained from the feature selection methods, the following can be concluded:
(i)	By using the Akaike criterion as a feature selection technique, a reduction of 27% in the dataset size was achieved, keeping only 14 features out of the original 19. The selected features, which efficiently describe whether a patient has diabetes or not according to this method, are shown in Table 5
(ii)	By applying genetic algorithms as a feature selection technique, a reduction of 73% in the number of features from the original dataset was achieved, resulting in only 5 selected features out of the initial 19. The frequency and rank of the genes (features) in the models determined by the genetic algorithm implementation are shown in Figures 2 and 3, respectively. The features selected using this approach are presented in Table 7
(iii)	The reduction achieved in the number of features through the implementation of the described feature selection methods is significant, particularly in the case of genetic algorithms. This reduction in the dataset size has a generally positive impact on the performance of the systems. By working with a smaller amount of data, several advantages can be obtained, including reduced processing time and lower energy consumption
(iv)	Although the implementation of feature selection techniques on the original dataset resulted in a reduction of 27% and 73% using the Akaike information criterion and genetic algorithms, respectively, it is important to note that in certain applications of the classification model, maximizing classification accuracy is often preferred over minimizing the amount of processed data. This is particularly relevant in the medical field, as is the case here. Therefore, for the generation of the classification models, the complete set of features was also considered a reference. This approach allows for finding a balance between the model’s performance and the amount of data used for analysis

Based on the findings derived from the exploration of classification models for distinguishing between diabetic and nondiabetic patients, utilizing six distinct classification algorithms, and employing the entire dataset encompassing 19 features, the subsequent conclusions can be established:
(i)	The random forest (RF), gradient boosting (GB), and extra trees (ET) models consistently perform well across multiple metrics, including high AUC, specificity, sensitivity, accuracy,  score, and precision. These models demonstrate a strong ability to accurately classify both positive and negative instances
(ii)	The SVM model also performs well, achieving high scores in AUC, specificity, sensitivity, accuracy,  score, and precision. It shows a balanced performance in correctly classifying both positive and negative instances
(iii)	The naive Bayes (NB) model achieves moderate performance with relatively lower scores in specificity, accuracy,  score, and precision compared to the other models. It may have higher rates of false positives and lower overall accuracy compared to the top-performing models
(iv)	The k-nearest neighbor (kNN) model shows relatively lower performance in terms of specificity, accuracy, and  score. It may have higher rates of false positives and lower overall accuracy compared to the other models
(v)	In summary, the random forest (RF), gradient boosting (GB), and extra trees (ET) models exhibit strong overall performance across multiple metrics, while the SVM model also performs well. The naive Bayes (NB) model achieves moderate performance, and the k-nearest neighbor (kNN) model shows relatively lower performance

For the classification models that use the subset of features obtained through the Akaike criterion (14 features), the following can be concluded:
(i)	The random forest (RF), gradient boosting (GB), and extra trees (ET) models maintain their high performance across multiple metrics, including AUC, specificity, sensitivity, accuracy,  score, and precision, even when using the reduced 14-feature subset. These models demonstrate the advantage of feature selection, as they maintain their strong classification abilities with a smaller set of features
(ii)	The SVM model also maintains a high level of performance, with consistently high scores in AUC, specificity, sensitivity, accuracy,  score, and precision when using the reduced feature subset
(iii)	The naive Bayes (NB) model shows a slight decrease in performance compared to the other models, particularly in specificity, accuracy,  score, and precision. However, it still achieves moderate performance overall
(iv)	The k-nearest neighbor (kNN) model exhibits the lowest performance among the models, with lower scores in specificity, sensitivity, accuracy,  score, and precision when using the reduced feature subset
(v)	In summary, the random forest (RF), gradient boosting (GB), and extra trees (ET) models maintain their strong classification performance even with a reduced feature subset. The SVM model also demonstrates robust performance, while the naive Bayes (NB) model shows a slight decrease in performance. The k-nearest neighbor (kNN) model performs relatively weaker compared to the other models. These findings highlight the effectiveness of feature selection in reducing the dimensionality of the dataset while maintaining good classifier performance

For the classification models that use the subset of features obtained through genetic algorithms (5 features), the following can be concluded:
(i)	The random forest (RF), gradient boosting (GB), and extra trees (ET) models maintain their strong classification performance even with the significantly reduced 5-feature subset. These models consistently achieve high scores in various metrics, including AUC, specificity, sensitivity, accuracy,  score, and precision. This highlights the advantage of feature selection in reducing the dimensionality of the dataset while preserving good classifier performance
(ii)	The SVM model also maintains a relatively high level of performance, with consistently good scores in AUC, specificity, sensitivity, accuracy,  score, and precision when using the reduced 5-feature subset
(iii)	The naive Bayes (NB) model performs slightly lower in terms of specificity, accuracy,  score, and precision compared to the other models. However, it still achieves moderate performance overall
(iv)	The k-nearest neighbor (kNN) model exhibits the lowest performance among the models, with lower scores in specificity, sensitivity, accuracy,  score, and precision when using the reduced 5-feature subset
(v)	In summary, the random forest (RF), gradient boosting (GB), and extra trees (ET) models demonstrate their robustness by maintaining their strong classification performance even with a highly reduced feature subset. The SVM model also maintains good performance, while the naive Bayes (NB) model shows slightly lower performance. The k-nearest neighbor (kNN) model performs relatively weaker compared to the other models. These findings emphasize the effectiveness of feature selection in reducing the dimensionality of the dataset while preserving or even improving classifier performance

From the Wilcoxon statistical test performed on the complete dataset, the following can be concluded:
(i)	The results of the Wilcoxon test reveal significant differences between the case group (diabetic patients) and the control group (nondiabetic patients) concerning the dataset features. This indicates that the selected features play a relevant role in distinguishing between the two groups. Selecting appropriate features is essential for generating accurate and effective classification models for diabetes detection
(ii)	The statistical significance obtained through the Wilcoxon test provides robust validation for our classification models. The results support the effectiveness of the models in distinguishing between diabetic and nondiabetic patients using the selected features. This reinforces confidence in the utility and applicability of our models in diabetes detection for future clinical scenarios
(iii)	The inclusion of the Wilcoxon test in our study has enabled a comprehensive assessment of statistical significance and validity of the results obtained. This test has been instrumental in ruling out random chance as the cause of observed differences between the groups and supporting the robustness of our findings. The incorporation of this statistical test strengthens the quality and reliability of our study and its conclusions

In conclusion, the effectiveness of feature selection techniques in conjunction with classification algorithms has been demonstrated in this study. The reduction of the dataset to a smaller subset of features while maintaining strong classification performance highlights the efficiency of the approach employed. The results obtained, which are comparable to or surpass the performance reported in related studies, emphasize the superiority of the feature selection methods utilized in this research. By leveraging feature selection, the study successfully extracted the most relevant and discriminative features, resulting in accurate and efficient classification models. The robustness of these models, as evidenced by metrics such as AUC, specificity, sensitivity, accuracy,  score, and precision, further validates the effectiveness of the approach employed in this investigation.

Furthermore, the advantages of feature selection in terms of model interpretability and efficiency are demonstrated in this study. Through the reduction of dataset dimensionality, both computational efficiency and interpretability of the models are improved by focusing on the most informative features. The findings emphasize the significance of feature selection in classification tasks. The integration of feature selection methods with classification algorithms results in compact yet powerful models that exhibit comparable or superior performance to related studies. This further highlights the potential of the approach employed and emphasizes the crucial role of feature selection in optimizing classification outcomes.

Based on all of the above, some points considered for future work are as follows:
(i)	Further investigation of feature selection techniques: although the Akaike information criterion and genetic algorithms were utilized in this study, other feature selection methods can be explored. Techniques such as recursive feature elimination, principal component analysis, and lasso regression could be considered to evaluate their impact on the performance of classification models
(ii)	Integration of domain knowledge: incorporating domain expertise and prior knowledge can potentially improve the feature selection process. Collaborating with medical professionals or domain experts to identify relevant features or incorporating additional clinical variables could enhance the accuracy and interpretability of the classification models
(iii)	Evaluation on larger datasets: conducting experiments on larger datasets can provide a more comprehensive understanding of the performance and scalability of the classification models. By including a broader range of patient samples, the generalizability of the models can be further assessed
(iv)	Incorporation of ensemble methods: ensemble methods, such as model stacking or boosting, can be explored to further enhance the classification performance. Combining the predictions of multiple classification models can potentially improve the accuracy and robustness of the overall system
(v)	External validation and clinical application: it is crucial to validate the developed models on independent datasets or in real clinical settings. Conducting external validation studies with different patient populations and healthcare settings can provide valuable insights into the generalizability and real-world applicability of the classification models"
25,"Diabetes disease detection and classification on Indian demographic and health survey data using machine learning methods. BACKGROUND & AIM: Diabetes mellitus has become one of the out brakes causing major health issues in developing countries like India. The need for leveraging technology is felt in diabetes management. The main objective of this work is to deploy machine learning methods for the detection and classification of diabetes having clinical relevance.

METHODS: Indian demographic and health survey-2016 dataset is considered and determined the risk factors for continuous and categorical data. Kernel entropy component analysis is used for the dimensionality reduction of the feature set. Predictive exploration-based machine learning methods like logistic regression, gaussian naive Bayes, linear discriminant analysis, support vector classifier, k-nearest neighbor, decision tree, extreme gradient boosting, kernel entropy component analysis, and random forest are deployed in the work. The deployed methodology has three phases: feature extraction, classification, and prediction.

RESULTS: Random Forest gave the maximum classification accuracy of 99.84% and 96.75% for imbalanced and kernel entropy component analysis-induced balanced datasets (using synthetic minority oversampling technique) respectively. The maximum precision of 99.64% is obtained using a support vector classifier on the balanced dataset. The area under the curve is 99%, which is observed from kernel entropy component analysis induced random forest on the balanced dataset. All other models performed moderately when applied to kernel entropy component analysis trained dataset.

CONCLUSIONS: Random Forest model performed better in comparison with other models. The overall performance of the machine learning models can be improved by training the diabetes dataset using kernel entropy component analysis. In this study, nine ML-based classifiers are applied to the risk strata of patients into diabetic or non-diabetic. we considered 12 factors that lead to the prevalence of diabetes such as State, Type of place of residence, electricity, educational attainment, wealth index, body mass index, district, age, age within 5 years group, arm circumference, glucose level, wealth index in the state, currently have diabetes. Eight ML-based classifiers including LR, LDA, SVM, DT, XGBoost, RF, G-NB, & KNN are adopted, and performance is assessed using accuracy, precision, recall, f1-score, an area under the curve, and MCC [15]. It is observed that the SVM model performed well out of linear classifiers and DT & RF performances were impressible in the case of non-linear models. The induction of kernel ECA has increased the performance of classifiers in comparison with the regular dataset. The IDHS dataset is imbalanced and there were chances of observing the bias across minority & majority samples. The values of the f1-score and MCC were poor in the case of an imbalanced dataset which indicates that the models were not performing well. Information representing the kernel ECA induced on an imbalanced set, results are better in comparison with the first experimentation but still, the f1-score and MCC values are very poor. It was resolved by using SMOTE technique and the performance of the balanced dataset was calculated. The overall observation is that the performance of the balanced dataset using SMOTE model and kernel ECA transformed data has given optimal results in comparison with imbalanced and kernel ECA-induced datasets as shown in Fig. 2. The area under the curve (AUC) was drawn for all eight models for three experimentations. The ROC curve for all eight models on kernel ECA-induced balanced datasets (using synthetic minority oversampling technique) is demonstrated in Fig. 3. The results obtained through the proposed model are more reliable and efficient compared to existing models as discussed in Table 2. The study has the following limitations: (i) The risk factors like diabetes pedigree function (family history), and waist circumference are not considered. (ii) Lifestyle, physical activities, and diet are other missing parameters. Diabetes mellitus has widespread world wide. Traditional parameters like glucose level, body mass index, etc., and non-traditional attributes like hemoglobin level, arm circumference, etc. are other features that indicate presence of diabetes. The ML-based classifiers namely, DT, SVM, and RF gave good classification accuracies. The classifier models are subjected to a k-10 cross-validation process. It is observed that the Random Forest classifier has given the highest accuracy of 96.3% for the balanced dataset. Diabetes remission with the support of technology helps doctors to suggest patients at risk of prediabetes and those who attained the diabetic stage within one to five years.
"
26,"Evaluation of Machine Learning Methods Developed for Prediction of Diabetes Complications: A Systematic Review Background: With the rising prevalence of diabetes, machine learning (ML) models have been increasingly used for prediction of diabetes and its complications, due to their ability to handle large complex data sets. This study aims to evaluate the quality and performance of ML models developed to predict microvascular and macrovascular diabetes complications in an adult Type 2 diabetes population.

Methods: A systematic review was conducted in MEDLINE®, Embase®, the Cochrane® Library, Web of Science®, and DBLP Computer Science Bibliography databases according to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) checklist. Studies that developed or validated ML prediction models for microvascular or macrovascular complications in people with Type 2 diabetes were included. Prediction performance was evaluated using area under the receiver operating characteristic curve (AUC). An AUC >0.75 indicates clearly useful discrimination performance, while a positive mean relative AUC difference indicates better comparative model performance.

Results: Of 13 606 articles screened, 32 studies comprising 87 ML models were included. Neural networks (n = 15) were the most frequently utilized. Age, duration of diabetes, and body mass index were common predictors in ML models. Across predicted outcomes, 36% of the models demonstrated clearly useful discrimination. Most ML models reported positive mean relative AUC compared with non-ML methods, with random forest showing the best overall performance for microvascular and macrovascular outcomes. Majority (n = 31) of studies had high risk of bias.

Conclusions: Random forest was found to have the overall best prediction performance. Current ML prediction models remain largely exploratory, and external validation studies are required before their clinical implementation. This review has evaluated the performance of 87 prognostic prediction ML models for diabetes complications in people with Type 2 diabetes. Most ML models reported an AUC between 0.6 and 0.75 (possibly helpful discrimination), while 36% achieved an AUC above 0.75 (clearly useful discrimination). 33

From 16 comparison studies, ML methods generally showed better performance than non-ML methods. It must be noted, however, that these studies were rated at high risk of bias. This was similar to a review by Christodoulou et al, which found that the performance of ML and non-ML methods for prediction of clinical outcomes in the general population using models with low risk of bias were comparable. However, they noted that comparisons among models with high risk of bias tended to favor ML methods. 30

Among ML methods, random forest showed an overall better discrimination ability for both microvascular and macrovascular outcomes. A possible explanation is that random forest combines multiple models to overcome the limitations of single models, thereby reducing variance and improving prediction accuracy.

In terms of predictors used in ML models, common predictors for both microvascular and macrovascular outcomes include age, duration of diabetes, and body mass index. Prolonged hyperglycemia is known to cause vascular damage through nonenzymatic glycosylation of proteins, oxidative stress, and inflammation. 68 Likewise, the relationship between age and diabetic complications has been linked to age-related impaired vascular function such as arterial stiffening, increased insulin resistance, and obesity. 66 The relationship between body mass index and diabetes complications is less clear, with a previous study suggesting that it was positively correlated with diabetic kidney disease but not with diabetic retinopathy. 69 Given that age and duration of diabetes can be obtained from electronic health data sets and their clinical relevance in the development of diabetes complications, researchers should consider including them when developing future ML predictive models.

In terms of model development, many studies were limited by small number of outcomes examined and sample sizes, often with events per variable below 10. In addition, internal validations with resampling and external validations were inconsistently performed. This raises concerns of model overfitting and optimism, as ML techniques have been found to require significantly larger events per variable (>200) to achieve a stable AUC and a small optimism compared with traditional statistical methods such as logistic regression. 32

The ML models in this review were considered largely exploratory, and future validation studies are required before clinical implementation. 16 For studies with externally validated models, further model-impact studies should also be considered. 70 For example, the random survival forest-based model developed by Segar et al, 67 which was validated in a diabetes population with high cardiovascular risk, require further validation in a general setting with lower-risk individuals with Type 2 diabetes. Likewise, support vector machine classifier developed by Good et al would benefit from further studies to determine the cost-effectiveness of utilizing urinary proteomics (which is more expensive than standard urine albumin tests) as predictors in clinical practice.

The overall reporting quality was not standardized across studies, where details such as inclusion and exclusion criteria, method of measure of outcomes, and relevant performance measures were omitted in several studies. In view of the inconsistencies in reporting across studies, future developers of prediction models should consider adopting the reporting guidelines recommended by TRIPOD. 31 All development studies should also perform internal validation with resampling methods such as bootstrap, to quantify model overfitting and optimism. 34

Based on the findings from this review, future researchers may wish to consider the use of random forest algorithms either as the primary prediction model or as a comparison model during evaluation. Another ensemble method—extreme gradient boosting (XGBoost)—which was not covered in this review has also shown good prediction performance and can be explored in future studies. 71

It is important to recognize that the prediction performance of ML models is heavily dependent on the choice of data (for training and testing) and the tuning of model parameters. For example, class imbalance due to small minority class and poor-quality data sets can affect prediction accuracy. 72 Consequently, fair evaluation and comparisons can only be made through standardized benchmark testing with fixed data sets. We propose for data-sharing via open-access data sets to be made available to researchers for external validation of their prediction models. Future studies could also look at standardizing the various outcome definitions for diabetes complications to allow for more objective comparisons of prediction models across different studies.

Finally, to facilitate the clinical translation of models, it is important to select predictors that can be readily obtained in clinical practice (eg, demographics and routine investigations such as fasting blood glucose) and to ensure that ML model predictions can be easily interpreted."
27,"Improving Accuracy for Diabetes Mellitus Prediction by Using Deepnet Diabetes is a salient issue and a significant health care concern for many nations. The forecast for the prevalence of diabetes is on the rise. Hence, building a prediction machine learning model to assist in the identification of diabetic patients is of great interest. This study aims to create a machine learning model that is capable of predicting diabetes with high performance. The following study used the BigML platform to train four machine learning algorithms, namely, Deepnet, Models (decision tree), Ensemble and Logistic Regression, on data sets collected from the Ministry of National Guard Hospital Affairs (MNGHA) in Saudi Arabia between the years of 2013 and 2015. The comparative evaluation criteria for the four algorithms examined included; Accuracy, Precision, Recall, F-measure and PhiCoefficient. Results show that the Deepnet algorithm achieved higher performance compared to other machine learning algorithms based on various evaluation matrices. The aim of this study was the following: to build a machine learning model(s) that can predict diabetes with high accuracy. Therefore, the usage of the BigML machine learning platform helped in the creation of the four machine learning models, namely Ensemble, Models (decision tree), Deepnet and Logistic Regression. Each machine learning algorithm has different machine learning techniques. The overall goal of this study was to find the best performance model and apply its technique to predict diabetes. The performance of the Deepnet model was better than Models (decision tree), Ensemble and Logistic Regression on all of the evaluation criteria, Table 2. The prediction of diabetic patients who may not know they have the disease is a crucial challenge in the healthcare domain. The machine learning technique demonstrates the ability to predict diabetes with high accuracy using only 17 attributes. Furthermore, an offered perk of this method is information collection can occur from routine checkups at a healthcare clinic. This process will allow the integration of up-to-date information into the system expediting medical care and easing the burden on healthcare workers and patients.

Changing the healthcare workflow can enhance the early healthcare assessments of those with diabetes. As a result, this can decrease the prevalence of the disease and improve initial management practices. Furthermore, this will increase patients' satisfaction and overall quality of care.

A comprehensive diagnostic framework has the potential to streamline medical services and empower patients. Machine learning based on algorithms offers a unique tool for healthcare professionals to utilize, from both an epidemiological and treatment perspective. From a systems standpoint, the ability to centralize medical data and predict trends in population health would allow resource allocation to the identified gaps, which in turn, strengthens the population's health.

Moreover, another meaningful impact of integrating machine learning is the benefit to the patients. The WHO [31] defines empowerment as ""a process through which people gain greater control over decisions and actions affecting their health"" and affects both individual and community levels. To empower the population, they must get access to their information and have it delivered it in an understandable format, transparent, and overall—user-friendly. Ease of use is paramount for patient engagement.

Literature indicates that within the 21st century, mobile health technologies resulted in increases in connecting users on a community, population, and global level [32]. Mobile health addresses the rising burden of chronic diseases while encouraging health systems to shift towards patient-centric designs [32]. Mobile health consists of medical practice supported by a portable diagnostic device [32]. The use of these devices at the point-of-care resulted in not only a change in healthcare delivery, but an increase in patient engagement, a reduction in healthcare costs, and improved patient prognosis [32].

Model learning has the potential to increase patient empowerment via mobile health. The compatibility for our connected world through accessibility from a smartphone, desktop or other personal electronic devices, in the way of an app, is potentially highly useful in capitalizing on our mobile interconnectedness. However, before the implementation of mobile health, guidelines to manage these machine learning models are essential for healthcare.

Systematically developed statements based on research, best practices, best scientific evidence, and experience act as guidelines [31,33]. Guidelines support healthcare providers with an outline for patient care to ensure that individuals receive the same or similar patient care across healthcare facilities.

Standardizing guidelines across healthcare facilities can aide authorities in bridging the gap between research and practices within these facilities, which helps to foster consistent services. Additionally, standardizing guidelines across healthcare facilities helps healthcare providers to identify the what, where, when, and how of the patient's health; while collecting, sharing, and reporting data improves and streamlines the process. The collected and reported data based on clinical guidelines assists healthcare and public health authorities in identifying the age groups or individual patients at high risk of having diabetes (Type 1 or Type 2).

The collected and reported data assists healthcare authorities in planning prevention and treatment plans. The flexibility of the process allows healthcare authorities to navigate the ever-changing healthcare landscape. Gaps, limitations, and needs of population health are dynamic, and to avoid steep healthcare costs, the allocation of resources must have a basis in evidence to resolve pressing issues best."
28,"Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes. Medical experts may use Artificial Intelligence (AI) systems with greater trust if these are supported by 'contextual explanations' that let the practitioner connect system inferences to their context of use. However, their importance in improving model usage and understanding has not been extensively studied. Hence, we consider a comorbidity risk prediction scenario and focus on contexts regarding the patients' clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We explore how relevant information for such dimensions can be extracted from Medical guidelines to answer typical questions from clinical practitioners. We identify this as a question answering (QA) task and employ several state-of-the-art Large Language Models (LLM) to present contexts around risk prediction model inferences and evaluate their acceptability. Finally, we study the benefits of contextual explanations by building an end-to-end AI pipeline including data cohorting, AI risk modeling, post-hoc model explanations, and prototyped a visual dashboard to present the combined insights from different context dimensions and data sources, while predicting and identifying the drivers of risk of Chronic Kidney Disease (CKD) - a common type-2 diabetes (T2DM) comorbidity. All of these steps were performed in deep engagement with medical experts, including a final evaluation of the dashboard results by an expert medical panel. We show that LLMs, in particular BERT and SciBERT, can be readily deployed to extract some relevant explanations to support clinical usage. To understand the value-add of the contextual explanations, the expert panel evaluated these regarding actionable insights in the relevant clinical setting. Overall, our paper is one of the first end-to-end analyses identifying the feasibility and benefits of contextual explanations in a real-world clinical use case. Our findings can help improve clinicians' usage of AI models. In this section we analyze both the quantitative and qualitative results presented in Section 4. We analyze both the feasibility of supporting contextual explanations from authoritative sources such as CPGs, and the usefulness of providing contextual explanations from an analysis of the themes derived from our expert panel sessions.

5.1. Feasibility of extracting and generating contextual explanations from authoritative sources
For our feasibility analysis, we further analyze the results for type 3 questions with respect to the disease groups to gain a deeper understanding of the QA performance. Specifically, we want to understand whether SOTA LLM backed QA methods, potentially augmented with knowledge, are ready for real-world use for our states use-case as well as identify patterns that might apply to other CPGs in different disease areas. We pose a number of questions around this idea as follows:

Is CPG guideline suitable for T2DM contexts: How well are the disease subgroups among T2DM patients covered in the guidelines? Here, we attempt to understand the applicability of ADA 2021 CPG in our use-case. From Fig. 8, we can interpret that the guidelines cover a smaller number of disease groups9 than the patient data. Since CPGs are authoritative literature in their disease fields, their coverage is mainly limited to the primary disease area. Thus ADA 2021 CPG focuses on Diabetes, an Endocrine, Nutritional and Metabolic Disorder, and its comorbid conditions (mainly spanning diseases of the circulatory and genitourinary systems). Unsurprisingly, these patterns are seen in Fig. 8 as well where the Endocrine, Nutritional and Metabolic Disorder have the largest coverage in the guideline data, a 66%. In contrast, patients might have other conditions that do not arise from the T2DM diagnosis alone, and hence we can deduce that we see more diversity in disease groups in the patient data.

Can a single SOTA LLM method be used to extract the contexts? We attempt to understand if the LLMs are inherently better at certain disease groups over others. Fig. 9 shows the distribution of base LLM models over the disease groups. We can see that there are a few disease groups which have a higher MAP performance than others (towards the right end of the plot), some have their the box centers in the middle of the plot and others who are not doing as well since they are in the first quadrant of the plot. While the results are not strikingly decisive and statistically significant everywhere, in concordance with our overall results, we note that SciBERT and BERT models have better performance over most of the disease groups. Thus to further discern between these top 
 performing models, we conducted a point-wise analysis of relative performance difference between BERT and SciBERT (distribution of residual values between the MAP performances of BERT and Scibert under equal performance hypothesis). Fig. 10 shows the outcomes of the analysis where orange box indicates that BERT performs better on average while yellow indicates the same for SciBERT. We see that BERT is better for most disease groups, especially for ‘Disease of the blood and bone forming organs’, ‘Diseases of the digestive system’, ‘Diseases of the nervous system and sense organs’, ‘Endocrine, nutritional, and metabolic diseases and immunity disorders’, ‘Injury and poisoning’, and ‘Neoplasms’ (
 not contained in the inter-quartile range). SciBERT is only doing better on ‘Diseases of the respiratory system’ (and marginally better for ‘Mental Illness’). These results, in addition to the quantitative results, indicate that LLM models are better at addressing some disease groups than others. While vanilla BERT is a defensible choice, the results point to the need for domain adaptation for LLM for this problem. However, considering the limited availability of data, novel ML methods such as one-short learning and as weak supervised techniques may be required to improve the performances of these LLMs reliably across multiple disease groups. Does knowledge augmentation reliably improve QA methods? We proposed 
 possible strategies for Knowledge-augmentation (See Appending Appendix A.2). Among these settings, the best knowledge augmentation strategies reflected in Table 8 originated from a composite of strategies. As seen from Table 7, 8 and  9, the guideline QA’s best MAP score of 0.82 is obtained in a BERT 
 knowledge augmented setting on drug questions and among disease features, the guideline QA’s best MAP score is 0.438. The recall with its highest value of 0.405 is obtained in a post-filtering knowledge augmentation setting 5 of SciBERT ( Table 8), where we sort answers by disease overlap between question and answer and we also see the best BLEU score of 0.19 in this setting. These points to the fact that there is value in either filtering the answers to be passed to a LLM or sorting the answers from it, using aids from known domain knowledge sources that data sources like guidelines are expected to adhere to. We further analyzes these at disease sub-group level in Fig. 11 where we plot the lift in performance over corresponding base LLM using any particular strategy. While any one strategy is not found to be dominating the others, for most disease groups we can find one or more strategy that improves the performance (median lift greater than 
). These results support our previous insight that there is a value in augmenting domain knowledge. However, finding a single universal strategy is difficult and may need further research. Overall, how feasible it is to extracting contexts from guidelines? Which strategies are beneficial? Are the methods scalable? We have addressed 
 clinically relevant questions that provide context around 
 prototypical patients, their predicted risk, and the factors influencing their risk. We have implemented logical adaptations given what we know about the guideline data to improve the LLM model’s capabilities and performance. These adaptations include knowledge augmentation from well-used medical ontologies like Metamap and Snomed to improve semantic overlap and rule augmentation to address numerical range questions. Our baseline LLM, BERT itself, has a variable performance that does well on some questions and not on others. Similarly, the best performances on the LLM 
 knowledge augmentation approaches varies across pre-filtering settings 3 and 4, that filter by Metamap disease codes and Snomed disease hops and post filtering-setting 5 that sorts by Snomed disease hops. From our result evaluations, we see that the order of introducing the knowledge augmentation outputs impacts the accuracy scores, namely the MAP and recall. Mainly, pre-filtering the answer set before passing to a LLM can help it output more precise answers. In the best case, pre-filtering settings provide a gain of 4% over the baseline LLMs both for disease and drug questions (BERT-KA from Table 7 and BERT-KA from Table 9). Similarly, post sorting the answers from a LLM can improve the recall, and in the best case (SciBERT-KA from Table 8), we see a gain of 5% from the baseline LLM.

Our result numbers also indicate that unsupervised adaptations can only reach a certain accuracy and point to the need for domain adaptations to medical guidelines. Additionally, since we were dealing with a setting with little or no annotations on the ADA 2021 CPG, we had to create our own annotations. Currently, we are dealing with a relatively small annotated corpora (
 questions and 
 candidate sentences), and we consulted with a medical expert on our team to review these annotations. Even for this small corpora, we find that it is time-consuming for a clinical expert (s) to review the annotations or create them. We are exploring techniques like weak supervision to scale and improve the coverage of the annotations. In summary, our guideline QA results depict incremental gains in adding knowledge and rule augmentations to enhance LLMs’ performance and capabilities in domain applications and point to the need for supervised and semi-supervised approaches to improve these gains.

We are, to the best of our knowledge, the first to report any QA performance numbers on the ADA CPG 2021 dataset. Additionally, we are the first few who have tried a LLM approach for more scalable upstream tasks on medical guidelines like question answering than the current more time-consuming and dataset-dependent task of converting guidelines to rules and applying logical reasoning techniques over these rules [48], [49]. Our approach to guideline extraction and question answering (Fig. 4) is a step towards providing a more flexible way  [46], [50] to swap in guideline text from different diseases as needed. Our enhanced LLM approach (Fig. 4) can be applied to any medical text corpus like medical guidelines extracted into a machine-comprehensible format and can address different question types (as seen in Table 2) relevant in risk prediction settings.

5.2. Understanding the added benefit of the derived contexts
What were the takeaways and feedback from clinicians about the supported contextual explanations? The four major themes - Clinical Value of Explanations and Contextualizations, Highlighting Actionability, Connections to Patient Data, and Connections to External Knowledge Sources - that we found during the expert panel interviews to evaluate our contextualization approach, mainly point to the overall value of supporting different types of contexts, both from literature and patient data, and the need to better present connections between these contexts. Many of the contexts the clinicians on our expert panel were looking for were around the post-hoc explanations of the factors contributing to the risk. This finding corroborates a recent study that reports that post-hoc explanations themselves are insufficient to provide reasoning that clinicians can interpret and act upon [7], and also add to the well-accepted belief that risk scores are insufficient.

Do the supported data sources address the clinicians needs? Through further analysis, we find that the contexts the clinicians were looking for and discussing can be addressed either by connections to patient history and data – patients’ diagnoses, medications, and lab values – or through published literature. Specifically, we find that the different questions and question types (Table 2) that we support from the T2DM guidelines can address 
 of the 
 sub-themes (see Fig. 12), i.e., providing contextual information around patient’s T2DM state, their CKD risk and the individual features (Theme 1), highlighting the impact of CKD risk on treatment decisions for T2DM (Theme 2), providing links to published articles (Theme 4), and showcasing connections to patient clinical indicators (Theme 3) where mentioned. Some other themes can be easily addressed by enabling connections from the CKD risk scores and the features contributing to them, to patient timelines for diagnoses and lab values. Other themes - support for familiar categorizations (Theme 4) and the need for information on related diagnoses (Theme 3) - benefit from connections to medical ontologies that support either drill-downs to more specific diagnoses or abstracting up to higher-level pathways. We currently only support abstractions to higher level physiological pathways on the prototype dashboard (e.g., all disease of the circulatory system can be filtered from the patient’s feature importances) and are investigating how to support drill-downs based off of these pathways more broadly. What can be the impact of our contextual explanations beyond the comorbidity risk prediction setting? To identify specific scenarios in which our contextualizations might be most impactful, we discussed with a clinical expert typical situations of T2DM patient care by clinician type and patient characteristics to understand where risk predictions could help clinicians improve patient care planning. While current literature [13] on designing healthcare models points to a user-centered approach, from this understanding of clinician workflows, our discussion showed the importance of grounding such user centered work in specific clinical scenarios (see Fig. 13). For example, it became clear that dashboards containing contextual explanations around risk prediction could be used by clinicians in different ways. For example, a clinician seeing a patient for the first time and/or for the first diagnosis would be interested in creating their mental model of the patient’s diagnosis and understanding the causes for the risk, whereas a clinician seeing a returning patient with a previously established diagnosis (where clinicians might be more interested in understanding any changes to the risk prediction over time and the effectiveness of various interventions). Hence, based on these understandings, we formalized our use case to provide contextual explanations to a PCP around the predicted risk of CKD among new T2DM patients at their first diagnosis. Additionally, while we focus our approach in the risk prediction of CKD among T2DM patients, the contextualization approach can be applied to other comorbid risk prediction settings given access to authoritative guidelines in the disease area, and likewise, the themes that we analyze from our expert panel interviews are also general enough to be considered applicable in other disease settings. These themes indicate a larger need for AI systems to support insights from multiple data and knowledge sources and present them as actionable and contextual explanations [9] (also pointed out in the self-explanation scorecard from [51]). In summary, our approach is a step towards extracting clinically relevant context from different data and knowledge sources, including guidelines, patient data, and medical ontologies, and using these contexts to augment and explain answers to a list of clinically relevant questions that can help clinicians reason and interpret the risk predictions for patients.

What are some future directions that emerge from the clinician discussions? Some subthemes under Theme 2: Highlighting Actionability provide future directions, highlighting actionable factors and suggesting specific actions to reduce CKD risk are not currently addressed by our contextualization approach. These sub-themes require more investigation and development of methods to identify actionable, most relevant factors to CKD risk. Another point which we observed is that some of the factors that the model picked up on are not covered by the T2DM guidelines, and could either be factors only relevant to CKD, or are those that are not considered to be well-known enough to be covered in position statements like CPGs. We are also investigating how to combine insights from multiple guidelines (also mentioned in [52]) and if that would be useful. In summary, while some of these themes provide validation for the modules we currently support in our multi-method approach to provide context, others offer directions for us to build towards, such as enabling connections to external medication databases, supporting temporality in post-hoc explanations of risk, and efforts to better present answers in terms of relevance and actionability. We are also considering interviewing more user groups within the clinical domain to strengthen an understanding of where such a risk prediction tool would be most impactful. Future steps would also include a practical study in a clinical setting to further assess the utility of our method."
29,"KFPredict: An ensemble learning prediction framework for diabetes based on fusion of key features. BACKGROUND AND OBJECTIVE: Diabetes is a disease that requires early detection and early treatment, and complications are likely to occur in late stages of the disease, threatening the life of patients. Therefore, in order to diagnose diabetic patients as early as possible, it is necessary to establish a model that can accurately predict diabetes.

METHODOLOGY: This paper proposes an ensemble learning framework: KFPredict, which combines multi-input models with key features and machine learning algorithms. We first propose a multi-input neural network model (KF_NN) that fuses key features and uses a decision tree-based selection recursive feature elimination algorithm and correlation coefficient method to screen out the key feature inputs and secondary feature inputs in the model. We then ensemble KF_NN with three machine learning algorithms (i.e., Support Vector Machine, Random Forest and K-Nearest Neighbors) for soft voting to form our predictive classifier for diabetes prediction.

RESULTS: Our framework demonstrates good prediction results on the test set with a sensitivity of 0.85, a specificity of 0.98, and an accuracy of 93.5%. Compared with the single prediction method KFPredict, the accuracy is up to 18.18% higher. Concurrently, we also compared KFPredict with the existing prediction methods. It still has good prediction performance, and the accuracy rate is improved by up to 14.93%.

CONCLUSION: This paper constructs a diabetes prediction framework that combines multi-input models with key features and machine learning algorithms. Taking tthe PIMA diabetes dataset as the test data, the experiment shows that the framework presents good prediction results. This paper proposes an ensemble learning framework: KFPredict, which combines multi-input models with key features and machine learning algorithms. In order to more effectively combine the key features in the data set that have a greater impact on the prediction results, we first designed a multi-input neural network including the key feature side and the secondary feature side, and used a deeper network structure to extract information from key factors. Then we constructed our KFPredict ensemble learning framework by soft voting, adding three machine learning methods of SVM, RF and KNN to the framework to further improve the prediction accuracy. This model has higher accuracy than other single machine learning models. Compared with the prediction methods proposed by other researchers in recent years, the performance of our model has been improved to varying degrees.

However our work also has certain limitations. First of all, the patients in our data set are all female, and we did not take the gender factor into consideration; secondly, the ability of diabetics to handle blood sugar can directly affect the fluctuation trend of blood sugar, and the blood sugar changes of different patients can better reflect their disease conditions. we were unable to incorporate blood glucose changes into the predicted features. In our current study, the blood sugar changes were not included in the predictive characteristics.

In the future, we will collect our own datasets that will take into account patients of different genders. And we will consider the individual condition of each patient to establish a more personalized detection method. At the same time, the blood glucose fluctuation of each volunteer was collected through the CGM continuous blood glucose meter, and a prediction model that could integrate the blood glucose time series data and the patient's basic data was constructed. Ultimately, we will take some more advanced methods to optimize the data and model [25], [26], [27]. Considering the rapid development of artificial intelligence in medical imaging [11], the complications of diabetes such as diabetic foot, diabetic nephropathy and diabetic retinopathy, we will consider using more advanced medical imaging diagnostic methods to detect the complications of diabetes [28], [29], [30]."
30,"LASSO-based machine learning algorithm to predict the incidence of diabetes in different stages. BACKGROUND: Formal risk assessment is crucial for diabetes prevention. We aimed to establish a practical nomogram for predicting the risk incidence of prediabetes and prediabetes conversion to diabetes.

METHODS: A cohort of 1428 subjects was collected to develop prediction models. The LASSO was used to screen for important risk factors in prediabetes and diabetes and was compared with other algorithms (LR, RF, SVM, LDA, NB, and Treebag). Multivariate logistic regression analysis was used to construct the prediction model of prediabetes and diabetes, and drawn the predictive nomogram. The performance of the nomograms was evaluated by receiver-operating characteristic curve and calibration.

RESULTS: These findings revealed that the other six algorithms were not as good as LASSO in terms of diabetes risk prediction. The nomogram for individualized prediction of prediabetes included ""Age,"" ""FH,"" ""Insulin_F,"" ""hypertension,"" ""Tgab,"" ""HDL-C,"" ""Proinsulin_F,"" and ""TG"" and the nomogram of prediabetes to diabetes included ""Age,"" ""FH,"" ""Proinsulin_E,"" and ""HDL-C"". The results showed that the two models had certain discrimination, with the AUC of 0.78 and 0.70, respectively. The calibration curve of the two models also indicated good consistency.

CONCLUSIONS: We established early warning models for prediabetes and diabetes, which can help identify prediabetes and diabetes high-risk populations in advance. Prior to model construction, we used the LASSO regression method in machine learning to screen 28 risk factors affecting the development of prediabetes and diabetes. According to the screening results, we developed the utility of the nomogram model based on multiple logistic regression to predict the risk of prediabetes in healthy people and the risk of diabetes in prediabetes. Furthermore, the nomogram was subjected to 1000 bootstrap resamples for internal validation to assess their predictive accuracies. Early detection of prediabetes and diabetes patients, thus early intervention, early treatment, can effectively delay the process of the disease, reduce the occurrence of related complications, and ultimately improve the quality of life of patients.

Diabetes is a chronic metabolic disease. Due to the lack of understanding of the disease or the absence of symptoms of diabetes, nearly one-third of diabetes patients do not understand their own status [Citation18]. In the past 20_years, with the rapid progress of science and technology and the rapid development of social economy, China has now become the country with the largest number of person with diabetes in the world. A 1987–2019 Chinese Urban and Rural diabetes mortality survey showed that the age-standardized diabetes mortality increased by about 38.5% in urban areas and 254.9% in rural areas during the entire study period [Citation19]. In addition, uncontrolled diabetes will also lead to the occurrence of numerous complications [Citation20, Citation21], including retinopathy, coronary heart disease, high blood pressure, and nonalcoholic fatty liver disease, which will bring enormous economic pressure to patients and their families. Thus, it is very important to identify and screen prediabetes and diabetes high-risk groups. Implementing early lifestyle intervention strategies to prevent diabetes has resulted in a low rate of conversion from prediabetes to diabetes.

In order to reduce the impact of diabetes on patients, several teams have used different methods to build diabetes prediction models, including machine learning and artificial intelligence (AI) [Citation22, Citation23]. Compared with previous diabetes prediction models, the model constructed in this study firstly used the LASSO-logistics method to estimate the relationship between risk predictors and diabetes. The regularized LASSO method is a method to manage over-fitting and variable selection, which is more suitable for complex and variable factors. It is a deep learning algorithm that has been widely used in the construction of many disease prediction models [Citation16, Citation17]. The L1 norm of the coefficients was added to the function as a penalty term, and the sum of squares of residuals was minimized. A subset of predictors was selected and an interpretable logistic regression model was provided [Citation24].

The 28 candidate predictors were reduced to 14 and 7 potential predictors by examining the predictors-prediabetes and diabetes association by shrinking the regression coefficients with the LASSO method. Compared with the baseline models (logistic regression, RF, SVM, LDA, NB, and Treebag), the LASSO model performed better. Then, selected subset of predictors was incorporated into the multivariate logistic regression analysis and a nomogram was used to simplify the parameters in the model presentation. Results showed that age, Tgab, HDL-C, FH, TG, hypertension, Insulin_F, and proinsulin_F were predictors of normal to prediabetes, while FH, HDL-C, and proinsulin_E were predictors of prediabetes to diabetes. The AUC indexes of prediabetes and diabetes prediction models are 0.78 and 0.70, respectively, indicating that the model has reasonable performance.

In recent years, several diabetes-prediction models have also been developed in various populations around the world. A cohort study showed that the Cambridge model has been applied successfully to identify individuals with a higher risk of T2DM during follow-up [Citation25]. On the basis of demographic, lifestyle and simple anthropometric measures, AUSDRISK developed and validated a diabetes risk assessment tool for Australia [Citation26]. The most common predictors in these models were age, sex, family history of diabetes, waistline, body mass index and smoking status, use of antihypertensive medications, and physical inactivity [Citation3, Citation4, Citation6]. However, waistline, body mass index and smoking status were not independent risk factors for prediabetes and diabetes in our study. We considered that the biological disparity between the baseline and outcome is not large enough for minor variables to be identified as independent risk factors.

There is a direct positive correlation between increase in age and the incidence of diabetes. On the one hand, it may be related to the aging decline of human body function with the increase of age, and the cell apoptosis in the body can cause the disorder of glucose metabolism. A study of diabetes in a Mexican population, followed for seven years, suggests that elevated blood pressure independently induces impaired glucose metabolism, leading to prediabetes [Citation27]. On the other hand, it may be due to the decrease of activity with increase in age, and the slowdown of basic metabolism, which leads to weight gain [Citation28]. The analysis of cohort studies conducted by Wang Chao et al. [Citation29] further pointed out that the risk of diabetes in obese people is nearly four times higher than that in normal people. In addition, dyslipidemia is an important risk factor of glucose metabolism disorder. According to current literature studies, it is relatively certain that elevated TG level is a marker of high risk of diabetes. High levels of HDL-C have a protective effect on blood glucose balance, which in turn protects against the development of diabetes [Citation30]. This is consistent with our results.

Our study has several flaws. First, all the patients in this study are from 6 of 19 cities and counties in Hainan Province and the sample size is insufficient, negative results may be produced. Therefore, in order to increase the reliability of the model, it is necessary to randomly select a larger sample of patients in Hainan Province. Secondly, the relevant factors included in this study couldn’t include all potential factors related to prediabetes, the patient information input into the model needs to be further improved. More importantly, our predictive models have performed well in internal validation, but further evaluation of model effectiveness in external queues is needed."
31,"Long-Term Prediction of Blood Glucose Levels in Type 1 Diabetes Using a CNN-LSTM-Based Deep Neural Network. BACKGROUND: In this work, we leverage state-of-the-art deep learning-based algorithms for blood glucose (BG) forecasting in people with type 1 diabetes.

METHODS: We propose stacks of convolutional neural network and long short-term memory units to predict BG level for 30-, 60-, and 90-minute prediction horizon (PH), given historical glucose measurements, meal information, and insulin intakes. The evaluation was performed on two data sets, Replace-BG and DIAdvisor, representative of free-living conditions and in-hospital setting, respectively.

RESULTS: For 90-minute PH, our model obtained mean absolute error of 17.30 +/- 2.07 and 18.23 +/- 2.97 mg/dL, root mean square error of 23.45 +/- 3.18 and 25.12 +/- 4.65 mg/dL, coefficient of determination of 84.13 +/- 4.22% and 82.34 +/- 4.54%, and in terms of the continuous glucose-error grid analysis 94.71 +/- 3.89% and 91.71 +/- 4.32% accurate predictions, 1.81 +/- 1.06% and 2.51 +/- 0.86% benign errors, and 3.47 +/- 1.12% and 5.78 +/- 1.72% erroneous predictions, for Replace-BG and DIAdvisor data sets, respectively.

CONCLUSION: Our investigation demonstrated that our method achieved superior glucose forecasting compared with existing approaches in the literature, and thanks to its generalizability showed potential for real-life applications. Advantages and Limitation of the Proposed CNN-LSTM Model
In this article, we proposed a hybrid CNN-LSTM algorithm, for the prediction of BG concentration in people with T1D. Consisting of an automatic feature extraction component, CNN, and a sequence learner part, LSTM, our proposed CNN-LSTM demonstrated superior performance in extracting hidden features and correlations between various physiological variables, as well as learning their causal effect, to be used for forecasting future BG values.
Nonetheless, as seen in the “Patient-Wise Analysis” subsection, to obtain acceptable performance, CNN-LSTM, like all other DNN-based architectures, needs to be trained on a large enough data set. That is why the model generally performs better on the Replace-BG than the DIAdvisor data set. On the other hand, one of the primary drawbacks of dealing with physiological data sets is the large number of missing data points, which can have a substantial impact on model performance. As noted in the “Data Preprocessing” subsection, we used linear interpolation on the training set to address this issue when gaps in CGM data were shorter than 60 minutes; however, it would be more efficient to have a data set with fewest possible missing data points.
Comparison With Existing Algorithms
This study demonstrated the superiority of our proposed CNN-LSTM model over the ARX, SVR, LSTM, and CRNN models, in terms of both predictive accuracy metrics and clinical acceptability. This higher performance is due to a more sophisticated architecture comprised of stacks of convolutional and LSTM layers, which results in a more robust method for learning complex and hidden features in multivariate data sets as well as learning to predict abrupt changes in the CGM level caused by alteration in other variables, like food or insulin intakes. In addition, as illustrated in Figure 3, our proposed CNN-LSTM model is more capable of capturing rapid and abrupt changes in the CGM trend, owing to its capacity for learning the complex dynamics and correlations between variables in the data set. Sufficient data are required to produce the desired results, however, which accordingly raises the computational cost of the CNN-LSTM model as opposed to the reference models.
Conclusion and Future Work
In this article, we proposed a hybrid deep learning–based model, comprised of convolutional and LSTM layers, and proved its superior performance in predicting future BG levels, for two multivariate in vivo data sets of T1D patients, Replace-BG and DIAdvisor, respectively, over previously published models in the literature.
To account for intersubject variability, we used the FC, and trained the model on different train/test subsets of data set on a rolling basis with ratio of 80/20, to be able to leverage all patients both in train and in test subsets of both data sets. We found that the proposed method worked well for both short-term, 30-minute PH (Replace-BG: MAE 6.60 ± 0.76 mg/dL, RMSE 9.28 ± 1.31 mg/dL, and R2 97.92 ± 2.14%; DIAdvisor: MAE 6.92 ± 0.68 mg/dL, RMSE 9.81 ± 0.91 mg/dL, and R2 97.03 ± 1.24%) and long-term, 60-minute PH (Replace-BG: MAE 11.74 ± 1.66 mg/dL, RMSE 16.51 ± 2.19309 mg/dL, and R2 91.97 ± 3.33%; DIAdvisor: MAE 12.06 ± 1.87 mg/dL, RMSE 18.32 ± 2.76 mg/dL, and R2 91.23 ± 3.32%) and 90-minute PH (Replace-BG: MAE17.30 ± 2.07 mg/dL, RMSE 23.45 ± 3.18 mg/dL, and R2 84.13 ± 3.66%; DIAdvisor: MAE 18.23 ± 2.97 mg/dL, RMSE 25.12 ± 4.65 mg/dL, and R2 82.34 ± 4.54%). The clinical acceptability of the proposed model was further assessed using CG-EGA measures, as shown in Table 2. The suggested model was then trained and evaluated patient-by-patient to assess its robustness to intra-subject variability. Based on the results, it is observable that our suggested technique, like any other DNN-based methodology, is extremely dependent on the quality and size of the data set. In the future, we aim to leverage the transfer learning methodology, that is transferring the knowledge resulted from training on a large data set, to generalize the trained model on any unseen patient. Last, intensity and type of PA and stress level were not taken into account for BG prediction in this work. While we acknowledge the limitations introduced by this, we are currently planning to incorporate these signals into future studies."
32,"Machine learning and deep learning predictive models for type 2 diabetes: a systematic review Diabetes Mellitus is a severe, chronic disease that occurs when blood glucose levels rise above certain limits. Over the last years, machine and deep learning techniques have been used to predict diabetes and its complications. However, researchers and developers still face two main challenges when building type 2 diabetes predictive models. First, there is considerable heterogeneity in previous studies regarding techniques used, making it challenging to identify the optimal one. Second, there is a lack of transparency about the features used in the models, which reduces their interpretability. This systematic review aimed at providing answers to the above challenges. The review followed the PRISMA methodology primarily, enriched with the one proposed by Keele and Durham Universities. Ninety studies were included, and the type of model, complementary techniques, dataset, and performance parameters reported were extracted. Eighteen different types of models were compared, with tree-based algorithms showing top performances. Deep Neural Networks proved suboptimal, despite their ability to deal with big and dirty data. Balancing data and feature selection techniques proved helpful to increase the model's efficiency. Models trained on tidy datasets achieved almost perfect models.
 This section discusses the findings for each of the research questions driving this review.

RQ1: What kind of features makes up the database to create the model?
Our findings suggest no agreement on the specific features to create a predictive model for type 2 diabetes. The number of features also differs between studies: while some used a few features, others used more than 70 features. The number and choice of features largely depended on the machine learning technique and the model’s complexity.

However, our findings suggest that some data types produce better models, such as lifestyle, socioeconomic and diagnostic data. These data are available in most but not all Electronic Health Records. Also, retinal fundus images were used in many of the top models, as they are related to eye vessel damage derivated from diabetes. Unfortunately, this type of image is no available in primary care data.

RQ2: What machine learning technique is optimal to create a predictive model for type 2 diabetes?
Figure 3 shows a scatter plot of studies that reported accuracy and AUC (ROC) values (x and y axes, respectively. The color of the dots represents thirteen of the eighteen types of model listed in the background. Dot labels represent the reference number of the study. A total of 30 studies is included in the plot. The studies closer to the top-right corner are the best ones, as they obtained high values for both validation metrics. Figures 4 and _and55 show the average accuracy and AUC (ROC) by model. Not all models from the background appear in both graphs since not all studies reported both metrics. Notably, most values represent a single study or the average of two studies. The exception is the average values for SVMs, RFs, GBTs, and DNNs, calculated with the results reported by four studies or more. These were the most popular machine learning techniques in the included studies. RQ3: Which are the optimal validation metrics to compare the models’ improvement?
Considerable heterogeneity was found in this regard, making it harder to compare the performance between the models. Most studies reported some metrics computed from the confusion matrix. However, studies focused on statistical learning models reported hazard ratios and the c-statistic.

This heterogeneity remains an area of opportunity for further studies. To deal with it, we propose reporting at least three metrics from the confusion matrix (i.e., accuracy, sensitivity, and specificity), which would allow computing the rest. Additionally, the AUC (ROC) should be reported as it is a robust performance metric. Ideally, other metrics such as the F1-score, precision, or the MCC score should be reported. Reporting more metrics would enable benchmarking studies and models.

Summary of the findings
Concerning the datasets, this review could not identify an exact list of features given the heterogeneity mentioned above. However, there are some findings to report. First, the model’s performance is significantly affected by the dataset: the accuracy decreased significantly when the dataset became big and complex. Clean and well-structured datasets with a few numbers of samples and features make a better model. However, a low number of attributes may not reflect the real complexity of the multi-factorial diseases.
The top-performing models were the decision tree and random forest, with an similar accuracy of 0.99 and equal AUC (ROC) of one. On average, the best models for the accuracy metric were Swarm Optimization and Random Forest with a value of one in both cases. For AUC (ROC) decision tree with an AUC (ROC) of 0.98, respectively.
The most frequently-used methods were Deep Neural Networks, tree-type (Gradient Boosting and Random Forest), and support vector machines. Deep Neural Networks have the advantage of dealing well with big data, a solid reason to use them frequently [27, 28]. Studies using these models used datasets containing more than 70,000 observations. Also, these models deal well with dirty data.
Some studies used complementary techniques to improve their model’s performance. First, resampling techniques were applied to otherwise unbalanced datasets. Second, feature selection techniques were used to identify the most relevant features for prediction. Among the latter, there is principal component analysis and logistic regression.
The model that has a good performance but can be improved is the Deep Neural Network. As shown in Figure 4, their average accuracy is not top, yet some individual models achieved 0.9. Hence, they represent a technique worth further exploration in type 2 diabetes. They also have the advantage that can deal with large datasets. As shown in Table 2 many of the datasets used for DNN models were around 70,000 or more samples. Also, DNN models do not require complementary techniques for feature selection.
Finally, model performance comparison was challenging due to the heterogeneity in the metrics reported."
33,"Machine Learning and Smart Devices for Diabetes Management: Systematic Review (1) Background: The use of smart devices to better manage diabetes has increased significantly in recent years. These technologies have been introduced in order to make life easier for patients with diabetes by allowing better control of the stability of blood sugar levels and anticipating the occurrence of dangerous events (hypo/hyperglycemia), etc. That being said, the main objectives of the self-management of diabetes is to improve the lifestyle and life quality of patients with diabetes; (2) Methods: We performed a systematic review based on articles that focus on the use of smart devices for the monitoring and better management of diabetes. The search was focused on keywords related to the topic, such as ""Diabetes"", ""Technology"", ""Self-management"", ""Artificial Intelligence"", etc. This was performed using databases, such as Scopus, Google Scholar, and PubMed; (3) Results: A total of 89 studies, published between 2011 and 2021, were included. The majority of the selected research aims to solve a diabetes management problem (e.g., blood glucose prediction, early detection of risk events, and the automatic adjustment of insulin doses, etc.). In these studies, wearable devices were used in combination with artificial intelligence (AI) techniques; (4) Conclusions: Wearable devices have attracted a great deal of scientific interest in the field of healthcare for people with chronic conditions, such as diabetes. They are capable of assisting in the management of diabetes, as well as preventing complications associated with this condition. Furthermore, the usage of these devices has improved illness management and quality of life. The introduction of new technologies such as continuous glucose monitoring (CGM) devices, smart wearables (bracelets, smartwatches, smart clothing, and patches, etc.), and artificial pancreas (AP) development, and, arguably, the use of the data collected from these new tools, have revolutionized the overall diabetes management ecosystem over the past decade [94]. There are powerful AI methods for designing models that aim to prevent events such as hypoglycemia, predict the value of blood glucose levels, and predict the right amount of insulin to administer, all with the goal of improving the quality of life and illness management of people with diabetes, designing personalized management for each patient [95], and saving them from complications due to diabetes and early mortality [75].

Advanced tools that are used for diabetes management include continuous glucose monitors (CGM). Nine studies have used this device to collect the data needed for the development of smart systems [77,79,81,82,84,86,89,90,92]. Allam et al. [79] collected the inputs for a system that will be able to forecast future glucose concentration levels with prediction horizons (PH) of 15, 30, 45, and 60 min using a continuous glucose monitoring (CGM) device. Similarly, Pustozerov et al. [84] employed the CGM to develop a blood glucose prediction model to successfully support women with gestational diabetes (GDM).

New deep learning algorithms were developed to automate the diagnosis of DR. Retinal screening based on AI is an attainable, precise, and highly accepted method for the detection and monitoring of diabetic retinopathy. A sensitivity (DR: 83.3%, RDR: 93%) and a specificity (DR: 95.5%; DRR: 92.5%) were reported for both the automated screening of referable diabetic retinopathy and diabetic retinopathy, as shown in the study conducted by Sosale et al. [93]. Convolutional neural networks (CNNs) and a smartphone fundus camera were used to develop such a system.

Artificial intelligence enables patients with diabetes to make daily decisions about diet and activity. There are many applications designed to analyze the contents of meals and provide detailed information on the nutritional and caloric value of foods. Anthimopoulos et al. [78] developed an application for patients to help them assess the quality and caloric value of the food they eat. The management of diabetes is more effective when patients take a picture of their own food and evaluate what they eat.

Physical exercise has been identified as one of the most effective initial prevention strategies for diabetes in high-risk individuals. With wearable devices that record the number of steps and the duration and intensity of activities, daily activity levels can be tracked. These technologies allow for the monitoring of daily activity and may encourage a person to include activity as part of their routine to better stabilize their blood glucose levels. Yom-Tov et al. [52] designed a reinforcement learning algorithm-based system that aims to personalize messages for each patient’s situation in order to better encourage them to practice sports activities.

Among all the articles included, different techniques were employed with the aim of establishing a device that could properly manage or assist in the management of diabetes. By analyzing the results of the different papers, we can find that the most commonly employed function of the smart devices is the “prediction of blood glucose levels”, with a percentage of (36.84%). We can also see that the most used approach is classification, with a percentage of 52.6% compared to regression, which was used in 47.4% of the studies. However, it is necessary to note that in future clinical applications it will be required to perform longitudinal studies in order to measure both the inter- and intra-subject variability. On the other hand, to conduct such studies can be challenging because the large-scale deployment of wearables can be financially and technically challenging. Wearable technologies have sparked a lot of scientific interest in the field of healthcare, especially for patients with chronic conditions, such as diabetes, during the last two decades. They are capable of assisting in the treatment of diabetes, as well as preventing problems connected to the condition. Furthermore, the usage of these devices has improved diabetes control as well as quality of life.

This article provides a systematic review of intelligent systems developed for use as diabetes-control instruments. A thorough screening process identified 19 articles. These papers were evaluated and studied to obtain the various information needed to answer the review questions. These included the types and models of the smart devices used, different sensor-based methods, participant information, and the AI technologies and approach used.

In summary, it can be said that new digital technologies, big data-based analytics, and the application of AI to diabetes data will revolutionize the way diabetes and diabetes-related complications are treated, as well as their prevention and control."
34,"Machine Learning as a Support for the Diagnosis of Type 2 Diabetes. Diabetes is a chronic, metabolic disease characterized by high blood sugar levels. Among the main types of diabetes, type 2 is the most common. Early diagnosis and treatment can prevent or delay the onset of complications. Previous studies examined the application of machine learning techniques for prediction of the pathology, and here an artificial neural network shows very promising results as a possible valuable aid in the management and prevention of diabetes. Additionally, its superior ability for long-term predictions makes it an ideal choice for this field of study. We utilized machine learning methods to uncover previously undiscovered associations between an individual's health status and the development of type 2 diabetes, with the goal of accurately predicting its onset or determining the individual's risk level. Our study employed a binary classifier, trained on scratch, to identify potential nonlinear relationships between the onset of type 2 diabetes and a set of parameters obtained from patient measurements. Three datasets were utilized, i.e., the National Center for Health Statistics' (NHANES) biennial survey, MIMIC-III and MIMIC-IV. These datasets were then combined to create a single dataset with the same number of individuals with and without type 2 diabetes. Since the dataset was balanced, the primary evaluation metric for the model was accuracy. The outcomes of this study were encouraging, with the model achieving accuracy levels of up to 86% and a ROC AUC value of 0.934. Further investigation is needed to improve the reliability of the model by considering multiple measurements from the same patient over time. Many efforts are oriented towards improvements in diabetes prevention, diagnosis, and care. Applications of AI methods are the most advanced approach based on computational resources. Data obtained by clinical studies should be opportunely integrated within AI approaches, as well as information from investigations at the molecular and cellular levels. As an example, the role of parameters used in our work as features is the object of studies reported in the literature [30,31,32,33,34,35], and novel biomarkers for the evaluation of diabetes and diabetes-related complications could be added in the future, as evidenced by studies on the role of erythrocytes [36].
Since the numerousness of the data is a crucial point in representing a given phenomenon, our work has focused on being able to construct a dataset with large, high-quality data. A well-designed dataset is essential for the success of training and evaluating neural networks, as the quality and representativeness of the data will significantly impact the performance of the network. We used three public datasets to extract data, to introduce heterogeneity into the data. Data extracted were preprocessed to remove data with missing values for the features of interest, obtaining a final dataset of 13,687 individuals, i.e., with a similar number of individuals with and without T2DM. In this way, we obtained a balanced dataset with suitable numerousness. The features were selected for the evidence of relationships to T2DM and for the ease of obtaining them, being measurements of common practice.
We decided not to apply any data augmentation techniques, to preserve the quality of the information, which is fundamental for machine learning algorithms as they search for correlations within the data; all rows with implausible or missing values for at least one characteristic were eliminated. The use of a dataset of at least 13,000 samples represents the first step towards models with performances that increasingly represent their true capabilities on unknown data.
The use of a neural network as a machine learning model was chosen due to its ability to approximate any function with a high degree of precision [37]. These models have been extensively used in the diagnosis of various diseases such as tuberculosis [38], malignant melanomas [39], and neuroblastomas [40]. Furthermore, neural networks have shown the potential in enhancing predictive accuracy when the connections between variables are nonlinear or unknown. Studies have demonstrated that neural networks exhibit superior long-term predictive capabilities in bariatric surgery patients [41] when compared to linear [42] and logistic regression models [43].
Our study suggests that the model applied to the dataset generated can predict the T2DM state with very high performances, based on features chosen by the scientific literature [30,31,32,33,34,35].
The most significant features were blood glucose level, HDL level in the blood, diastolic blood pressure, gender, and weight, while triglycerides, age, BMI, and systolic blood pressure resulted less significant.
The ROC curve is a commonly used method for evaluating the performance of (binary) classification models. It uses a combination of the true positive rate (the percentage of correctly predicted positive examples, defined as recall) and the false positive rate (the percentage of incorrectly predicted negative examples) to obtain a snapshot of classification performance.
By analyzing ROC curves, one assesses the classifier’s ability to discern between, for example, a healthy and a sick population, by calculating the area under the ROC curve (Area Under Curve (AUC)). The AUC value, between 0 and 1, is equivalent to the probability that the result of the classifier applied to an individual randomly drawn from the sick group is higher than that obtained by applying it to an individual randomly drawn from the healthy group.
The higher the area under the ROC curve (AUC), the better the classifier. A classifier with an AUC higher than 0.5 is better than a random classifier. If the AUC is less than 0.5, then there is something wrong with the model. A perfect model would have an AUC of 1. ROC curves are widely used because they are relatively simple to understand, capture more than one aspect of classification (taking into account both false positives and false negatives), and allow for visual and low-effort comparisons of the performance of different types of models. In our study, the calculated ROC AUC value is 0.934. This value suggests a high predictive value for the method developed.
To verify that the heterogeneity of ethnicity does not bias the final results, we performed an analysis for each ethnic group, obtaining very similar results (see Supplementary Material).
As can be seen from Figure 4, the best single neural network (SGD) and the ensemble predictions appear to be calibrated, thus interpretable as probabilities of membership in one class or the other. This is also confirmed by the Brier score, whose extremely low values give us confidence about the accuracy of the predictions in probabilistic terms. The calibration of the models must be checked carefully because faulty calibration might result in bad decisions, and reporting both is crucial for prediction models [44]."
35,"Machine learning for diabetes clinical decision support: a review Type 2 diabetes has recently acquired the status of an epidemic silent killer, though it is non-communicable. There are two main reasons behind this perception of the disease. First, a gradual but exponential growth in the disease prevalence has been witnessed irrespective of age groups, geography or gender. Second, the disease dynamics are very complex in terms of multifactorial risks involved, initial asymptomatic period, different short-term and long-term complications posing serious health threat and related co-morbidities. Majority of its risk factors are lifestyle habits like physical inactivity, lack of exercise, high body mass index (BMI), poor diet, smoking except some inevitable ones like family history of diabetes, ethnic predisposition, ageing etc. Nowadays, machine learning (ML) is increasingly being applied for alleviation of diabetes health burden and many research works have been proposed in the literature to offer clinical decision support in different application areas as well. In this paper, we present a review of such efforts for the prevention and management of type 2 diabetes. Firstly, we present the medical gaps in diabetes knowledge base, guidelines and medical practice identified from relevant articles and highlight those that can be addressed by ML. Further, we review the ML research works in three different application areas namely—(1) risk assessment (statistical risk scores and ML-based risk models), (2) diagnosis (using non-invasive and invasive features), (3) prognosis (from normoglycemia/prior morbidity to incident diabetes and prognosis of incident diabetes to related complications). We discuss and summarize the shortcomings or gaps in the existing ML methodologies for diabetes to be addressed in future. This review provides the breadth of ML predictive modeling applications for diabetes while highlighting the medical and technological gaps as well as various aspects involved in ML-based diabetes clinical decision support.
 Our review was focused on ML prevention and management efforts specifically for type 2 diabetes. We drew out some observations and findings from the review study as follows—majority of the included works focused on developing early, rapid or minimally invasive T2D prediction systems, with risk assessment considered to be implicitly carried out while prediction. A host of research works such as those presented in (Battineni et al. 2019; Farran et al. 2019; García-Ordás et al. 2021; Kopitar et al. 2020; Lai et al. 2019; Lee and Kim 2016; Maniruzzaman et al. 2018; Olivera et al. 2017; Pei et al. 2019; Zheng et al. 2017;  Zou et al. 2018) have evaluated the predictive ability of various classifiers for T2D prediction with comparative analysis and some others have carried out the effectiveness of classifier predictions with different combinations of pre-processing (Maniruzzaman et al. 2018; Wang et al. 2019) and feature selection techniques (De Silva et al. 2020; Roy et al. 2021; Rubaiat et al. (2018). Logistic Regression, a traditional statistical technique for binary and multivariate analysis has been considered in many of the included works owing to its simplicity and ability to model the interrelationships between dependent and independent variables. Nusinovici et al. (2020) compared the efficiency of logistic regression with different ML models in predicting four different chronic diseases. They concluded that logistic regression is as efficient as ML models for disease risk prediction when the dataset considered has fewer incident cases and simple clinical predictors. On the other hand, a variety of ML algorithms have been applied to predict diabetes among which more common ones include Naïve Bayes, k-Nearest Neighbors, support vector machines, k-means clustering, decision trees, neural networks, ensemble models like random forests, gradient boosting etc.

The implicit risk assessment in many of the works if at all included was for the patient population in general and not at individual level, though some studies particularly investigated the role of certain specific risk factors in T2D development (Lee and Kim 2016; Peddinti et al. 2017; Perveen et al. 2019; Yokota et al. 2017). Many of the research works validated the results on test dataset derived from the original input dataset, with external validation on different dataset found in few of the works. A number of features were considered for predictive modeling of T2D including clinical (lab-based or physical examination parameters), lifestyle, demographic and some unconventional, non-invasive features (toenails, iris images), which makes comparison of results less straightforward owing to difference in accuracy with differing features. This was primarily due to use of different datasets that varied with respect to patient population and characteristics. It is thus imperative to validate the predictive ability and usefulness of ML predictive models on real life patient characteristics other than the input dataset considered.

In case of prognostic modeling, ML models have been proposed to predict prognosis of individuals with high risk to future incident T2D as well as prognosis of prevalent diabetes to potential complications. Both areas have huge clinical and economical value that target the core problem of identifying high risk candidates eligible for certain medical interventions/treatments so as to prevent future health degradation as well as healthcare costs.

An important and inevitable parameter of clinical decision support systems, i.e., interpretability is gaining lot of emphasis nowadays so as to develop explainable, white-box models. Kopitar et al. (2019) throws light on local v/s global interpretability with a case of diabetes prediction and stresses on the importance of local interpretation techniques for risk assessment at individual patient level. Table _Table77 enlists the major gap areas and underlying challenges in ML-based diabetes clinical decision support."
36,"Machine learning for predicting diabetic metabolism in the Indian population using polar metabolomic and lipidomic features. AIMS: To identify metabolite and lipid biomarkers of diabetes in the Indian subpopulation in newly diagnosed diabetic and long-term diabetic individuals. To utilize the global polar metabolomic and lipidomic profiles to predict the susceptibility of an individual to diabetes using machine learning algorithms.

MATERIALS AND METHODS: 87 individuals, including healthy, newly diabetic, and long-term diabetics on medication, were included in the study. Post consent, their serum was used to isolate polar metabolome and lipidome. NMR and LCMS were used to identify the polar metabolites and lipids, respectively. Statistical analysis was done to determine significantly altered molecules. NMR and LCMS comprehensive data were utilized to generate diabetic models using machine learning algorithms. 10 more individuals (pre-diabetic) were recruited, and their polar metabolomic and lipidomic profiles were generated. Pre-diabetic metabolic profiles were then utilized to predict the diabetic status of the metabolome and lipidome beyond glucose levels.

RESULTS: Mannose, Betaine, Xanthine, Triglyceride (38:1), Sphingomyelin (d63:7), and Phosphatidic acid (37:2) are some of the top key biomarkers of diabetes. The predictive model generated showed the receiver operating characteristic area under the curve (ROC-AUC) as 1 on both test and validation data indicating excellent accuracy. This model then predicted the diabetic closeness of the metabolism of pre-diabetic individuals based on probability scores.

CONCLUSION: Polar metabolic and lipid profile of diabetic individuals is very different from that of healthy individuals. Lipid profile alters before the polar metabolic profile in diabetes-susceptible individuals. Without regard to glucose, the diabetic closeness of the metabolism of any individual can be determined. Metabolic biomarkers of diabetes (as studied in the Framingham Offspring cohort) can convey diabetes risk in white people years before the glucose levels shoot up (Wang et al., 2011). This sparks the interest to find out what perturbations make the metabolism diabetic in the Indian population and how this can be utilized to identify diabetes-susceptibility in seemingly healthy individuals. With this objective, this study focused on deciphering metabolic reprogramming in diabetes. Metabolic perturbations are complex and are a reflection of both the genesis and response to the disease. ML algorithms were employed to capture the metabolic fingerprint of diabetes and utilize it for prognostic and diagnostic purposes.

The initial part of the study explores differential metabolites having the potential to be considered biomarker candidates in the Indian population. Statistical analysis provided insight into the relationships between metabolites and diabetes. Next, the study incorporated this knowledge into the pathways analysis, which pointed towards the reprogramming of metabolism in the disease case. High blood glucose levels in diabetes are well established by now. The presence of high glucose in the bloodstream increases flux in different directions downstream of glucose. More flux towards glycolysis is well expected. However, in our calculation, pyruvate levels increased while lactate levels decreased. This finding was in contrast with the Finnish men's study, where both pyruvate and lactate increased (Vasishta et al., 2022). Higher glucose increases the levels of other hexose sugars, even disaccharides like trehalose. Conversion into pentose sugar increases flux in the pentose phosphate pathway. Glycolysis intermediate dihydroxyacetone phosphate (DHAP) transforms glucose into phosphatidic acid (PA), which enhances glucose production in the liver (UT Southwestern Medical Center, 2014). PA in diabetic individuals reduces significantly while all other lipid classes/species have increases. Lower levels of PA in the blood can be attributed to the accumulation of PA in the liver in diabetic individuals. The conversion of the whole PA pool into other lipid classes/species is another factor for its decrease. PA, post-generation from DHAP, acts as a precursor for lipid species like diacylglycerol (DG), phosphatidylglycerol (PG), lysophosphatidic acid (LPA), etc. DG facilitates the generation of triacylglycerol (TG) and phosphatidylcholine (PC). PC makes the way forward for the generation of Phosphatidylethanolamine (PE), Phosphatidylserine (PS), Lysophosphatidylcholine (LPC), Lysophosphatidylethanolamine (LPE), etc. In the interconversion of different lipid classes, lots of free fatty acid is generated. This accumulation of free fatty acid (FFA) is responsible for the generation of Insulin Resistance (IR). FFA also has insulin-stimulatory action which potentiates an increase in insulin secretion. More insulin availability makes up for the insulin resistance and can act as one of the factors for some obese people not developing diabetes (Boden, 2003). Glycolysis product pyruvate transfers the acetyl group to coenzyme A. Acety-CoA then interconverts into different amino acids and their metabolism products. Amino acid metabolism is linked to lipid metabolism via glycine as the common node. DHAP gets converted into serine as well, which along with palmitoyl CoA leads to generation of dihydrosphingosine which results in lipids (ceramides, ceramide phosphates, and sphingomyelin), playing a crucial play role in cell signalling. The dysregulation of these lipid signalling molecules can aggravate insulin resistance (IR) by promoting immune cell activation, causing chronic low-grade inflammation, and interfering with insulin signalling pathways. Ceramides are considered to be toxic fat and a causative factor for IR hence T2DM (Kusminski and Scherer, 2019). This way, all polar metabolites and lipids are interlinked via different metabolic pathways (Fig. 3). Perturbation, even at a single locus, is capable of reprogramming the whole metabolism. Differential expression of some metabolites is linked to the genesis and persistence of diabetes, while others are linked to the body’s response mechanism to fight the disease. Increased levels of ketone bodies: 2-hydroxybutyrate, acetoacetate and isopropanol, leucine, glutamate, urea, phosphatidylethanolamine, phosphatidylcholine, and ceramides bring about phenomena like ketoacidosis, suppression of insulin secretion and sensitivity, at times even deterioration of _-cells (Chang et al., 2019; Davalli et al., 2012; Saasa et al., 2019; Xie et al., 2018). These events augment the pathogenesis of diabetes. To reverse the anomalies in metabolism, the body reprograms certain pathways to increase concentrations of certain metabolites such that normalcy can be restored. These metabolites include amino acids like tryptophan, isoleucine, alanine, histidine and lysine, 2-aminoadipate, cholate, betaine, S-sulfocysteine, acetate, carnitine, phosphatidylinositol (PI), etc help maintain insulin levels, increase insulin-mediated glucose uptake, reduce inflammation and regenerate islets of Langerhans (Chang et al., 2019; Dandare et al., 2021; Feng et al., 2013; Ferrell & Chiang, 2019; Inubushi et al., 2012; Krijt et al., 2021; Xu et al., 2019).

Altered metabolite levels provided a piece of subtle information about reprogrammed metabolism. However, the utilization of information for diagnostics remains largely unexplored. Primarily, this is because many metabolites remain undetected or masked under more prominent metabolite peaks. Also, statistical significance makes certain specific molecules dominate the analysis and low-abundant while critical molecules remain unnoticed, which leads to a loss of information conveyed by those critical but less abundant metabolites. To capture the metabolism holistically, in this work, ML was utilized. For polar metabolomics, the use of a complete NMR spectrum rather than using identified metabolite levels for ML features helped surpass the software and database limitations of identifying metabolites. Lipid species data rather than lipid class data provided deeper insights into the lipidome. ML model (Random Forest) trained on healthy vs. diabetic (ND) polar metabolome and lipidome performed well on both model validation and evaluation parameters. Other studies on major depression, cancer, myocardial ischemia and other cardiovascular diseases’ metabolomics have also utilized machine learning algorithms to develop prediction models with accuracy ranging from 80–100% (Bifarin et al., 2021; Cao et al., 2023; Moskaleva et al., 2022; Zheng et al., 2017). This accuracy is quite comparable to the resultant accuracy: 92–100% received on training different algorithms on healthy vs ND data.

When pre-diabetic individuals were tested using the Random Forest model trained on healthy vs. diabetic (ND) data, an estimate of the similarity of the polar metabolome or lipidome of the test individuals appeared closer to a diabetic person (Fig. 5A and B). It was observed that a person with lower BMI and HbA1C levels could have an unhealthier metabolic pattern as compared to a person with high BMI and HbA1C levels. This can help doctors identify altered metabolism that insinuates diabetes, beyond glucose levels or HbA1C and BMI. The proposed prediction model can be highly useful for fitness enthusiasts, people with a family history, and those who are at a high risk of developing diabetes to check where their metabolic profile falls on the healthiness spectrum concerning diabetes. This can aid in the decision-making of doctors to provide the right line of guidance/treatment to the diabetes susceptible population. Since the model prediction is quantitative, the scores can be utilized to see the progression, containment, or reversal of diabetes-like patterns to track prevention/treatment measures."
37,"Machine learning in precision diabetes care and cardiovascular risk prediction Artificial intelligence and machine learning are driving a paradigm shift in medicine, promising data-driven, personalized solutions for managing diabetes and the excess cardiovascular risk it poses. In this comprehensive review of machine learning applications in the care of patients with diabetes at increased cardiovascular risk, we offer a broad overview of various data-driven methods and how they may be leveraged in developing predictive models for personalized care. We review existing as well as expected artificial intelligence solutions in the context of diagnosis, prognostication, phenotyping, and treatment of diabetes and its cardiovascular complications. In addition to discussing the key properties of such models that enable their successful application in complex risk prediction, we define challenges that arise from their misuse and the role of methodological standards in overcoming these limitations. We also identify key issues in equity and bias mitigation in healthcare and discuss how the current regulatory framework should ensure the efficacy and safety of medical artificial intelligence products in transforming cardiovascular care and outcomes in diabetes.
 Ensuring good research practices
Clinical predictions rarely rely on a single factor and are most often multivariable by design. In 2015, to provide a standardized framework for the creation and reporting of such statistical models, the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD) statement was published [39]. This followed multiple reports, including systematic reviews of risk prediction models in type 2 diabetes, that showed widespread use of poor methods and reporting [122, 123] contributing to “avoidable waste” in research evidence [124]. Unfortunately, several subsequent reviews have shown poor adherence to these standards, particularly among studies using ML algorithms [125]. The increasing adoption of ML algorithms in clinical prediction modeling, despite the lack of clear incremental value beyond simpler methods such as logistic regression in most settings [115, 126], has prompted the original TRIPOD authors to update their statement (TRIPOD-AI, see [127]) researchers, clinicians, systematic reviewers, and policy-makers critically appraise ML-based studies. ML models should generally be used when processing large amounts of multi-dimensional or complex inputs (e.g. time-series from wearables, videos etc.), whereas head-to-head comparisons to traditional statistical models should be provided when feasible to assess the trade-off between performance, complexity, and interpretability.

Mitigating bias through AI
Since ML models learn from existing data and care patterns, they can perpetuate human and structural biases [128, 129] (Table _(Table1).1). Careful evaluation of the historical training data for health care disparities, ensuring that historically disadvantaged subgroups have adequate representation, review of model performance across key subgroups, and incorporating feedback from key stakeholders and patient representatives are some approaches that can be taken to mitigate bias [76, 128]. This is not a straightforward task and requires caution when assessing for confounders [130], since ML models have shown the ability to identify features such as race even when blinded to such labels [131]. Rapid advances in AI and ML have revolutionized the field of medicine and have identified new ways to optimize the management of diabetes and its cardiovascular complications. Nevertheless, several challenges remain, ranging from standardizing the assessment of model performance along with model interpretability and explainability to mitigating bias during both development and deployment. Acknowledging these challenges and fostering a collaborative environment between clinicians, researchers, sponsors, and regulatory agencies is a prerequisite to harness the full potential of AI in catalyzing the transition towards a more patient-centered approach to the care of diabetes and CVD.
"
38,"Machine learning models for diabetes management in acute care using electronic medical records: A systematic review Background: Machine learning (ML) is a subset of Artificial Intelligence (AI) that is used to predict and potentially prevent adverse patient outcomes. There is increasing interest in the application of these models in digital hospitals to improve clinical decision-making and chronic disease management, particularly for patients with diabetes. The potential of ML models using electronic medical records (EMR) to improve the clinical care of hospitalised patients with diabetes is currently unknown.

Objective: The aim was to systematically identify and critically review the published literature examining the development and validation of ML models using EMR data for improving the care of hospitalised adult patients with diabetes.

Methods: The Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) guidelines were followed. Four databases were searched (Embase, PubMed, IEEE and Web of Science) for studies published between January 2010 to January 2022. The reference lists of the eligible articles were manually searched. Articles that examined adults and both developed and validated ML models using EMR data were included. Studies conducted in primary care and community care settings were excluded. Studies were independently screened and data was extracted using Covidence® systematic review software. For data extraction and critical appraisal, the Checklist for Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS) was followed. Risk of bias was assessed using the Prediction model Risk Of Bias Assessment Tool (PROBAST). Quality of reporting was assessed by adherence to the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) guideline. The IJMEDI checklist was followed to assess quality of ML models and the reproducibility of their outcomes. The external validation methodology of the studies was appraised.

Results: Of the 1317 studies screened, twelve met inclusion criteria. Eight studies developed ML models to predict disglycaemic episodes for hospitalized patients with diabetes, one study developed a ML model to predict total insulin dosage, two studies predicted risk of readmission, and one study improved the prediction of hospital readmission for inpatients with diabetes. All included studies were heterogeneous with regard to ML types, cohort, input predictors, sample size, performance and validation metrics and clinical outcomes. Two studies adhered to the TRIPOD guideline. The methodological reporting of all the studies was evaluated to be at high risk of bias. The quality of ML models in all studies was assessed as poor. Robust external validation was not performed on any of the studies. No models were implemented or evaluated in routine clinical care.

Conclusions: This review identified a limited number of ML models which were developed to improve inpatient management of diabetes. No ML models were implemented in real hospital settings. Future research needs to enhance the development, reporting and validation steps to enable ML models for integration into routine clinical care. Among hospital admissions for patients with diabetes, only 5% of admissions were primarily for a diabetic issue indicating the remaining 95% of hospitalisations were due to a different co-morbidity [27]. Traditionally, diabetes in hospital is managed by the treating team overseeing care of the primary condition, with specialist endocrinology consultation available on request. As such, CDSS may be of benefit to those care givers who are not specifically qualified for diabetes management. For example, clinical decision support for prescribers has been shown to improve evidence-based insulin prescribing and glycaemic performance [28]. Additionally, clinical guidelines have been published by the American Diabetes Association (ADA) for the implementation of clinical decision support for diabetes care within EMRs [29].

Considering the negative implications of poor glycaemic management in hospitals, prediction models may assist clinicians in gauging the trajectory of glucose for hospitalised patients. Previous studies have utilised clinical data to predict the risk of hypoglycemia for inpatients using traditional statistical models with acceptable prediction performance [30], [31], [32]. Kilpatrick et al. [31] developed an alert system identifying patients at high risk of hypoglycaemia and integrated this into a clinical care workflow. They were able to achieve a reduction in hypoglycaemic events when patients at high risk of hypoglycaemia were identified to clinical staff. These early studies show the potential for predictive algorithms to identify high risk patients, allowing proactive early intervention for negative outcomes rather than the traditional model of reactive intervention. Since the volume of EMRs in healthcare is expanding at an exponential rate, traditional learning algorithms are insufficient to process such vast amounts of data, referred to as “big data” [33]. Instead, ML models have been shown to be effective in uncovering underlying patterns and for building predictive models using big data [34]. Therefore, development of ML models using EMRs may enable the identification of those hospitalised patients who are at highest risk of dysglycaemia and readmission. This would allow for early targeted specialist diabetes team review and would likely benefit patients, providers, and health services alike.

4.1. Model characteristics and performance
From a systematic review of all studies published in the last 11 years, we identified twelve studies that used ML models to predict adverse glycaemic events, readmission risk and total insulin dosage amongst adult patients admitted to hospitals using EMR data. Our quality assessment using the IJMEDI checklist [11] revealed potential critical shortcomings in methodology, indicating that these studies would have been rejected or needed major revisions if submitted to the International Journal of Medical Informatics. The PROBAST [8] tool, in addition, showed that all these studies were at high risk of bias. Consequently, we may suspect that the findings of these studies were flawed.

Two studies [14], [18] adhered to the TRIPOD [9] guideline when reporting their ML prediction models, while the remaining studies were limited in reporting the descriptions and performance characteristics of their prediction modelling details. Previous researchers stated that poor reporting is a common flaw of many studies which developed and validated multivariable prediction models [35], [36], [37]. While lack of external validations, failure of co-designing with clinicians and inadequate regulations are some of the key challenges in the development and implementation of ML models in healthcare [38], it is not surprising that many prediction models fail to translate to real-world clinical settings [9], [39]. To enable readers, care providers, and policymakers to assess which models are beneficial in which clinical scenarios [40], key details of how prediction models were developed and validated need to be transparent [41], [42]. Studies are recommended to improve the quality of their reporting by following the available guidelines [9], [43], [44] for reporting their ML models, enabling them for future clinical integration. According to our assessment, robust external validation was not performed on any of the studies included in this review. This casts significant doubt on the generalisability of their models. It is possible that carrying out such validations would have revealed additional methodological or performance issues.

It is important to measure how close the prediction results are to the actual observed outcome. In assessing the performance of clinical decision-making models, internal validation of model performance utilising, for example, a train–test split of available data, is known to be insufficient [45]. ML models should also be subjected to external validation using different datasets that were not used to develop the model [39], [46], [47], [48]. It is crucial to conduct external validations not only on targeted populations at various sites [45], [49], but also performance should be evaluated over time (temporal validation) [50]. External validation of such algorithms is usually conducted using two principal methods: discrimination and calibration [51], [52], [53]. Although AUC statistic is a metric for discrimination measurement of ML models, other factors such as model specification and complexity need to be considered when assessing model outcomes [54]. Calibration, a vital aspect of model validation, is required to be accurate to avoid making inappropriate decisions [45]. In this review, three studies assessed their model calibration by plotting the observed probability against the predicted probability of their desired outcome [18], [19], [23]. More importantly, the characteristics of the care environment needs to be considered for evaluation of ML models in healthcare [55]. Performance indicators such as decision curve analysis incorporates clinical consequences into evaluation of prediction models [56]. While discrimination and calibration were described as “not directly informative” metrics to clinical values in biostatistical practice, decision curve analysis overcomes this limitation by calculating a clinical net-benefit of prediction models to determine whether utilising predictive algorithms to make clinical decisions is more beneficial than harmful [57], [58]. Imbalanced data is one of the hurdles for supervised ML models; it occurs when there is a significant difference in the number of samples in each data class [59]. As a result, the model performance may be degraded [60]. Although two of the papers included in this review employed “resampling” technique to resolve data imbalance [22], [25], it is advised that researchers apply the existing strategies [61] and document the process in their reports.

A strength of all of our included studies was that input variables were chosen in consultation with clinicians and subject matter experts [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25]. Ruan et al. [20] described their use of feature selection algorithms such as greedy search and genetic algorithms for selecting input variables; however, they did not clearly specify what dataset they used to run the algorithms on. Greedy search and genetic algorithms are data preparation methods for finding an optimal set of features, especially when applying artificial intelligence models, to reduce the cost and running time of systems [62]. Medical nutrition therapy is one of the key components in diabetes management and previous researchers suggested that this therapy should be incorporated into glycaemic control of hospitalized patients [63], [64]. Four of our included studies included dietary intake as model input [14], [15], [18], [24].

Although the statistical performance of the ML models in this review were difficult to compare due to the heterogeneity in their cohort populations and modelling techniques, reviewing performance characteristics of these 12 studies suggested that ML models can result in improved clinical outcomes. Despite this, all studies in the present review were assessed as low quality, mainly due to inappropriate reporting of their methodologies and lack of external validation. As rigorous ML reporting guidelines and regulatory requirements continue to emerge, it is likely that this trend will change.

4.2. Towards a learning healthcare system
As healthcare organisations engage in digital transformation, they start to routinely collect digital data for every patient and use this aggregated data to make better decisions and to develop new models of care [65]. Learning Healthcare Systems (LHS) seek to collect health data from clinical practices and information systems in order to enhance clinicians’ decision-making [66] and the development of predictive analytics using ML techniques and routinely collected data can play a significant role in advancing greater patient care quality and safety. The identified studies in this review proposed innovative techniques using ML models to help achieve a LHS by improving diabetes care models that have potential for adoption in hospitals. This review may benefit those closely involved in ML-based prediction model development, evaluation and implementation to address in-hospital care of diabetes, including software engineers, data scientists and healthcare professionals.

Many ML models have been developed to tackle various problems in healthcare over the recent years, yet clinical implementation of ML models using EMR is still very limited with only 44 ML models implemented in clinical settings in 2020 [67]. For successful deployment of ML models into routine clinical care, clinicians’ engagement during development, validation and implementation is vital. It is suggested that clinical teams have a general understanding of how ML models were developed before adopting such models into routine clinical care [68]. While there are currently barriers for adoption of ML-based tools in healthcare such as inadequate performance evaluation, ethical issues, lack of trust by clinicians, regulatory ambiguity, the newly published guidelines [69], [70] may pave the way to address transparency, reproducibility, ethics and effectiveness challenges of ML-based tools in healthcare. Such recommendations may assist researchers on how to build and publish their ML models, serve as a starting point for editors and peer reviewers in assessing methodological quality and clinical relevance, and assist consumers in appraising their results critically. It has been recommended that the recruitment of ML editors in clinical journals would also help in identifying peer reviewers with expertise in machine learning algorithms to ensure the description of the methods is appropriate and accurate for their targeted readers [71]. As the quality of published research in this field improves, there will be a potential surge in the production of high-quality ML models, increasing the likelihood of their integration into routine clinical care. As a result, relevant workforce including researchers, data scientists, software developers and clinicians will be attracted to develop new models, leading to further advancements in this field.

4.3. Limitations
All the reviewed studies originated from either the United States, United Kingdom, Australia, Canada, China and India which limits the diversity of our assessment in other countries. Due to heterogeneity of ML model types, data sets, input predictors, measures of performance and validation, meta-analysis of extracted data on prediction models’ efficacy was not possible.

Since the literature is still evolving is this area, we excluded many studies which did not have clinical outcome assessment, resulting in a limited number of studies included in this review.

The distinction between minor and major revisions using the IJMEDI checklist [11] is a subjective matter in viewpoints of editor and reviewers prior to publication which may vary over time and as a result, this limits the applicability of this checklist for critically reviewing already-published literature in this review paper."
39,"Machine-Learning-Based Disease Diagnosis: A Comprehensive Review Globally, there is a substantial unmet need to diagnose various diseases effectively. The complexity of the different disease mechanisms and underlying symptoms of the patient population presents massive challenges in developing the early diagnosis tool and effective treatment. Machine learning (ML), an area of artificial intelligence (AI), enables researchers, physicians, and patients to solve some of these issues. Based on relevant research, this review explains how machine learning (ML) is being used to help in the early identification of numerous diseases. Initially, a bibliometric analysis of the publication is carried out using data from the Scopus and Web of Science (WOS) databases. The bibliometric study of 1216 publications was undertaken to determine the most prolific authors, nations, organizations, and most cited articles. The review then summarizes the most recent trends and approaches in machine-learning-based disease diagnosis (MLBDD), considering the following factors: algorithm, disease types, data type, application, and evaluation metrics. Finally, in this paper, we highlight key results and provides insight into future trends and opportunities in the MLBDD area.
 According to the International Diabetes Federation (IDF), there are currently over 382 million individuals worldwide who have diabetes, with that number anticipated to increase to 629 million by 2045 [71]. Numerous studies widely presented ML-based systems for diabetes patient detection. For example, Kandhasamy and Balamurali (2015) compared ML classifiers (J48 DT, KNN, RF, and SVM) for classifying patients with diabetes mellitus. The experiment was conducted on the UCI Diabetes dataset, and the KNN (K = 1) and RF classifiers obtained near-perfect accuracy [72]. However, one disadvantage of this work is that it used a simplified Diabetes dataset with only eight binary-classified parameters. As a result, getting 100% accuracy with a less difficult dataset is unsurprising. Furthermore, there is no discussion of how the algorithms influence the final prediction or how the result should be viewed from a nontechnical position in the experiment.

Yahyaoui et al. (2019) presented a Clinical Decision Support Systems (CDSS) to aid physicians or practitioners with Diabetes diagnosis. To reach this goal, the study utilized a variety of ML techniques, including SVM, RF, and deep convolutional neural network (CNN). RF outperformed all other algorithms in their computations, obtaining an accuracy of 83.67%, while DL and SVM scored 76.81% and 65.38% accuracy, respectively [73].

Naz and Ahuja (2020) employed a variety of ML techniques, including artificial neural networks (ANN), NB, DT, and DL, to analyze open-source PIMA Diabetes datasets. Their study indicates that DL is the most accurate method for detecting the development of diabetes, with an accuracy of approximately 98.07% [71]. The PIMA dataset is one of the most thoroughly investigated and primary datasets, making it easy to perform conventional and sophisticated ML-based algorithms. As a result, gaining greater accuracy with the PIMA Indian dataset is not surprising. Furthermore, the paper makes no mention of interpretability issues and how the model would perform with an unbalanced dataset or one with a significant number of missing variables. As is widely recognized in healthcare, several types of data can be created that are not always labeled, categorized, and preprocessed in the same way as the PIMA Indian dataset. As a result, it is critical to examine the algorithms’ fairness, unbiasedness, dependability, and interpretability while developing a CDSS, especially when a considerable amount of information is missing in a multiclass classification dataset.

Ashiquzzaman et al. (2017) developed a deep learning strategy to address the issue of overfitting in diabetes datasets. The experiment was carried out on the PIMA Indian dataset and yielded an accuracy of 88.41%. The authors claimed that performance improved significantly when dropout techniques were utilized and the overfitting problems were reduced [74]. Overuse of the dropout approach, on the other hand, lengthens overall training duration. As a result, as they did not address these concerns in their study, assessing whether their proposed model is optimum in terms of computational time is difficult.

Alhassan et al. (2018) introduced the King Abdullah International Research Center for Diabetes (KAIMRCD) dataset, which includes data from 14k people and is the world’s largest diabetic dataset. During that experiment, the author presented a CDSS architecture based on LSTM and GRU-based deep neural networks, which obtained up to 97% accuracy [75]. Table 6 highlights some of the relevant publications that employed ML and DL approaches in the diagnosis of diabetic disease."
40,"mHealth app using machine learning to increase physical activity in diabetes and depression: clinical trial protocol for the DIAMANTE Study Introduction: Depression and diabetes are highly disabling diseases with a high prevalence and high rate of comorbidity, particularly in low-income ethnic minority patients. Though comorbidity increases the risk of adverse outcomes and mortality, most clinical interventions target these diseases separately. Increasing physical activity might be effective to simultaneously lower depressive symptoms and improve glycaemic control. Self-management apps are a cost-effective, scalable and easy access treatment to increase physical activity. However, cutting-edge technological applications often do not reach vulnerable populations and are not tailored to an individual's behaviour and characteristics. Tailoring of interventions using machine learning methods likely increases the effectiveness of the intervention.

Methods and analysis: In a three-arm randomised controlled trial, we will examine the effect of a text-messaging smartphone application to encourage physical activity in low-income ethnic minority patients with comorbid diabetes and depression. The adaptive intervention group receives messages chosen from different messaging banks by a reinforcement learning algorithm. The uniform random intervention group receives the same messages, but chosen from the messaging banks with equal probabilities. The control group receives a weekly mood message. We aim to recruit 276 adults from primary care clinics aged 18-75 years who have been diagnosed with current diabetes and show elevated depressive symptoms (Patient Health Questionnaire depression scale-8 (PHQ-8) >5). We will compare passively collected daily step counts, self-report PHQ-8 and most recent haemoglobin A1c from medical records at baseline and at intervention completion at 6-month follow-up.

Ethics and dissemination: The Institutional Review Board at the University of California San Francisco approved this study (IRB: 17-22608). We plan to submit manuscripts describing our user-designed methods and testing of the adaptive learning algorithm and will submit the results of the trial for publication in peer-reviewed journals and presentations at (inter)-national scientific meetings. In this randomised controlled trial, we aim to examine the effect of a smartphone app that uses RL to predict the most effective messages for increasing PA in 276 low-income, ethnic and racial minority patients with diabetes and depression in urban public sector primary care clinics. We will compare this intervention to uniform random messages, delivered with equal and unchanging probabilities, and a control group that only receives a weekly mood message.

Decreasing health disparities
Though the numbers of mHealth pilot studies are increasing in vulnerable populations, many of these fail to follow through with an implementation component to the study design.42 Here, we are using a blended design: while the intervention is in addition to current care, there are ways we are attempting to make it more a part of patients’ clinical care. For instance, patients are mainly approached through primary care health providers, which recommend eligible patients whom they think are directly interested in a PA intervention. In addition, we will make patients’ data available to providers: a summary of the step increase for that patient at the conclusion of the study and updated PHQ-8 and GAD scores entered into the record. Our study therefore is the first step to addressing this gap because of its integration in primary care clinics that serve low-income patients. Future work should focus more specifically on implementation of the app as part of routine clinical care.

PA measure
We chose passively collected daily step count from patients’ preowned digital devices as a measure of PA. Although there are many different ways to measure PA, daily step count seems to be a particularly relevant measure because of: (1) its relative ease to measure, and (2) the clinical importance of individuals’ walking behaviour. Low number of daily step counts have been associated with all-cause mortality in some longitudinal studies43 and results from pooled population studies show clear dose–response effects of PA to overall mortality.44 In patients with type 2 diabetes, several studies have now shown that increasing step counts can significantly decrease HbA1c levels. For instance, a 10_000 steps per day walking prescription increased steps and decreased HbA1c in patients with type 2 diabetes.45 Further, Manjoo et al found that each SD increase in daily steps was associated with a 0.21% decrease in HbA1c.46 However, negative findings have also been reported. For instance, a meta-analysis by Qiu et al found that step-counter use was associated with increased steps per day (over 1800 more steps compared with a control group) among people with diabetes, but not with lowering of HbA1c.47

In exploratory post hoc analyses, we will also be able to examine the more immediate effect of PA messages, for example, on hourly steps in addition to daily steps, which will help to improve future PA interventions (eg, deliver messages at the right times). For instance, it is possible that one could receive a message in the morning and make plans to walk in the afternoon or evening, or messages could have more of an immediate impact. This information is currently unknown.

Personalisation of intervention
The results of this RCT will help us understand if adaptive mHealth interventions for depression and diabetes are more beneficial than interventions that do not use learning algorithms. If mHealth interventions are not personalised, their efficacy might be low, due to low engagement and high drop-out rates.20 The use of machine learning to adapt interventions according to users’ characteristics and behaviours is still in its early stages, but shows promise.48 For instance, Yov-Tom et al using an adaptive learning algorithm found that adaptive feedback messages were more effective in increasing the amount and speed of PA and also reduced HbA1c in sedentary patients with type 2 diabetes.33 Further, Zhou et al showed short-term efficacy of using adaptive weekly step goals determined by RL in healthy patients.49 The current study, with a relatively large group of patients, will further increase our understanding of the potential of machine-learning-driven text-messaging interventions.

Limitations and strengths
Limitations First, the results of this study might be specific to this population of low-income ethnic minority patients and might therefore not be generalisable to other populations. However, the inclusion of vulnerable patients in a primary care setting increases the likelihood that this intervention will be effective for other underserved populations. Further, our study procedures do not allow a double-blind design, as researchers and patients need to be made aware of the nature and frequency of messages they are receiving. Additionally, as with all digital interventions, technical issues might arise leading to unreliable step-count data and reduced ability for the algorithm to predict the most effective messages. The researchers and technical personnel will frequently check our server for data collection troubleshooting. Patients will also be made aware when their data are not collected correctly.
Strengths Diabetes and depression are among the top 10 causes of disability in the USA.50 Developing cost-effective and scalable models of care for patients with common chronic conditions has been postulated as of key importance in improving the performance of healthcare systems.51 If mHealth apps that target diabetes and depression through their common risk-factor physical inactivity are effective, they can have a major public health impact. Further, because the learning algorithm that we apply in this study is automated and delivers adaptive messages based on patients’ behaviours, it can potentially be applied in other patient populations with a wide range of conditions.
To conclude, the outcome of this trial will provide information on the effectiveness of a text-message-based smartphone app that uses machine learning to increase PA in low-income ethnic minority patients in primary care settings. The results will provide key information on the effectiveness of adaptive mobile applications, compared with more traditional static digital interventions. If effective, this application has the ability to decrease healthcare disparities by providing a type of personalised care to a diverse and traditionally hard to reach group of underserved patients."
41,"Nailfold capillaroscopy and deep learning in diabetes Objective: To determine whether nailfold capillary images, acquired using video capillaroscopy, can provide diagnostic information about diabetes and its complications.

Research design and methods: Nailfold video capillaroscopy was performed in 120 adult patients with and without type 1 or type 2 diabetes, and with and without cardiovascular disease. Nailfold images were analyzed using convolutional neural networks, a deep learning technique. Cross-validation was used to develop and test the ability of models to predict five5 prespecified states (diabetes, high glycosylated hemoglobin, cardiovascular event, retinopathy, albuminuria, and hypertension). The performance of each model for a particular state was assessed by estimating areas under the receiver operating characteristics curves (AUROC) and precision recall curves (AUPR).

Results: A total of 5236 nailfold images were acquired from 120 participants (mean 44 images per participant) and were all available for analysis. Models were able to accurately identify the presence of diabetes, with AUROC 0.84 (95% confidence interval [CI] 0.76, 0.91) and AUPR 0.84 (95% CI 0.78, 0.93), respectively. Models were also able to predict a history of cardiovascular events in patients with diabetes, with AUROC 0.65 (95% CI 0.51, 0.78) and AUPR 0.72 (95% CI 0.62, 0.88) respectively.

Conclusions: This proof-of-concept study demonstrates the potential of machine learning for identifying people with microvascular capillary changes from diabetes based on nailfold images, and for possibly identifying those most likely to have diabetes-related complications.
 The relationship between capillary disease and long_term diabetes consequences suggests that capillary morphology may contain diagnostic or prognostic information. This cross_sectional analysis of nailfold capillaroscope images shows that a deep learning approach can be used to distinguish between people with and without diabetes. It also demonstrates that it can identify people whose HbA1c level is in the diabetes range and suggests that in the subset of people with diabetes, it may identify individuals with cardiovascular disease. The observation that dysglycemia is reflected in morphologic abnormalities, easily visualized in nailfold capillaries, suggests that machine_read images of these capillary beds may have both diagnostic and prognostic value.

The harmful effect of hyperglycemia on small vessels has been recognized since the development of the ophthalmoscope in the middle of the 19th century 20 and the recognition of arteriolar and capillary pathology in the retinae of people with hyperglycemia. Indeed, the glycemic thresholds that are currently used to diagnose diabetes are based on glucose levels that are associated with a high prevalence of these retinal vascular abnormalities. 21 Moreover, much evidence shows that the capillary disease apparent in the retina is distributed throughout the body, including the brain, kidney, heart, and elsewhere, 5 and is linked to strokes, 22 cognitive decline, 22 cardiovascular disease, 23 , 24 kidney disease, and other long_term consequences of diabetes.

Nailfold video capillaroscopy has been used extensively by rheumatologists in the assessment of connective tissue diseases, with nailfold changes being associated both with disease presence and disease severity. 25 Several studies have attempted to examine nailfold video capillaroscopic changes in people with diabetes 26 , 27 , 28 , 29 , 30 , 31 , 32 and associations with complications such as retinopathy. Previous methods have relied on manual image analysis and qualitative or semiquantitative assessment of nailfold changes (such as capillary density, distribution, and morphology) that are difficult to standardize. Moreover, the literature has yielded mixed results about the association between nailfold changes and diabetes complication, 26 , 27 , 28 , 29 , 30 , 31 , 32 perhaps related to variability in approaches to image acquisition and analysis. Our deep learning approach offers promise as a more scalable and generalizable method that is not reliant on manual scoring.

The link between diabetes and retinal changes, and the widespread availability of retinal images, has prompted many studies exploring the value of machine learning based on these images. For example, in one study the application of machine learning to 115_344 retinal images from 57_672 patients discriminated diabetes from nondiabetes with an AUROC of 0.85. It also predicted a 2_fold higher incident of future diabetes. 33 Other studies have reported that machine_learning models can predict incidence cardiovascular events from retinal photographs. 34

The absence of large image repositories has precluded similar machine_learning analyses of nailfold capillary images. However, these findings add to prior manual assessments of nailfold images 30 , 35 and support ongoing studies of the role of machine learning for reading and interpreting such images. The fact that nailfold capillaries can be imaged easily, without the need for intravitreal dye injection and high_cost equipment needed to properly image retinal capillaries, further supports such studies and suggests that diagnostic and prognostic information in nailfold capillary images may supplement the information that is being read in retinal images.

Our proof_of_concept findings are limited by the small sample size, the highly selected population, the preponderance of males and White participants, and the absence of images from other organs beds such as the retina. They are also limited by the combination of different cardiovascular outcomes into one category of a cardiovascular event. These limitations may dilute the findings, leading to underestimates of the strength of the relationship between the images and specific cardiovascular outcomes. Similarly, the cardiovascular disease and retinopathy outcomes were established by history and not by active measurement, which may have also resulted in a more conservative estimate than the actual association. Nevertheless, our results clearly demonstrate the potential of deep learning for identifying people with tissue damage from diabetes based on nailfold images and for possibly identifying those most likely to have diabetes_related cardiovascular and other chronic consequences. They also clearly support the need to collect additional, more granular data to better elucidate the potential diagnostic and prognostic value of nailfold images."
42,"New-Onset Diabetes and Preexisting Diabetes Are Associated With Comparable Reduction in Long-Term Survival After Liver Transplant: A Machine Learning Approach Objective: To identify key predictors and survival outcomes of new-onset diabetes after transplant (NODAT) in liver transplant (LT) recipients by using the Scientific Registry of Transplant Recipients.

Patients and methods: Data of all adult LT recipients between October 1, 1987, and March 31, 2016, were analyzed using various machine learning methods. These data were divided into training (70%) and validation (30%) data sets to robustly determine predictors of NODAT. The long-term survival of patients with NODAT relative to transplant recipients with preexisting diabetes and those without diabetes was assessed.

Results: Increasing age (odds ratio [OR], 1.01; 95% CI, 1.00-1.02; P≤.001), male sex (OR, 1.09; 95% CI, 1.05-1.13; P=.03), and obesity (OR, 1.13; 95% CI, 1.08-1.18; P<.001) were significantly associated with NODAT. Sirolimus as a primary immunosuppressant carried a 33% higher risk of NODAT than did tacrolimus (OR, 1.33; 95% CI, 1.22-1.45; P<.001) at 1 year after LT. Patients with NODAT had significantly decreased 10-year survival than did those without diabetes (63.0% vs 74.9%; P<.001), similar to survival in patients with diabetes before LT (58.9%).

Conclusion: Using a machine learning approach, we found that older, male, and obese recipients are at especially higher risk of NODAT. Donor features do not affect risk. In addition, sirolimus-based immunosuppression is associated with a significantly higher risk of NODAT than other immunosuppressants. Most importantly, NODAT adversely affects long-term survival after LT in a manner similar to preexisting diabetes, indicating the need for more aggressive care and closer follow-up.
 Our study of NODAT represents the largest experience of this metabolic complication of LT and reports that NODAT has a significant adverse effect on long-term survival posttransplant that is comparable to that in preexisting diabetes. We discovered that use of the mTOR inhibitor sirolimus as a primary immunosuppressant was the most important predictor of NODAT. Sirolimus use increased the risk of new-onset diabetes by 33% as compared with tacrolimus-based immunosuppression. We used ML models to determine the factors most predictive of NODAT. This unique approach offers much greater precision in the identification of these factors than does standard logistic regression.

Patients with NODAT who underwent LT had significantly decreased survival in the long term, with a 55% higher risk of death at 10 years. This was surprisingly comparable to the 10-year survival of patients with preexisting diabetes, indicating that NODAT does have a dramatic effect on long-term outcomes. We speculate that this is due to adverse cardiovascular outcomes, as is the case for type 2 diabetes, although this was not a question we could address given the lack of granular data on patient outcomes. This significant adverse effect on the long-term survival of patients with NODAT is important to keep in mind when caring for patients who underwent LT in the long term. It suggests that patients should be screened closely for NODAT and managed more aggressively and optimally. This head-to-head comparison of NODAT with preexisting diabetes is a comparison that has not previously been made in studies of NODAT in LT.

Sirolimus has been found to be predictive of NODAT in the kidney transplant literature, 18-20 which represents that for the first time sirolimus has been clearly confirmed as a diabetogenic immunosuppressant in LT. Short-term administration improves insulin signaling and glucose uptake into muscle and adipose tissue through the mTOR pathway; long-term use of sirolimus has been associated with metabolic dysfunction in animal models. 21 In fact, switch of immunosuppression with calcineurin inhibitors to sirolimus has been shown to result in insulin resistance with a 30% increase in glucose intolerance in kidney transplant recipients. 19 This is explained at the molecular level by inhibition of 2 negative feedback loops in the mTOR pathway with chronic sirolimus use, which has the end result of producing insulin resistance. 21 In our recent study of the intestinal microbiome in the context of immunosuppressants, we discovered that exposure to sirolimus resulted in hyperglycemia that was significant enough not to be reversible with probiotic treatment. 22 This was in contrast to tacrolimus exposure, which led to changes in glycemia and the microbiome that responded to probiotic treatment. 22 High tacrolimus levels at 1 month after LT have been reported as a risk factor for NODAT. 23 However, this time point does not allow a comparison with sirolimus, given that sirolimus is introduced in patients who underwent LT only later in their course owing to safety concerns. On the basis of our findings on sirolimus, physicians should consider using calcineurin inhibitor–based immunosuppressant regimens in patients who develop NODAT.

An additional risk factor for NODAT was increasing recipient age, which has previously been identified in both liver 23 and kidney 24 transplant. Male sex was also a significant predictor of NODAT, which has been previously noted in kidney transplant. 24,25 Both age and male sex were nonmodifiable risk factors common to type 2 diabetes as well. 24 Recipient obesity was also significantly predictive of NODAT, whereas a BMI in the normal range was protective. Surprisingly, neither recipient nor donor race significantly affected the risk of NODAT, although this was a predominantly white patient population.

An improved understanding in the context of LT is important to implementing preventive and therapeutic measures, given the decreased survival and considerations unique to the transplanted liver relative to kidney transplant. Liver transplant recipients may also have conditions that, in and of themselves, generate an insulin-resistant state such as hepatitis C and fatty liver disease, although we did not find either of these conditions to be overrepresented in the NODAT group. In addition, the transplanted liver is denervated, preventing regulation of glucose metabolism by the hepatic branch of the vagus nerve.

Our study admittedly has certain limitations. The SRTR is unable to provide details on immunosuppression levels and glycemic control at time points after transplant. This would have been helpful in better understanding the effect of the immunosuppressants on the degree of hyperglycemia. Follow-up data at 1 year were not available in a large number of patients, given that SRTR follow-up data are based on questionnaire completion rather than chart review. Therefore, the exact incidence of NODAT in the population who underwent LT could not be determined. Pretransplant patient data on predictors of type 2 diabetes, such as prediabetes, history of gestational diabetes, family history of diabetes, and hyperlipidemia, were not captured in the questionnaire. In addition, it was left up to each individual program to make the diagnosis of diabetes in their clinical setting on the basis of established diagnostic criteria rather than any prospective assessment standardized across centers. Although deaths were documented in the SRTR database, the cause of death was not reliably recorded.

Nonetheless, this represents the largest experience of NODAT in LT to date, with the ability to assess the effect of NODAT on long-term survival in a large number of patients. The ML approach enabled a robust determination of key novel risk factors predictive of NODAT with greater precision than did standard logistic regression."
43,"Optimizing diabetes classification with a machine learning-based framework. BACKGROUND: Diabetes is a metabolic disorder usually caused by insufficient secretion of insulin from the pancreas or insensitivity of cells to insulin, resulting in long-term elevated blood sugar levels in patients. Patients usually present with frequent urination, thirst, and hunger. If left untreated, it can lead to various complications that can affect essential organs and even endanger life. Therefore, developing an intelligent diagnosis framework for diabetes is necessary.

RESULT: This paper proposes a machine learning-based diabetes classification framework machine learning optimized GAN. The framework encompasses several methodological approaches to address the diverse challenges encountered during the analysis. These approaches encompass the implementation of the mean and median joint filling method for handling missing values, the application of the cap method for outlier processing, and the utilization of SMOTEENN to mitigate sample imbalance. Additionally, the framework incorporates the employment of the proposed Diabetes Classification Model based on Generative Adversarial Network and employs logistic regression for detailed feature analysis. The effectiveness of the framework is evaluated using both the PIMA dataset and the diabetes dataset obtained from the GEO database. The experimental findings showcase our model achieved exceptional results, including a binary classification accuracy of 96.27%, tertiary classification accuracy of 99.31%, precision and f1 score of 0.9698, recall of 0.9698, and an AUC of 0.9702.

CONCLUSION: The experimental results show that the framework proposed in this paper can accurately classify diabetes and provide new ideas for intelligent diagnosis of diabetes. Result of data preprocessing
By counting the number of missing values, the results are shown in Table 5, which shows that Glucose, BloodPressure, SkinThickness, Insulin, and BMI contain missing values, among which SkinThickness and Insulin contain more missing values. The attribute Pregnancies represents the number of pregnancies, and it is reasonable for a value of 0 to exist in the dataset, indicating that some individuals have never been pregnant. Thus, it is considered appropriate and consistent with the nature of the attribute to refrain from filling in missing values for Pregnancies. After performing the mean median joint filling to handle missing values in the PIMA dataset, the distribution of the dataset is visualized in Fig. 6. The visualization provides insights into the distributions of different attributes. Specifically, it is observed that t after performing missing value imputation on the features Glucose, BloodPressure, SkinThickness, Insulin, and BMI, it is observed that the data distribution of these features tends to align more closely with a normal distribution. This indicates that the imputation process has effectively addressed the missing values, resulting in a more representative and reliable data distribution for these features.
To compare the efficacy of two outlier processing methods, the present study examines the results of utilizing the two methods with four machine learning models SVM, NB, KNN, and DT. Figure 7 illustrates the discernible trends across four models (SVM, NB, DT, KNN), wherein datasets treated with the capping method for outliers exhibited superior accuracy in comparison to datasets with directly removed outliers. Notably, the accuracy of capped datasets consistently surpassed the 70% threshold across all models. Conversely, the accuracy of datasets with directly removed outliers reached or exceeded 70% solely in the NB and SVM models. Based on these results, the present paper employs the capping method for outlier processing.
To address the issue of sample imbalance in the PIMA dataset, we employed the SMOTEENN hybrid sampling technique. Figure 8 showcases the result obtained after the application of this sampling method.
Figure 8 provides a detailed visualization of the significant improvements achieved by the SMOTEENN algorithm in addressing the issue of sample imbalance. The results presented in Fig. 8 clearly demonstrate a substantial reduction in the disparity of data labels after employing the SMOTEENN algorithm. Initially, the data suffered from a pronounced imbalance, with the ""1"" labeled samples being only half the number of the ""0"" labeled samples. However, through the implementation of the SMOTEENN mixed sampling technique, a significant decrease in label frequency variation was observed, effectively alleviating the previously observed data imbalance.

Result of correlation analysis
Upon exploring the correlation of features in the PIMA dataset, we generated a correlation coefficient heat map as illustrated in Fig. 9. The results indicated that Glucose exhibited a stronger correlation with the outcome compared to other features. To delve deeper into the impact of the features on the outcome, we further utilized logistic regression for conducting a correlation analysis. Given the limited interpretability of deep learning models, we have employed logistic regression to conduct correlation analysis. This approach enables us to quantify the specific degree of influence that features have on the results. By leveraging logistic regression, we aim to gain deeper insights into the impact of individual features on the outcomes, facilitating further investigations into their influence in subsequent analyses.

Table 6 presents the results of the logistic regression, revealing insightful findings on the relationship between the features and Outcome in the PIMA dataset. The results indicate that Pregnancies and Glucose have a significant effect on Outcome, while BloodPressure, SkinThickness, and Insulin do not. Specifically, for each unit increase in Pregnancies, the probability of Outcome being 0 decreases by 11.767%, and for each unit increase in Glucose, the probability of Outcome being 0 decreases by 3.633%. Similarly, BMI and DiabetesPedigreeFunction also have a significant effect on Outcome, with each unit increase in BMI leading to an 8.867% decrease in the probability of Outcome being 0, and each unit increase in DiabetesPedigreeFunction resulting in a 58.112% decrease in the probability of Outcome being 0. On the other hand, Age does not have a significant effect on Outcome as the significance p-value is 0.175, indicating that the original hypothesis cannot be rejected. Comparison with other models
Convolutional neural networks, deep neural networks, support vector machines, plain Bayesian, K-nearest neighbor algorithm, and decision trees were compared with our proposed DCSGAN using tenfold cross-validation, a commonly used method for machine learning model evaluation that assesses the generalization ability of the model. The original dataset was divided into 10 disjoint subsets, with one used as the validation dataset and the remaining nine used for training. The model was trained on the nine training datasets and evaluated on the validation dataset, and this process was repeated 10 times using different validation datasets. The final evaluation results were obtained by averaging the 10 evaluations, thus avoiding evaluation errors caused by inappropriate data partitioning.

According to the observations from Table 7, The DSGAN model demonstrated exceptional performance in both binary and tertiary classification tasks, achieving the highest accuracy rates of 96.27% and 99.31% respectively. Furthermore, the model exhibited impressive results across multiple evaluation metrics including precision, F1_score, recall, and AUC. Specifically, the precision, F1_score, recall, and AUC values were observed to be 0.9698, 0.9698, 0.9698, and 0.9702 respectively. These outstanding performance indicators affirm the effectiveness and robustness of the DSGAN model in accurately classifying the given data samples. And a comparative analysis with recent studies was conducted, as presented in Table 8. The findings reveal that our results yielded the highest accuracy rate, demonstrating the superior performance of our approach. In Fig. 10, we present a detailed depiction of the training process and the final confusion matrix achieved by the DCSGAN model. The visual representation clearly illustrates the exceptional classification ability demonstrated by our proposed model. The confusion matrix showcases the accurate assignment of samples to their respective classes, underscoring the model's robustness and effectiveness in accurately classifying the dataset. These findings provide compelling evidence of the outstanding performance exhibited by the DCSGAN model in the realm of classification.
"
44,"Optimizing Health Coaching for Patients With Type 2 Diabetes Using Machine Learning: Model Development and Validation Study Background: Health coaching is an emerging intervention that has been shown to improve clinical and patient-relevant outcomes for type 2 diabetes. Advances in artificial intelligence may provide an avenue for developing a more personalized, adaptive, and cost-effective approach to diabetes health coaching. Objective: We aim to apply Q-learning, a widely used reinforcement learning algorithm, to a diabetes health-coaching data set to develop a model for recommending an optimal coaching intervention at each decision point that is tailored to a patient's accumulated history. Objective: We aim to apply Q-learning, a widely used reinforcement learning algorithm, to a diabetes health-coaching data set to develop a model for recommending an optimal coaching intervention at each decision point that is tailored to a patient's accumulated history. Methods: In this pilot study, we fit a two-stage reinforcement learning model on 177 patients from the intervention arm of a community-based randomized controlled trial conducted in Canada. The policy produced by the reinforcement learning model can recommend a coaching intervention at each decision point that is tailored to a patient's accumulated history and is expected to maximize the composite clinical outcome of hemoglobin A1c reduction and quality of life improvement (normalized to [ _0, 1 _], with a higher score being better). Our data, models, and source code are publicly available. Conclusions: Applying reinforcement learning to diabetes health coaching could allow for  the automation of health coaching and an improvement in health outcomes produced by this type of intervention. The study took a novel approach of developing artificial intelligence using diabetes health-coaching data to better fit the needs of diabetes management and to achieve better health outcomes. Using historical observational data from a community-based randomized controlled trial, we developed a reinforcement learning model that can automate the task of personalized adaptive diabetes health coaching and demonstrates the potential to outperform human diabetes health coaches in maximizing a composite outcome of HbA1c reduction and QoL improvement. Our approach is also able to leverage data that is often overlooked, such as self-reported behavioral data, which allows us to generate personalized adaptive interventions for each patient using comprehensive health data. The model-based decision-making process is fully automated, which requires less involvement from health care professional resources. In practice, our model could be integrated into existing diabetes health-coaching programs to dynamically suggest personalized adaptive coaching interventions, either as a decision-making support tool for the diabetes health coaches or combined with a patient-facing mobile app to directly support patients with diabetes, which has the potential to reduce the cost and expand the reach of diabetes health coaching [32,33]. This study has several limitations. The internal working of the reinforcement learning model is difficult to interpret, and as a result, the model appears as a black box to health care professionals and patients, which may present a barrier to adoption in some clinical settings [34]. Due to the relatively small sample size, the data source for this study lacks heterogeneity, which may result in insufficient generalizability of the estimated optimal policy, despite its satisfactory performance on the study population. We plan to address this limitation in future work, which will seek to include a larger and more diverse group of patients. The aggregation of detailed diabetes health-coaching data into discrete intervention options may have led to a loss of fidelity, which in turn may translate into less optimal intervention recommendations. Future work in this area may look to more advanced statistical methods to fully use the fine-grained original coaching information to produce a better performance. Finally, diabetes health coach’s interventions can potentially have different consequences on patients due to the human factors (eg, patients’ adherence to coaching) that cannot be fully simulated, which may lead to lower performance in real-world clinical practice. Future work should investigate quantifying these human factors and including them in the reinforcement learning model. This pilot study presents a novel application of artificial intelligence in diabetes management and demonstrated that applying reinforcement learning to diabetes health-coaching data has the potential to automate coaching and yield substantial improvement in health outcomes. Future research will include applying the reinforcement learning approach to larger diabetes health-coaching data sets and exploring the feasibility and acceptability of diabetes health coaching supported by artificial intelligence."
45,"Personalized Blood Glucose Prediction for Type 1 Diabetes Using Evidential Deep Learning and Meta-Learning. The availability of large amounts of data from continuous glucose monitoring (CGM), together with the latest advances in deep learning techniques, have opened the door to a new paradigm of algorithm design for personalized blood glucose (BG) prediction in type 1 diabetes (T1D) with superior performance. However, there are several challenges that prevent the widespread implementation of deep learning algorithms in actual clinical settings, including unclear prediction confidence and limited training data for new T1D subjects. To this end, we propose a novel deep learning framework, Fast-adaptive and Confident Neural Network (FCNN), to meet these clinical challenges. In particular, an attention-based recurrent neural network is used to learn representations from CGM input and forward a weighted sum of hidden states to an evidential output layer, aiming to compute personalized BG predictions with theoretically supported model confidence. The model-agnostic meta-learning is employed to enable fast adaptation for a new T1D subject with limited training data. The proposed framework has been validated on three clinical datasets. In particular, for a dataset including 12 subjects with T1D, FCNN achieved a root mean square error of 18.64+/-2.60 mg/dL and 31.07+/-3.62 mg/dL for 30 and 60-minute prediction horizons, respectively, which outperformed all the considered baseline methods with significant improvements. These results indicate that FCNN is a viable and effective approach for predicting BG levels in T1D. The well-trained models can be implemented in smartphone apps to improve glycemic control by enabling proactive actions through real-time glucose alerts. To the best of our knowledge, FCNN is the first work that uses an attention-based GRU model for BG prediction, while incorporating model confidence and fast adaptation for the clinical benefits. There are several significant differences between the attention-based LSTM model presented by Mirshekarian et al. [34] and the proposed model in this work. Firstly, the authors evaluated the model on a previous version of the OhioT1DM dataset (2018 version), which contains six of the 12 T1D subjects in the 2020 OhioT1DM dataset that we used. If we evaluate our models on the six 2018 subjects, a 30-minute RMSE of 18.10 mg/dL can be obtained, which is lower than the best result of 18.70 mg/dL that the authors reported using the same experimental settings (i.e., agnostic scenarios without what-if events). Secondly, the authors used an additive form of attention module [28] to obtain a hidden state of LSTM cells with the largest attention weight, while we use a general form of attention mechanism [31] to calculate the weighted sum of the hidden states of bidirectional GRU cells. Finally, their attention module did not improve BG prediction for real clinical data, while ours significantly reduced RMSE by 0.45 mg/dL (p<0.005) in ablation analysis (Supplementary material S.VIII).

In the experiments, we have shown that FCNN is a superior prediction method when compared with the chosen baseline algorithms. In general, it is difficult to perform a fair comparison with the existing work, due to unavailable code, data, and experimental settings. A recent work proposed the glucose variability impact index (GVII) and glucose prediction consistency index (GPCI) as a method to assess the correlation between RMSE results of BG prediction and glucose variability [57], which can be used to compare algorithms across different studies. Here we measured glucose variability by means of the coefficient of variation (CV) and applied linear least-squares regression to obtain GVII and GPCI for each prediction method. However, it is to be noted that the correlation results on the OhioT1DM and ARISES dataset are not significant, possibly due to the small numbers of T1D subjects. Thus, we reported the GVII and GPCI results with Pearson correlation coefficients (r) and p-values (p) on the ABC4D dataset in Supplementary material S.VII. It is worth noting that the FCNN method achieved small GVII and GPCI results for both PHs, indicating that glucose variability has a low impact on the accuracy and consistency of BG prediction. Moreover, the FCNN framework can be adapted to many existing DNN models to improve their performance, such as the CNN [35], the CRNN [36] and the dilated RNN [37]. Particularly, such adaptation only involves three steps: replacing the dense top layer with evidential output layer, using the corresponding NLL loss, and applying the MAML procedures.

A limitation of FCNN is that the model outputs, i.e. the predictive BG levels, sometimes increase with an input of bolus insulin. Although it is reasonable when there is an upcoming meal, it would be more appropriate for a model to disentangle the effect of meal intake and insulin delivery before it can be used in clinical settings. This limitation is also found in other deep learning models, including the CRNN, Bi-LSTM and the transformer. To this end, there are two potential solutions for future work. One is to introduce monotonic constraints in the DNNs to specify the insulin’s negative effect on BG levels, such as restricting the layer weights of shallow networks [58] and training with heuristic regularizations [59]. The other is to incorporate physiological models to process these events, such as composite minimal models of glucose regulation [60]. Real-time hypoglycemia detection based on BG prediction is a challenging task. It is to be noted that, in the literature, the sensitivity of 75% and precision of 51% [61], and the sensitivity of 59% and precision of 68% [55], were obtained for a 30-minute PH, while an MCC score of 0.51 with the sensitivity of 48.5% was achieved for a 60-minute PH [62]. Therefore, the performance of FCNN (Table IV) is better than the state of the art. A potential improvement can be made by introducing another regularizer into the loss function to penalize the error of hypoglycemia detection (Equation (13)), although it may result in a larger RMSE for BG prediction. Meanwhile, we noted that the normalized time index used in this work lacks continuity for the time around midnight. Thus, we will explore sine and cosine embeddings to model time sequences in the future [16]. We are also planning to include other data features known to influence BG levels [63], such as the vital signs measured by wrist bands in the OhioT1DM and ARISES datasets, as well as the biomarkers that can be easily derived from available measurements (e.g., heart rate variability [64]). In addition, developing a population model to enable BG prediction [57] and hypoglycemia prediction [65] for a new T1D subject without model fine-tuning or personalized data is an interesting and relevant future work to be considered. However, a dataset with a much larger sample size such as the Tidepool Big Data Donation Dataset [57], may be required to capture the effects of inter-subject variability. Finally, FCNN can be used as an encoder in deep reinforcement learning to extract hidden features from physiological environment and improve decision support systems and automated insulin delivery algorithms (e.g., artificial pancreas) [66]–_[68].

To translate the potential clinical benefits of FCNN to people living with T1D, we are planning to evaluate it in an actual clinical setting, following the system architecture displayed in Fig. 1. A fast-adaptive meta-model is trained by using a historic dataset, for which the personalized fine-tuning will be performed with the newly collected data in the upcoming trial. Although weeks or months of data are required to achieve the reported performance in Table I, II and III, it is not unusual for CGM users to have such an amount of data collected [69]. As shown in Supplementary material S.IX, we implement the FCNN model in an iOS platform to provide real-time predictions and display the upper and lower bounds in the app interface to indicate the risk of adverse glycemic events. To test the on-device performance, we converted the trained model to a mobile-compatible format using TensorFlow Lite. It turns out that FCNN only consumes 1.15 MB of storage memory with an average execution time of 4 ms for model inference, which is proven to be a feasible approach on a mobile platform to provide real-time glucose alerts. In future work, the FCNN model with only CGM input will also be investigated with smartphone implementation and edge devices [70] to provide sub-optimal BG prediction for T1D subjects whose meal and insulin data are not available."
46,"Predicting adverse outcomes due to diabetes complications with machine learning using administrative health data Across jurisdictions, government and health insurance providers hold a large amount of data from patient interactions with the healthcare system. We aimed to develop a machine learning-based model for predicting adverse outcomes due to diabetes complications using administrative health data from the single-payer health system in Ontario, Canada. A Gradient Boosting Decision Tree model was trained on data from 1,029,366 patients, validated on 272,864 patients, and tested on 265,406 patients. Discrimination was assessed using the AUC statistic and calibration was assessed visually using calibration plots overall and across population subgroups. Our model predicting three-year risk of adverse outcomes due to diabetes complications (hyper/hypoglycemia, tissue infection, retinopathy, cardiovascular events, amputation) included 700 features from multiple diverse data sources and had strong discrimination (average test AUC = 77.7, range 77.7-77.9). Through the design and validation of a high-performance model to predict diabetes complications adverse outcomes at the population level, we demonstrate the potential of machine learning and administrative health data to inform health planning and healthcare resource allocation for diabetes management. This research demonstrated the feasibility of applying machine learning methods to administrative health data for public health planning. Our model can predict the 3-year risk of adverse outcomes due to diabetes complications (hyper/hypoglycemia, tissue infection, retinopathy, cardiovascular events, and amputation) with a test AUC of 77.7 (range 77.7–77.9, Fig. _Fig.2).2). It was not our goal for this model to be used for individual level patient care. Our model was trained on data from over 1.5 million patients from Ontario, which is among one of the most diverse populations in the world and, to our knowledge, one of the largest prediction modelling studies that takes into account multiple types of diabetes complications22–34,51–53. Our model was also well-calibrated and showed good discrimination.

While diabetes complications have been better managed in recent years, they remain a large burden because the incidence of diabetes continues to grow and even in the presence of interventions, not all cases can be prevented54. Thus, there is a need to effectively manage diabetes complications at both the individual patient and system levels. This is further emphasized as increasing age and years lived with diabetes have been found to independently predict diabetes morbidity and mortality55. Moreover, it has been well established that the complications of diabetes drive costs4,5. In Ontario alone, with a population of 14.5 million in 2019, adverse outcomes due to diabetes complications had an annual cost of over $3.9 billion, making diabetes a critical condition that warrants investment into analytic data-driven solutions for health system planning.

Health systems planning, for diabetes and other conditions, requires accurate assessments of population risk35,36. From the cost analysis in Fig. _Fig.4b,4b, we observe that the top 1% of most at-risk patients predicted by our model account for over $440_M or 11% of the overall annual cost. This increases to over $850_M or nearly 22% of the top 3% of patients’ total cost. In contrast, random selection would only capture 1% and 3% of the cost, respectively. Targeting policy interventions (e.g., subsidizing access to fruits and vegetables, community planning to facilitate active transportation) and resource allocation (e.g., incentivizing physicians to have more intensive diabetes follow-up care, either virtually or in-person) to communities projected at highest risk based on our model outputs could help maximize their effectiveness in changing the trajectories of diabetes complications11,36.

The observed differences in model calibration across complication type may be impacted by the inclusion of both episodic and progressive types of diabetes complications, which by nature have a different epidemiology and trajectory2. More specifically, episodic complications, such as tissue infection, can be treated and recur multiple times, whereas progressive complications, such as cardiovascular disease, generally result over an elongated period of time due to chronic damage to the organ system2. The overprediction of high-risk individuals could also be due to the relationship between age and years lived with diabetes as key drivers of complications55. Finally, it is possible that our overprediction of those at high risk could be due to the lack of valuable clinical features such as body mass index, smoking status, biomarkers in AHD. At the population level, applications that overpredict would still be appropriate for targeting resources and identifying individuals that would benefit from closer follow-up, including the use of other prediction models which include biomarkers and other individual risk factors.

The analysis of top features in Fig. _Fig.33 provides insight into the types of information used by our model to make predictions for each complication. Explainability is a major benefit of decision tree models, and is one of the main reasons why we focus on decision trees for this study. Administrative health databases typically have billions of records spread across multiple datasets making it highly challenging to work with. Moreover, predictive patterns inferred by the model at this scale can identify new trends at the population level (or validate existing hypotheses)56. In Fig. _Fig.3,3, we observe that socio-demographic factors such as length of stay in Canada for immigrants and ethnic concentration in the area of residence, play an important role in model prediction. We consistently found that features based on immigration status, age/sex, area of residence (particularly census statistics such as neighbourhood-level income, unemployment, ethnic concentration etc.) and other related information appeared within the top 20 most predictive features. This is also observed from Fig. _Fig.4d,4d, which shows that there are significantly fewer immigrants and lower ethnicity marginalization in the top 1% of the most at-risk patients predicted by the model as compared to the full cohort. Lower proportion of immigrants aligns with previous studies showing that immigrants have a different diabetes trajectory and are less at risk for these complications57. Clinical prediction models generally exclude such types of features and mainly focus on health data for each patient. Our results indicate that the social determinants of health, even at the census level, can be highly predictive for severe health outcomes. Thus the application of a model such as ours for population health planning, which leverages detailed information on the social determinants to allocate resources and plan policies to improve diabetes complications outcomes could offer a data-driven approach to addressing health disparities58–60.

Our study features a number of important strengths and contributions. The proposed model was developed and tested on a large cohort of over 1.5 million patients with minimal exclusion criteria, capturing virtually all incidences of target adverse outcomes. The cohort is ethnically diverse, with wide representation from across world regions. We demonstrate the applicability of machine learning methods using population data available in multiple jurisdictions around the world. We conducted extensive feature engineering and selection to capture correlations between different AHD sources and target outcomes. The final model has over 700 features from a variety of datasets such as demographics, census information, laboratory results, diagnosis history, physician billing claims, hospitalization and ambulatory usage, prescription medication history and others. Given the nature of administrative data, we believe that our approach could be applied for the forecasting of other chronic diseases at the population level. This is especially important given rising rates of multimorbidity internationally. One study in 2009 found that 24.3% of Ontarians were diagnosed with multiple comorbidities61. Since AHD is thought to be the most basic level of information collected by a healthcare system, we believe that our approach to population-level risk prediction would be feasible in other jurisdictions with universal health coverage and databases suitable for linkage such as the Scandinavian countries, United Kingdom, Australia, and New Zealand or within large private insurers in the United States. Finally, modern machine learning approaches are often criticized for lack of interpretability, and are sometimes referred to as black-box models62. Using a model based on decision trees enabled us to determine which features are important for prediction, and how they are combined inside the model. This is important for transparency and practical deployment of such systems that clinical and health system specialists need to be audited.

Despite the mentioned strengths, our study also has several important limitations. First, we are limited by the algorithm that we used to flag diabetes and build our cohort42. This “2-claim” algorithm has a specificity of 97%, meaning that there are almost no healthy patients in our cohort. However, its 86% sensitivity means that we did not capture all patients diagnosed with diabetes. Moreover, we are working with a joint cohort of patients diagnosed with type 1 and patients diagnosed with type 2. While patients diagnosed with both types share the complications and adverse outcomes we explored in this paper, their diabetes trajectory differs, with type 1 patients typically being diagnosed at a much younger age2. We considered using a validated type 1 diabetes algorithm63 to identify and remove the type 1 subcohort, but with a sensitivity of 80.6% on administrative health data, it would leave out hundreds of patients diagnosed with type 1 in our cohort. We argue that it is preferable for a system-level analysis to predict adverse outcomes of diabetes complications from both patients diagnosed with type 1 and patients diagnosed with type 2 since systems-level barriers are shared between the two populations11. We focused on hospitalization and ambulatory care service usage due to diabetes complications. Hence, we do not account for associated adverse events treated in the primary care settings as they could not be identified accurately in our data. In addition, we lacked prescription information for individuals under 65 years old (Ontario’s health system provides age-based drug coverage for individuals 65 years and older and those receiving social assistance). If available, they may improve predictive performance even further. More generally, as AHD systems around the world are being increasingly integrated with other data sources such as EHRs, we can believe that our models could be retrained to leverage newly linked databases, with increased discriminative performance. However, as it has been shown with EHRs64, one must always keep in mind that AHD reflect not only the health state of a patient but also the interactions they had with the healthcare system. Our temporal sliding window framework is robust to the bias of events in the administrative data reflecting past true health states (time of diagnosis is posterior to the time when symptoms started). Our model learns correlations between observed events and target adverse outcomes. Most of these correlations are not causal, and cannot be used to explain why a specific outcome has occurred. Inferring causal relationships would require a different conceptual and analytic framework, which is for future work65. Finally, as with other predictive models, external validation with recalibration and prospective validation as well as monitoring for distribution shifts over time would be important to conduct prior to widespread implementation and adoption.

In conclusion, we outline the development and validation of a machine learning model to predict adverse outcomes due to a range of diabetes complications three years ahead at the population level using routinely collected administrative data. We believe that after such models are externally and prospectively validated, public health officials will have a powerful tool for the ongoing risk assessment and cost-effective targeting of prevention efforts and resource allocation related to diabetes complications care at a population-scale."
47,"Predicting Immunological Risk for Stage 1 and Stage 2 Diabetes Using a 1-Week CGM Home Test, Nocturnal Glucose Increments, and Standardized Liquid Mixed Meal Breakfasts, with Classification Enhanced by Machine Learning. Background: Predicting the risk for type 1 diabetes (T1D) is a significant challenge. We use a 1-week continuous glucose monitoring (CGM) home test to characterize differences in glycemia in at-risk healthy individuals based on autoantibody presence and develop a machine-learning technology for CGM-based islet autoantibody classification. Methods: Sixty healthy relatives of people with T1D with mean +/- standard deviation age of 23.7 +/- 10.7 years, HbA1c of 5.3% +/- 0.3%, and body mass index of 23.8 +/- 5.6 kg/m2 with zero (n = 21), one (n = 18), and >=2 (n = 21) autoantibodies were enrolled in an National Institutes of Health TrialNet ancillary study. Participants wore a CGM for a week and consumed three standardized liquid mixed meals (SLMM) instead of three breakfasts. Glycemic outcomes were computed from weekly, overnight (12:00-06:00), and post-SLMM CGM traces, compared across groups, and used in four supervised machine-learning autoantibody status classifiers. Classifiers were evaluated through 10-fold cross-validation using the receiver operating characteristic area under the curve (AUC-ROC) to select the best classification model. Results: Among all computed glycemia metrics, only three were different across the autoantibodies groups: percent time >180 mg/dL (T180) weekly (P = 0.04), overnight CGM incremental AUC (P = 0.005), and T180 for 75 min post-SLMM CGM traces (P = 0.004). Once overnight and post-SLMM features are incorporated in machine-learning classifiers, a linear support vector machine model achieved the best performance of classifying autoantibody positive versus autoantibody negative participants with AUC-ROC >=0.81. Conclusion: A new technology combining machine learning with a potentially self-administered 1-week CGM home test can help improve T1D risk detection without the need to visit a hospital or use a medical laboratory. Trial registration: ClinicalTrials.gov registration no. NCT02663661. In this study, we used data from a recent NIH-funded TrialNet ancillary study using relatives of people with T1D of 12–42 years of age to characterize the extent to which features derived from a 1-week CGM home test can stratify individuals with different number of T1D-specific autoantibodies. Whereas standard metrics, such as MG, SD, and CV, were unable to stratify the different autoantibodies groups in the overall 7 days or overnight CGM traces, T180 based on the overall 7 days CGM traces distinguishes between the three autoantibodies groups, which was also the case for the CGM IAUC based on the overnight CGM traces, where IAUC was lower in the Ab– group versus Ab+.

Besides, the post-SLMM periods T180 was a statistically significant difference between the three autoantibodies groups, and Tmax approached significance. Therefore, the highest glucose excursions (T180) appear as a metric that differentiates between the three autoantibodies groups, likely driven by different meal responses. This is in line with what was observed previously for children with median age 11.5 years,20 with the caveat in our study, T140 was not as predictive as T180. In contrast, the ability of overnight IAUC to distinguish between the different groups suggests the ability of the participants with a lower number of autoantibodies to reach their baseline glucose values faster.

The data collected during the home CGM study and the glycemia metrics/features derived from it allowed the use of machine-learning methodology to develop an autoantibodies status classifier. Notably, features based on the complete 7-day CGM traces were unable to classify with sufficient accuracy the Ab+ versus Ab_ participants, but the overnight and post-SLMM CGM traces were able to better capture the differences between the groups. Glycemic features extracted from the overnight and post-SLMM CGM traces were able to distinguish the Ab+ versus Ab_ participants, and predict the autoantibodies status with only a small number of significant features such as T180 and IAUC from the overnight traces, and Tmax from the post-SLMM traces.

The proposed methodology of the autoantibody classifier, which combines the CGM home test data with a linear SVM-based classifier, was able to predict with high accuracy (i.e., AUC-ROC ≥0.81) the participant's presence or absence of autoantibodies. Overall, these results support the notion that adding the SLMM intervention to the home CGM test improves our ability to use the test to distinguish Ab+ versus Ab_ participants with a small number of features with different but complementary physiological meaning. We also note that proposed technology allows addressing not only the question of classifying Ab+ versus Ab–, but also exploring the option for classifying low-risk (zero and one autoantibody) versus high-risk (two and more autoantibodies; Stage 1 and 2).

As mentioned in the introduction, a recent study in individuals in T1D probands has identified several CGM-derived metrics of hyperglycemia significantly associated with rapid progression to Stage 3 disease, including in those with normal OGTT results.23 These metrics are based on selected percent time (5% or 8%) with glucose above different glucose level thresholds (e.g., glucose over 120, 140, and 160_mg/dL). Even though our technology is not tailored to stratify progressors to Stage 3 from nonprogressors, it identifies new metrics derived from the overnight and post-SLMM CGM periods that can be explored to estimate the imminent risk for progression to Stage 3 T1D.

The proposed CGM home test can be self-administered after a carefully designed interactive online teaching session and would not require a visit to a health care facility or use of a medical laboratory. Therefore, it could be used as an alternative or in addition to current home screening methods such as the GTT@ home (https://www.digostics.com) and self-collected capillary blood autoantibodies test currently employed by TrialNet.29 It can provide additional information on the level of dysglycemia that cannot be obtained by a single-finger stick for autoantibody presence or a genetic test.

Future studies will demonstrate whether it can also complement other T1D risk biomarkers (including genetic), to estimate the autoantibodies status better, and the overall risk of developing T1D, and/or separate progressors from nonprogressors in autoantibody-positive individuals. Ultimately, this could provide insight toward onset of therapy, potentially avoiding cases of DKA and highlighting individuals who could benefit from future immune-modulatory interventions such as teplizumab.30

This study benefited from prospectively collected data from T1D proband individuals with known autoantibody status involved in TrialNet studies. Its limitations include the relatively small number of participants, the fact that 7 out of 60 subjects had breakfast around the time of the SLMM and were excluded from the analysis (PostSLMM), the small number of CGM days available for CGM-based characterization of glycemia, and lack of more detailed information on the autoantibodies (type, confirmation, persistence, etc.). As such, we were not able to perform a meaningful comparison of Stage 1 versus Stage 2 participants in the ≥2 autoantibodies group to be consistent with the current understanding of the pathophysiology of T1D. Using an independent sample in a future study will be needed to confirm the performance of the tested machine-learning methods and the predictive power of the selected features.

In addition, the data used for developing the autoantibodies classifier originated from a limited population of volunteers that have relatives with T1D and are of age between 12 and 42 years. Finally, in this study we use data collected with the Dexcom G4 Platinum CGM, rather than the more advanced G6 model typically used in recent studies. We cannot assess objectively the implications of using an older CGM, but we do not have reasons to expect the outcomes to be sensor-specific. In contrast, newer sensors have many advantages, including improved usability and longer duration of use and are better candidates to be used to provide data for the proposed methodology in this study."
48,"Predicting Long-Term Type 2 Diabetes with Artificial Intelligence (AI): A Scoping Review Type 2 diabetes mellitus (T2DM) is a chronic metabolic disorder that affects a significant portion of the global population. Artificial intelligence (AI) has emerged as a promising tool for predicting T2DM risk. To provide an overview of the AI techniques used for long-term prediction of T2DM and evaluate their performance, we conducted a scoping review using PRISMA-ScR. Of the 40 papers included in this review, 23 studies used Machine Learning (ML) as the most common AI technique, with Deep Learning (DL) models used exclusively in four studies. Of the 13 studies that used both ML and DL, 8 studies employed ensemble learning models, and SVM and RF were the most used individual classifiers. Our findings highlight the importance of accuracy and recall as validation metrics, with accuracy being used in 31 studies, followed by recall in 29 studies. These discoveries emphasize the critical role of high predictive accuracy and sensitivity in detecting positive T2DM cases. Our scoping review revealed that the majority of studies (58%) utilized Machine Learning (ML) techniques, while 33% employed both ML and Deep Learning (DL) techniques, and only 10% exclusively explored the potential of DL models. However, the use of DL techniques for predicting the onset of T2DM is still relatively underexplored, as mentioned earlier, with only 10% of the reviewed studies employing DL models exclusively. Nonetheless, we discovered that out of the four studies that exclusively used DL models, an average accuracy of 81.57% is commendable, indicating that DL models need to be further investigated in this area of research.
In terms of the validation metrics used, our review found that accuracy was the most commonly used metric (77.5%), followed by recall (72%), area under the curve (57.5%), and specificity (55.0%). It is understandable that recall was the second most frequently used validation metric for T2DM predictive tools since these tools need to be evaluated in terms of their sensitivity to avoid misclassification of positive incidents. Additionally, it is crucial for predictive tools to have a high recall score, which can assist in early intervention and management of T2DM and its complications. We found that ensemble learning models had the highest average recall score (85.08%) among the 40 papers analyzed. Thus, we strongly believe that ML models that use ensemble learning as their classifiers, in addition to DL models, could lead to the development of a generalized predictive tool. Our review has some limitations, including the possibility of bias in the Pima dataset used in most studies, the complexity of DL models for physicians to interpret, the potential for missing vital information in studies published before 2019, and the potential for important research in non-English languages to be overlooked."
49,"Predicting Prediabetes Using Simple a Multi-Layer Perceptron Neural Network Model. In over 60% of patients with prediabetes, the evolution to diabetes can be stopped by changing lifestyle. Application of prediabetes criteria existing in accredited guidelines is very useful, representing an effective way to avoid prediabetes and diabetes. Although these guidelines imposed by the international diabetes federation are constantly updated, many doctors do not apply, mainly due to lack of time, the recommended steps for diagnosis and treatment. In this paper, a multi-layer perpeptron neural network model for prediabetes prediction is proposed, based on a dataset with 125 persons (men and women), with the following features: gender (S), serum glucose (G), serum triglycerides (TG), serum high-density lipoprotein cholesterol (HDL), waist circumference (WC) and systolic blood pressure (SBP). The output feature in the dataset (prediabetes or not) was based on a standardized medical criterion named Adult Treatment Panel III Guidelines (ATP III), which specifies that prediabetes diagnostic can be establish if at least three of five parameters are outside the scale of their normal values. Satisfactory results were obtained in evaluating the model. The sequential model having one, two and three hidden layers respectively, was built. Activation function was chosen ‘relu’ for all hidden layers and ‘sigmoid’ for the output layer.
The optimizer was choosen, in turn: Adam, Adadelta, Adagrad, Adamax, Nadam, RMSprop, FTRL and SGD. The loss function was chosen ‘binary_crossentropy’, which the most suitable for binary classification.
There were no significant differences in performance for the model with one, two or three hidden layers.
In Table 1 is shown accuracy for different values of optimizer and loss function, for model using scaled data. The model trained and tested on unscaled data has accuracy below the accuracy for model trained and tested on scaled data and those values are not represented in the table."
50,"Predicting Progression of Type 2 Diabetes Using Primary Care Data with the Help of Machine Learning. Type 2 diabetes – a prevalent chronic disease worldwide – increases the risk for serious health
consequences including heart and kidney disease. Forecasting diabetes progression can inform
disease management strategies, thereby potentially reducing the likelihood or severity of its
consequences. We use continuous glucose monitoring and actigraphy data from 54 individuals
with type 2 diabetes to predict their future hemoglobin A1c, HDL cholesterol, LDL cholesterol,
and triglyceride levels one year later. Using a combination of convolutional and recurrent neural
networks, we develop a deep neural network architecture that can learn the dynamic patterns in
different sensors’ data and combine those patterns with additional demographic and lab measurment data. To further demonstrate the generalizability of our model, we also evaluate its
performance using an independent public dataset of individuals with type 1 diabetes. In addition
to diabetes, our approach could be useful for other serious and chronic physical illness, where
dynamic (e.g., from multiple sensors) and static (e.g., demographic) data are used for creating
predictive models.  In this study, we presented a new architecture for predicting the progression of T2D using static (e.g., patient demographics) and
dynamic (e.g., wearable sensor) data. The results in Table 3 show that the performance for our model in terms of accuracy and AUROC
was improved by including both (static and dynamic) datasets. Specifically, for the classification test on the HbA1c, which is the most
informative sequalae in monitoring the T2D progression, the accuracy was improved by 3.41% in our model, compared to the Wide
and Deep approach. While evaluating the performance of our model using only the error values may show its relevance to the practical
cases less clearly, we can use some relevant measures in the field as a comparison. As an established threshold for the medical laboratories, the error rate of ±6% is considered as the maximum tolerable error in measuring HbA1c (Little, 2014). This is a threshold set
by the US Food and Drug Administration (FDA) agency for measuring the HbA1c in lab tests. In our model, we achieved an RMSE of
1.37%, meaning any predicted value of HbA1c in our model will have an average error rate of 1.37%. This is lower than the medically
acceptable error rate by the FDA and may demonstrate the applicability of our proposed method to actual T2D management programs.
Besides HbA1c, our proposed model also shows a good performance in estimating the future HDL and LDL cholesterol values, achieving
82% AUROC for both values in the classification task, and NRMSE of 0.13 and 0.08 respectively, in the regression task. We have also
compared the performance of our model with three baseline methods based on random forest, XGBoost, and Wide and Deep architecture. As Table 4 shows, the accuracy of the proposed model surpasses the baseline scenarios in predicting all four T2D sequelae.
The underlying structure of the Wide and Deep method and our model are very similar, while our model could consistently achieve
more competitive results. This shows that extracting the frequency-domain localizations at multiple levels could enhance the overall
performance of our model. T1D and T2D are two different diseases with unique properties of their own. While our study is primarily
focused on T2D, in lieu of a comparable public T2D dataset along with published predictive models on it, we have picked a T1D dataset
to evaluate our predictive model from a technical standpoint. We evaluated the performance of our model using the OhioT1DM dataset
for predicting blood glucose values in the next 60 min in a separate case study related to type 1 diabetes. This was the closest publicly
available dataset to our problem that we could find. In this experiment, with the absence of four primary targeted sequelae for prediction, the value of CGM itself was used as the predicted value. Among the existing studies, two of the best results were reported by
(Yin et al., 2019) (RMSE of 33.3) and (Martinsson et al., 2018) (RMSE of 33.2). We have achieved an RMSE of 34.2 on the same tasks,
using the model presented in this work. While our model design was not specifically tuned for the OhioT1DM data (unlike the two
existing studies mentioned earlier), obtaining similar performance to the state of the art in the field further demonstrates the potential
of our method for studying similar problems. We expect that the true potential of our method can be particularly revealed in scenarios
where more than one type of signal is available. Our method can effectively extract intra-sensor and inter-sensor dependencies.
According to our SHAP experiments shown in Fig. 5, the importance degree (feature ranking) of the temporal measurements was
found to be higher than physical attribute records consistently across the prediction tasks for the four sequelae. Among the temporal
measurements, CGM measurements were found to be the highest-ranking features, and the acceleration measurements had the next
highest ranks. As discussed above, we have observed that the performance of the model without considering the static features is lower
than a model utilizing static and dynamic (temporal) data together. For triglycerides, this difference is smaller than the other three.
This smaller difference might be due to the lower dependency of triglycerides to the available physiological features. Another possible
explanation for this could be the higher variance of the triglyceride values compared to other three sequalae, making the prediction
task essentially more challenging for all models. Based on the importance ranking of each time-series signal compared to the other
features, it can be observed that excluding each of the four wearable data sources would result in worse performance. The higher
importance of acceleration in the Z dimension, compared to X and Y dimensions may be related to the greater contribution of this
ActiGraphy measurement in indicating the physical activity intensity (Bai et al., 2016). The Z dimension corresponds to the movement
of the ActiGraphy device toward a perpendicular axis to its screen. Weight and waist circumference were found to be the two most
important factors among the physical attributes for predicting future values in our model.
One unique property of our study was demonstrating that our model can predict future values of four major T2D sequelae, using
data collected over a relatively short period. While we did not explicitly extract specific activity patterns (such as sleeping and
running), we expect that our model was able to implicitly use those patterns for its predictions, as actigraphy patterns generally are
considered as a reliable source of activity recognition. Our multi-level architecture enabled our model to identify the underlying
patterns from various sources of data.
Potential limitations of our study include the age range of the participants as well as the sample size of patients. Because of the focus
of the original study was on adults, we did not have access to the data from younger individuals. We note that as the prevalence of T2D
increases with the age, our model can still be useful on a large variety of T2D patients. It should be also noted that HbA1c levels alone
are generally not enough for the diagnosis of T2D (American Diabetes Association, 2021). Accordingly, a limitation of our study is the
lack of blood glucose and serum insulin measurements. Including such additional physiological and biochemical data would further
improve our work. While our study has mainly focused on diabetes, the method we described may be useful for studying a variety of
similar health problems and predicting future health outcomes. The scope of such health problems is very large, as the presence of both
static and dynamic datasets is ubiquitous in many health problems."
51,"Predicting the Onset of Diabetes with Machine Learning Methods The number of people suffering from diabetes in Taiwan has continued to rise in recent years. According to the statistics of the International Diabetes Federation, about 537 million people worldwide (10.5% of the global population) suffer from diabetes, and it is estimated that 643 million people will develop the condition (11.3% of the total population) by 2030. If this trend continues, the number will jump to 783 million (12.2%) by 2045. At present, the number of people with diabetes in Taiwan has reached 2.18 million, with an average of one in ten people suffering from the disease. In addition, according to the Bureau of National Health Insurance in Taiwan, the prevalence rate of diabetes among adults in Taiwan has reached 5% and is increasing each year. Diabetes can cause acute and chronic complications that can be fatal. Meanwhile, chronic complications can result in a variety of disabilities or organ decline. If holistic treatments and preventions are not provided to diabetic patients, it will lead to the consumption of more medical resources and a rapid decline in the quality of life of society as a whole. In this study, based on the outpatient examination data of a Taipei Municipal medical center, 15,000 women aged between 20 and 80 were selected as the subjects. These women were patients who had gone to the medical center during 2018-2020 and 2021-2022 with or without the diagnosis of diabetes. This study investigated eight different characteristics of the subjects, including the number of pregnancies, plasma glucose level, diastolic blood pressure, sebum thickness, insulin level, body mass index, diabetes pedigree function, and age. After sorting out the complete data of the patients, this study used Microsoft Machine Learning Studio to train the models of various kinds of neural networks, and the prediction results were used to compare the predictive ability of the various parameters for diabetes. Finally, this study found that after comparing the models using two-class logistic regression as well as the two-class neural network, two-class decision jungle, or two-class boosted decision tree for prediction, the best model was the two-class boosted decision tree, as its area under the curve could reach a score of 0.991, which was better than other models.
 Through the above data input and feature classification, this study showed that the subjects were prone to developing diabetes (especially during pregnancy) due to low insulin absorption, high cholesterol levels, or elevated blood pressure [35,36]. After model training, storing of the result models, and model testing were completed, cross-validation and comparison were carried out. The verification results of the metrics used for evaluation of the model, including the true positive, false positive, false negative, true negative, accuracy, precision, recall, F1 score, and AUC values, were obtained in this study, as shown in Figure 9. A summary of the verification results is shown in Table 1.
 Diabetes is one of the most serious chronic diseases today, and early diagnosis can greatly improve patients’ chances of managing it. The latest developments in machine intelligence can be used to improve the understanding of the factors that lead to the onset of diabetes. This study used eight different characteristics (number of pregnancies, plasma glucose level, diastolic blood pressure, sebum thickness, insulin level, BMI, diabetes pedigree function, and age) for data preprocessing. After training, testing, cross-validation, and comparison, this study obtained the data for the model performance analysis.

The results showed that all models achieved good results; however, the best models were the two-class decision jungle and two-class boosted decision tree. The area under the curve (AUC) was selected as the performance indicator and AUC scores of 0.976 and 0.991 were achieved, which was better than expected based on the literature Hasan et al [27].

These results provided an improvement to the existing prediction methods for diabetes. It is worthwhile to explore these models using unsupervised machine learning and deep learning techniques in future research [37]."
52,"Predicting the onset of diabetes-related complications after a diabetes diagnosis with machine learning algorithms. Aims
Using machine learning algorithms and administrative data, we aimed to predict the risk of being diagnosed with several diabetes-related complications after one-, two- and three-year post-diabetes diagnosis.

Methods
We used longitudinal data from administrative registers of 610,019 individuals in Catalonia with a diagnosis of diabetes and checked the presence of several complications after diabetes onset from 2013 to 2017: hypertension, renal failure, myocardial infarction, cardiovascular disease, retinopathy, congestive heart failure, cerebrovascular disease, peripheral vascular disease and stroke. Four different machine learning (ML) algorithms (logistic regression (LR), Decision tree (DT), Random Forest (RF), and Extreme Gradient Boosting (XGB)) will be used to assess their prediction performance and to evaluate the prediction accuracy of complications changes over the period considered.

Results
610,019 people with diabetes were included. After three years since diabetes diagnosis, the area under the curve values ranged from 60% (retinopathy) to 69% (congestive heart failure), whereas accuracy rates varied between 60% (retinopathy) to 75% (hypertension). RF was the most relevant technique for hypertension, myocardial and retinopathy, and LR for the rest of the comorbidities. The Shapley additive explanations values showed that age was associated with an elevated risk for all diabetes-related complications except retinopathy. Gender, other comorbidities, co-payment levels and age were the most relevant factors for comorbidity diagnosis prediction.

Conclusions
Our ML models allow for the identification of individuals newly diagnosed with diabetes who are at increased risk of developing diabetes-related complications. The prediction performance varied across complications but within acceptable ranges as prediction tools. 4. Conclusions
Our study used administrative data to detect the most often diabetes-related complications along different time spans after a diabetes diagnosis. The results showed that RF outperformed the other machine learning algorithms for hypertension, myocardial and retinopathy, whereas, for the rest of the comorbidities, the best model was the LR. The four machine learning methods showed high predicting power, with AUC values ranging from 60 % to a maximum of 69 %, depending on the diabetes-related complication considered.

To our knowledge, no prediction models for micro- and macrovascular complications exist for individuals considering different time spans after a diabetes diagnosis; hence, a comparison with prior work is impossible. Still, some authors have already used prediction models for individuals with diabetes. The most similar work was that performed by Schallmoser et al. (2023) [25]. They used machine learning models to predict the risk of developing three micro- (retinopathy, nephropathy and neuropathy) and three macro-vascular (peripheral vascular disease, cerebrovascular and cardiovascular diseases) complications after five years since a diagnosis of diabetes or pre-diabetes. Although the forecasted time differed (they only considered the period of five years, and we considered one, two and three years since diabetes diagnosis), our prediction models for cardiovascular diseases (myocardial infarction and congestive heart failure) would potentially outperform their models at 5-years since figures are already similar at our three-year timespan to theirs (0.69). Their models did better than ours for retinopathy. Still, the results are not fully comparable given the different time frames and the fact that they grouped diabetes-related macro-vascular complications, limiting the comparison at disaggregated levels.

Similarly, Ljubic et al. (2020) [26] modelled the prediction risk for ten diabetes-related complications over nine years. In addition to considering a longer time span, comparability is restricted since performance measures are based on a potentially sicker group of people with diabetes (four visits to hospitals between diabetes diagnosis and diabetes-related complications) who are at higher risk of developing any health problem. Moreover, machine learning algorithms differ (Ljubic et al. report only the performance metrics for recurrent neural networks). Dagliati et al. (2018) [27] focused only on microvascular complications after the first visit to the hospital, not since diabetes diagnosis as we do, which outperformed our results when balancing the algorithms, but not in the raw ones. Moreover, our results confirm the role that some individual characteristics might pose on the onset of some comorbidities, such as the role of age [28], [29] with respect to the development of macrovascular complications.

Overall, the prediction performance of our models for individuals with diabetes is somehow comparable to the performances reported in prior work, with differences between studies explaining differences and providing new evidence in the existing literature. As recent systematic review literature showed (Gosak et al., 2022) [30], work on applying ML algorithms to predict the risk of diabetes-related complications is very scarce. Hence, one of the advantages of our study is that we are the first to account for different time spans after a diabetes diagnosis, which allows us to set short- and medium-term prediction risk models. Furthermore, the availability of administrative data, which refers to a representative population, allows for a large set of individual information (sociodemographics, biomarkers, comorbidities and other items included in the clinical history). The information provided here could be especially relevant for the clinical setting since it allows for early identification of individuals at risk who could benefit from prompt treatment, increasing the odds of preventing or delaying diabetes-related complications onset.

This is the first paper that applies different machine learning methods to study their prediction performance of diabetes-related complications after a diabetes diagnosis and whether it persists over different short-term periods. Moreover, our analysis is enriched using a large administrative dataset, allowing us to split the sample into nine diabetes comorbidities after a diabetes diagnosis. However, some limitations should be mentioned. The main limitations of our study are related to the availability of data, mainly due to the lack of data on more extended follow-up periods to check whether the pattern observed within our timeframe remains in the long term. Likewise, the lack of a longer follow-up period in case of a possible change in the characteristics of the diabetic population may lead to a dataset shift bias for the models’ applicability. Second, we did not include information on blood glucose measurements since this information was not available for all individuals and could contain measurement errors. For this purpose, we ran sensitivity analyses for the three-year period and the most important complications, and there was no impact on previous results. Third, our analysis is restricted to public healthcare use. However, it is common practice among private hospital patients to pick up prescriptions from primary public centres. This is probably true for moderate to severe patients; we may still lose the milder ones that might be using only private resources.

Overall, AI has the potential to improve diabetes care and help patients manage their condition more effectively to prevent or delay diabetes progression into the development of diabetes-related complications. However, it's important to note that AI is not a substitute for medical advice or treatment, and patients should always consult with their healthcare providers for any medical concerns."
53,"Prediction of diabetes disease using an ensemble of machine learning multi-classifier models. Background and objective
Diabetes is a life-threatening chronic disease with a growing global prevalence, necessitating early diagnosis and treatment to prevent severe complications. Machine learning has emerged as a promising approach for diabetes diagnosis, but challenges such as limited labeled data, frequent missing values, and dataset imbalance hinder the development of accurate prediction models. Therefore, a novel framework is required to address these challenges and improve performance.

Methods
In this study, we propose an innovative pipeline-based multi-classification framework to predict diabetes in three classes: diabetic, non-diabetic, and prediabetes, using the imbalanced Iraqi Patient Dataset of Diabetes. Our framework incorporates various pre-processing techniques, including duplicate sample removal, attribute conversion, missing value imputation, data normalization and standardization, feature selection, and k-fold cross-validation. Furthermore, we implement multiple machine learning models, such as k-NN, SVM, DT, RF, AdaBoost, and GNB, and introduce a weighted ensemble approach based on the Area Under the Receiver Operating Characteristic Curve (AUC) to address dataset imbalance. Performance optimization is achieved through grid search and Bayesian optimization for hyper-parameter tuning.

Results
Our proposed model outperforms other machine learning models, including k-NN, SVM, DT, RF, AdaBoost, and GNB, in predicting diabetes. The model achieves high average accuracy, precision, recall, F1-score, and AUC values of 0.9887, 0.9861, 0.9792, 0.9851, and 0.999, respectively.

Conclusion
Our pipeline-based multi-classification framework demonstrates promising results in accurately predicting diabetes using an imbalanced dataset of Iraqi diabetic patients. The proposed framework addresses the challenges associated with limited labeled data, missing values, and dataset imbalance, leading to improved prediction performance. This study highlights the potential of machine learning techniques in diabetes diagnosis and management, and the proposed framework can serve as a valuable tool for accurate prediction and improved patient care. Further research can build upon our work to refine and optimize the framework and explore its applicability in diverse datasets and populations. Diabetes is a chronic condition that significantly impacts individuals' quality of life, underscoring the critical need for accurate prediction methods in its management and prevention. In our study, we delve into the analysis and interpretation of results obtained from our ensemble machine learning models, which were designed to predict diabetes using the IPDD dataset. We also explore the implications of our findings, discuss the limitations of our study, and provide recommendations for future research.

The primary contribution of this research lies in introducing a introduces, pipeline-based framework of multi-class machine learning models for diabetes prediction. The framework utilizes the IPDD dataset, which encompasses three distinct groups: diabetic subjects (Y), non-diabetic subjects (N), and predicted diabetic subjects (P). The innovative nature of this framework lies in its ability to effectively classify individuals into these categories, thereby enhancing our understanding of diabetes prediction. This approach addresses the multi-class classification problem and ensures a comprehensive evaluation of performance by employing various evaluation metrics to assess the effectiveness of our proposed models. Data pre-processing plays a vital role in enhancing the accuracy and efficiency of predictive models. In our proposed model, we utilized several pre-processing techniques, such as filling missing values, standardization, normalization, feature selection, and dimensionality reduction. These techniques were implemented to meticulously prepare the data, improve model performance, and mitigate the impact of incomplete or inconsistent data. The results of our study emphasize the significance of data pre-processing in achieving accurate predictions for diabetes. By leveraging the collective intelligence of multiple individual classifiers, our ensemble approach demonstrates its effectiveness through improved overall performance and accuracy in diabetes prediction. This approach addresses biases and errors inherent in individual classifiers, which is particularly important given the challenges posed by imbalanced data and missing attribute values in diabetes prediction. Our experiments consistently showed that the random forest model, in combination with the MRMR and I_+_N stages of data pre-processing, outperformed other models. This highlights the importance of feature selection and dimensionality reduction techniques in enhancing diabetes prediction accuracy. The utilization of MRMR feature selection and PCA/ICA dimensionality reduction methods enables the identification of key features that significantly impact class determination. Furthermore, combining K-NN, AB, DT, and RF models with 11 features and I pre-processing exhibited superior performance in predicting diabetes in the IPDD dataset. This underscores the significance of employing a diverse set of machine learning models in an ensemble approach to enhance prediction accuracy. By harnessing the strengths of these models, we achieve more robust and reliable predictions. It is important to note that the evaluation of our models was not solely based on accuracy due to the imbalanced nature of the dataset. Instead, we employed multiple evaluation measures, including the Area Under the ROC Curve (AUC), to provide a comprehensive assessment of model performance. AUC is particularly suitable for imbalanced datasets as it considers the trade-off between true positive rate and false positive rate, offering a more accurate representation of the model's predictive power. Despite yielding promising results, our study has certain limitations. Firstly, the IPDD dataset used in our research may possess inherent biases and limitations that could affect the generalizability of our findings to other populations. Future studies should consider incorporating datasets from diverse patient populations to validate the effectiveness of our proposed models. Secondly, while we employed various data pre-processing techniques, there may be alternative approaches that could further optimize the performance of our models. Exploring alternative pre-processing techniques and comparing their efficacy could be a valuable avenue for future research. Ensemble models have their limitations, including increased model complexity, longer training and testing times, and the requirement of comprehensive data for model construction and configuration. Additionally, the interpretation of results from these models can be challenging due to their complexity across different datasets, potentially leading to inconclusive outcomes. Therefore, prior to utilizing these models, a meticulous examination and in-depth analysis of their features, data size, and other aspects are imperative.

In conclusion, our research demonstrates the potential of ensemble machine learning models, along with comprehensive data pre-processing techniques, in accurately predicting diabetes using the IPDD dataset. The results highlight the importance of feature selection and dimensionality reduction in improving prediction accuracy. Our proposed models offer a promising approach to diabetes prediction by addressing challenges posed by imbalanced data and missing attribute values. The findings of this study contribute to the field of diabetes diagnosis and treatment, providing valuable insights for researchers and practitioners. Our future research will focus on validating our models with larger and more diverse datasets, investigating additional preprocessing techniques to enhance the performance of diabetes prediction models, as well as exploring novel methods for early detection of COVID-19 disease and applications in mobile computing and manufacturing for comprehensive early disease diagnosis.
"
54,"Prediction of Diabetes Mellitus Progression Using Supervised Machine Learning. Diabetic peripheral neuropathy (DN) is a serious complication of diabetes mellitus (DM) that can lead to foot ulceration and eventual amputation if not treated properly. Therefore, detecting DN early is important. This study presents an approach for diagnosing various stages of the progression of DM in lower extremities using machine learning to classify individuals with prediabetes (PD; n = 19), diabetes without (D; n = 62), and diabetes with peripheral neuropathy (DN; n = 29) based on dynamic pressure distribution collected using pressure-measuring insoles. Dynamic plantar pressure measurements were recorded bilaterally (60 Hz) for several steps during the support phase of walking while participants walked at self-selected speeds over a straight path. Pressure data were grouped and divided into three plantar regions: rearfoot, midfoot, and forefoot. For each region, peak plantar pressure, peak pressure gradient, and pressure–time integral were calculated. A variety of supervised machine learning algorithms were used to assess the performance of models trained using different combinations of pressure and non-pressure features to predict diagnoses. The effects of choosing various subsets of these features on the model’s accuracy were also considered. The best performing models produced accuracies between 94–100%, showing the proposed approach can be used to augment current diagnostic methods. The purpose of this study was to determine the feasibility of using supervised machine learning algorithms to accurately classify persons who are diagnosed with pre-diabetes, diabetes, or diabetes with peripheral neuropathy. The classification accuracies, as well as the false negative rates for DN in the test dataset, were used to determine which algorithms and features subsets performed best. The false negative rates for DN participants are especially important because of the difficulty in diagnosing DN using current techniques.
Table 5 indicates that all datasets yielded reasonably accurate classifications. However, since false negative rates are important for this type of analysis, it is reasonable to consider dataset 1 and datasets 11 through 14; all had zero percent false negative rates. Of these five datasets, datasets 13 and 14 had the highest F1 score, of 100%. The results may indicate that a subset of the tested features can be more successful in classification than using all available features. A recent study that used biomechanical data to train and test machine learning algorithms for DN diagnosis also found that using a subset of features, rather than the entire dataset, yielded greater accuracies [32]. Thus, the identification and inclusion of significant features, rather than all available features, could produce more effective and less computationally costly algorithms.
The only pressure features in datasets 13 and 14 were for PTI. This indicates that PTI may play a significant role in DN classification. A previous study indicated that rearfoot values of peak pressure are significant in classification [22]. PPP has also been reported to be a significant indicator for DN [17,22]. This was corroborated in this study by the high performance of datasets 9 and 10, which relied solely on PPP pressure features, and datasets 5 and 6, which used PPP and PTI features. For these specific datasets, ensemble bagged trees and ensemble subspace KNN classifiers produced the best classifications of the data. It is also of note that subspace KNN was the second-best performing algorithm for five of the seven datasets where it was not the best performing, excepting only datasets 3 and 7. Table 5 also indicates that the exclusion of HbA1c did not yield any significant difference in results.
Testing only the non-pressure features, with and without HbA1c, resulted in 100% in all three measures. This may be because during data collection, while the pressure values would change for each trial, the non-pressure features for each participant would remain the same, since the measures were not variable during a single data collection session. As such, the current dataset did not produce any significant trends or results when looking only at the non-pressure features. No trends in the non-pressure features when plotted across the three classes were found, either. Since the non-pressure features did not have any discernable pattern in delineating the three classes, it could not be concluded that non-pressure features are all that is needed to classify individuals with PD, D, or DN, and further exploration into the impact of these features on classification was unfruitful. The non-pressure features alone were thus concluded to not have the ability to accurately classify the data. Analyzing solely the pressure features allowed the features that played a more significant role in accurate classification to become clearer.
The results of analyzing only the pressure features (Table 6) indicate that datasets 1, 5, 7, and 13 had test accuracies of greater than 80% and false negative rates of less than 3%. It should be noted that dataset 1 included all pressure features; dataset 5 had PPP and PTI features; dataset 7 combined PPG and PTI features; and dataset 13 only contained PTI features. The recurrence of PTI indicated its significance in classifying the progression of diabetes. When considering only pressure features, the results indicated that including more features does produce a higher test accuracy, but only when PTI features are included. For example, dataset 3, which included PPP and PPG features, achieved a test accuracy of 65% and false negative rate of 39%. Similarly, dataset 9 included only PPP features, and dataset 11 only included PPG features. All of these datasets produced test accuracies of less than 70% and false negative rates of greater than 50%. Overall, the inclusion of PTI, even as the sole pressure feature, greatly enhanced all three measures. The results from Table 6 indicate that a variety of pressure features used alone can still provide adequately accurate classifications, as datasets 1 and 5 had precision, recall, and F1 scores of higher than 90%. By comparing the results from Table 5 and Table 6, however, it can be seen that if using only a limited number of pressure features, having non-pressure features will greatly enhance the feasibility of this method, exemplified best by the drastic performance difference in dataset 11.
A study that used center of pressure data to train and test a deep clustering model found that individuals with DN tend to apply less force while walking and have longer stance times during gait [33]. As such, individuals who have DN might have values for PPG or PPP similar to those of the other classes, but their PTI values are more distinct, since the feature combines pressure, foot size (due to the increased number of sensors for larger insoles), and time. Additionally, since PTI represents the combined effect of these three factors, it can avoid the variability due to gait variation better than PPP or PPG. Given the high presence of PTI in the best performing algorithms, further study into the relationship between foot size, walking speed, and PTI is desirable.
Table 7 shows that PCA achieved comparable results for the best datasets of Table 5: datasets 1 and 11 through 14. The results that were achieved using ensemble classifier with subspace KNN model produced the highest results in conjunction with PCA. Apart from datasets 8, 9, and 12–14, PCA also led to higher false negative rates. While dimensionality reduction was explored to see if the model performance could be enhanced, the results indicate that PCA was unsuccessful in this.
While ensemble classifier with bagged trees was a high performing algorithm throughout the results, various forms of KNN also performed well for half of the datasets (Table 5). KNN performing well has been documented before in a previous study that also analyzed multiple algorithms in classifying DN using EMG and gait data [34]."
55,"Prediction of progression from pre-diabetes to diabetes: Development and validation of a machine learning model Aims: Identification, a priori, of those at high risk of progression from pre-diabetes to diabetes may enable targeted delivery of interventional programmes while avoiding the burden of prevention and treatment in those at low risk. We studied whether the use of a machine-learning model can improve the prediction of incident diabetes utilizing patient data from electronic medical records.

Methods: A machine-learning model predicting the progression from pre-diabetes to diabetes was developed using a gradient boosted trees model. The model was trained on data from The Health Improvement Network (THIN) database cohort, internally validated on THIN data not used for training, and externally validated on the Canadian AppleTree and the Israeli Maccabi Health Services (MHS) data sets. The model's predictive ability was compared with that of a logistic-regression model within each data set.

Results: A cohort of 852 454 individuals with pre-diabetes (glucose ≥ 100 mg/dL and/or HbA1c ≥ 5.7) was used for model training including 4.9 million time points using 900 features. The full model was eventually implemented using 69 variables, generated from 11 basic signals. The machine-learning model demonstrated superiority over the logistic-regression model, which was maintained at all sensitivity levels - comparing AUC [95% CI] between the models; in the THIN data set (0.865 [0.860,0.869] vs 0.778 [0.773,0.784] P < .05), the AppleTree data set (0.907 [0.896, 0.919] vs 0.880 [0.867, 0.894] P < .05) and the MHS data set (0.925 [0.923, 0.927] vs 0.876 [0.872, 0.879] P < .05).

Conclusions: Machine-learning models preserve their performance across populations in diabetes prediction, and can be integrated into large clinical systems, leading to judicious selection of persons for interventional programmes. We hereby describe the training of a machine learning model in the prediction of progression from pre-diabetes to diabetes in a large population. This model is unique in its nonlinearity, versatility, and ability to predict diabetes continuously. The predictive ability of the model was superior to alternative methods of risk stratification of pre-diabetes individuals such as the use of logistic regression or clinically acceptable cut-offs. Furthermore, it can be used to predict progression to diabetes over short or long time-periods, while compromising its predictive ability in longer time-frame predictions, yet maintaining superiority over other predictive methods.

Several aspects of the model merit further discussion. The model is versatile and enables the user to choose the sensitivity most suitable to the particular clinical condition or intended intervention, considering its cost, risk-benefit ratio and the available budget. Thus, for example, we may consider its use in two extreme clinical situations.

1. Delivery of a costly intervention – for example, an intensive lifestyle programme. For this purpose, the health administration may wish to identify a small population (low PR) with a high PPV – aiming to treat only those who are at extremely high risk. Use of this model compared to simple logistic regression will allocate a smaller number of persons to the intervention.

2. Delivery of a low-cost/low risk intervention – for example, the prescription of metformin. Choosing a higher sensitivity threshold may better address this goal, and with the use of the model the number needed to treat will be smaller. In this scenario, the majority of the people receiving treatment would probably not have developed diabetes during the subsequent year, yet the treatment generally carries a low risk and is justified.

Unsurprisingly, glucose and HbA1c, including all their derived parameters, were the strongest contributors to the model. Use of statins had a modest contribution to the model supporting their minor role in progression to diabetes. Of note, even when compromising on a low sensitivity the positive predictive value of the model was only ~50% highlighting the inherent biological difficulty of predicting diabetes. Progression to diabetes is dependent upon multiple factors, many of which cannot be captured in any medical database including personal decisions for lifestyle modification, environmental circumstances and supporting ecosystem.

Interestingly, in the majority of the THIN cohort an HbA1c level was not available, a situation reflective of an overall low-risk population, in which HbA1c is justifiably not used as a screening test. Our model may assist in highlighting those high-risk individuals in whom an HbA1c test, or an OGTT may be considered, which may diagnose pre-existing diabetes. Additionally, the model calculates the risk continuously, thus at any point in time it is possible to assess the individual risk of each person irrespective of whether laboratory measurements were taken at the time. Consequently, the model may be used to dictate individualized testing intervals rather than the global annual testing proposed by the ADA.26

Previous studies have assessed the clinical utility of various prediction models for assessment of the risk of developing diabetes. Abbasi et al. reviewed 25 prediction models and noted that most can identify people at high risk of developing diabetes within 5 to 10_years, yet the diabetes risk was generally overestimated.16 Additionally, models which included biomarkers performed slightly better than the classic models.16 Risk assessment based on biomarkers has been shown to be superior to a risk score based on non-laboratory parameters in a Korean population,17 although the combination of the Finish Diabetes Risk Score and HbA1c improved the sensitivity and specificity of detection of diabetes and pre-diabetes in an American population.18

Multiple biomarkers such as CRP, ALT and GGT have been used to identify the risk of progression to diabetes. A recent systematic review which assessed the ability of over 150 biomarkers to predict incident diabetes concluded that these failed to improve prediction over a model consisting of the traditional diabetes predictors, glucose and HbA1c.19

The above models are limited in their reliance upon measurements in a single point in time, generally the last available. However, the progression of pre-diabetes to diabetes follows a non-linear trajectory and often persons initially oscillate around the threshold until they present with overt diabetes.8 Recent adoption of electronic medical records (EMR) provides us with the opportunity to use data spanning over a decade or more for more accurate prediction. As shown in our sensitivity analysis, use of data spanning over 3 to 5_years is also of benefit, however the predictive ability of the model while using 1-year historical data is significantly limited. Whereas the ease and scalability of the predictions and possible integration within the EMR are clear advantages of this approach, the use of EHR is currently, for the most, limited to numerical or coded data. Lifestyle related information, family history, socioeconomics, smoking status and other patient related information are often not coded, or the coding is unreliable, and are therefore not included in the EMR based models. Future advances in natural language processing approaches may overcome this barrier to some extent.

The use of machine learning models, as opposed to linear models, carries the capability of capturing subtle multivariate relationships which may be otherwise difficult to detect. Additionally, machine learning methods have the capability of dealing with large numbers of variables whiles producing powerful predictive models.27 Employment of these methods in the predictions of diabetes has been described in several studies, yet these were based on small and select populations (less than 10_000 individuals) limiting the generalizability of the results.27

The strength of our study lies in the inclusion of >1 million individuals with pre-diabetes and the use of modern machine learning techniques. Moreover, the continuity, versatility and flexibility of our model enable its employment in multiple clinical set-ups. However, several limitations of our study should be noted. Similar to other studies based upon EMR, our study does not take into account parameters such as exercise schedule, family history of diabetes or adherence to any dietary regimen. Additionally, waist circumference which is an important predictor of diabetes beyond BMI is not reliably available in the data. Furthermore, the definition of the pre-diabetes and diabetes registries did not include OGTT, which is a more sensitive criterion, yet was not available in our databases, since it is not commonly performed in standard clinical care. Additionally, we did not include comorbidities, diagnoses, procedures, and READ codes in the features of our model. We restricted our model to the use of features, such as lab data and drugs code groups, which are more easily transferred between different data sources, allowing the option for simpler future external validation on different data sources. Nevertheless, the inclusion of these data may have improved our results.

In conclusion, we describe the employment of a machine learning model to predict the progression to diabetes in over 1 million persons with pre-diabetes during an average 5_years of follow up. The model may be incorporated in the EMRs and alert for screening intervals or enable selection of high-risk individuals for interventional programmes, demonstrating a better PPV than current alternatives."
56,"Prediction of type 1 diabetes with machine learning algorithms based on FTIR spectral data in peripheral blood mononuclear cells. The incidence of autoimmunity is increasing, to ensure timely and comprehensive treatment, there must be a diagnostic method or markers that would be available to the general public. Fourier-transform infrared spectroscopy (FTIR) is a relatively inexpensive and accurate method for determining metabolic fingerprint. The metabolism, molecular composition and function of blood cells vary according to individual physiological and pathological conditions. Thus, by obtaining autoimmune disease-specific metabolic fingerprint markers in peripheral blood mononuclear cells (PBMC) and subsequently using machine learning algorithms, it might be possible to create a tool that will allow the diagnosis of autoimmune diseases. In this preliminary study, it was found that the peak shift at 1545 cm_1 could be considered specific for autoimmune disease type 1 diabetes (T1D), while the shifts at 1070 and 1417 cm_1 could be more attributed to the autoimmune condition per se. The prediction of T1D, despite the small number of participants in the study, showed an inverse AUC = 0.33 ± 0.096, n = 15, indicating a stable trend in the prediction of T1D based on FTIR metabolic fingerprint data in the PBMC.
 Autoimmune diseases are a condition where the body's immune system is unable to distinguish its own cells from foreign cells, thus mistakenly attacking the body's normal cells. However, there are more than 80 different autoimmune diseases, among which T1D, psoriasis, rheumatoid arthritis, thyroid disease, Lupus erythematosus are more common.9 These diseases have a very different clinic and the changes in blood composition are also nonidentical, but on the other hand, several autoimmune diseases are common in one patient at the same time.9 It is interesting that based on the FTIR data with ML methods it has been possible to distinguish multiple sclerosis patients from Neuromyelitis optica spectrum disorder and healthy individuals with 100% accuracy,57 to diagnose pathological thyroid function,58 it has been possible to distinguish healthy individuals from T1D patients.59 Differences are sought, but the unifying factor is not found.
The development of a new diagnostic method is extremely important firstly for the diagnosis of autoimmune diseases, to ensure simplified screening in populations with frequent certain autoimmune diseases, e.g., in Finland the incidence of T1D is 52.2 per 100[thin space (1/6-em)]000 children,57 in Sweden – 39.6 per 100[thin space (1/6-em)]000,60 in Norway – 33.6 per 100[thin space (1/6-em)]000,60 while the world average – 15 per 100[thin space (1/6-em)]000,61 in turn in Latvia T1D incidence is lower than the world average – 13.5 per 100[thin space (1/6-em)]000.62 Secondly a relatively low incidence does not reduce the importance of the method, it would be important in differential diagnostics in cases when it is necessary to find out/clarify whether an autoimmune condition is responsible for unclear symptoms.

The data obtained by PCA in our work (Fig. 2C) did not show a conclusive result that could indicate a specific group of spectra with which autoimmunity could be identified. We can see that both the control group and the samples from individuals with autoimmunity are evenly distributed among the vectors without forming any specific clusters. This similarity likely indicates that many study participants with autoimmunity are in remission and that the control person may also have an undiagnosed autoimmune disease. Only a few samples taken from patients with autoimmunity show a difference, however, this difference is not uniform, which could indicate that these individuals represent different autoimmune diseases. It was not possible to achieve autoimmunity detection by FTIR data, based on different machine learning algorithms. The samples from the T1D and self-reported autoimmunity groups were combined and analysed as a single set of persons with autoimmunity against data from healthy individuals, thought to predict autoimmunity per se, the spread of AUC across 15 replicates was too wide (from 0.208 to 0.649), resulting in 0.45 ± 0.117305584. We believe that to identify clusters of spectra that could indicate autoimmunity by PCA and to provide prediction of autoimmunity by ML algorithms, first, much more individuals should be included in the study. Second, it is necessary to ensure that patients with various autoimmune diseases and with a well-characterized clinic were included in the study. Thirdly, it is necessary to anticipate that the control group may include individuals with undiagnosed autoimmune diseases, so the control group should undergo a medical examination to rule out the presence of undiagnosed autoimmune diseases, at least the most common ones. Of course, we must not forget that patients with autoimmunity may have several autoimmune diseases,9 which complicates data analysis.

On the other hand, when our study narrowed the analysis of FTIR data with ML algorithms to the range of one disease, it was possible to achieve a stable inverse prediction of T1D, AUC = 0.33 ± 0.095879885 n = 15, where AUC values ranged from 0.164 to 0.484 (Table 5), which shows that increasing number of samples, a good prediction result could be obtained. Which is also confirmed by the result, when people with a self-reported autoimmune disease were excluded from the control group, which reduced the number of samples in the group, the dispersion of AUC became very wide from 0.722 to 0.325, which gave a mean value of 0.47 ± 0.116743104 n = 15. On the other hand, PCA analysis did not provide a clear cluster of spectra indicating T1D (Fig. 2A). When analyzing T1D FTIR data against individuals without impaired glucose tolerance, several samples from analyzed groups overlap, which could be explained by 1st that T1D controls glucose levels very well; 2nd someone from the control group has the same additional autoimmune disease as the T1D patient; 3rd some other mutual important factor that significantly affects the metabolic fingerprint in PBMC (Fig. 2A). However, excluding individuals with autoimmune diseases from the control group, the picture did not become clearer, it was not possible to identify a cluster of T1D-specific spectra (Fig. 2B). Due to various reasons, including genetics and environmental factors, the control of glucose levels in T1D patients, which affects the clinic of the disease, including the development of diabetic complications, is very different.63 Therefore, the fluctuations of the metabolic fingerprint in PBMC are expected to be wide, individual for each case. Nevertheless, the diagnosis of type 2 diabetes by Guang and co-authors, which was based on the recording of FTIR spectra in whole blood followed by ML analysis, gave promising results, however, the authors pointed out that the purity of the whole blood samples was low, and the spectra contained a lot of redundant information.53 Therefore, we believe that our method provides a cleaner and more accurate result. Also, like the autoimmunity data in our study, there is a need to increase the number of patients and provide a complete description of the clinical picture of patients with T1D and concomitant autoimmune diseases, and use algorithms to provide a more personalized approach.

Although PCA analysis failed to provide conclusive results and the study included a limited number of participants, it was possible to find some differences in FTIR spectra between different groups using various analytic statistical methods, the results are summarized (Table 3) and displayed visually (Fig. 1 and 2).

A cross-comparison of FTIR data sets of T1D patient group and individuals without impaired glucose tolerance (Table 3) showed a trend of peak shift of a band at 2956 cm_1. In spectra of multicomponent bio-samples the main absorption bands are a superposition of all individual macromolecular component spectra. The absorption band with a maximum at 2956 cm_1, is assigned to the oscillations of –CH3 bonds in lipids, amino acid side chains and less intensive absorption of carbohydrates and nucleic acids.40,64,65 Therefore the shift of this absorption band shows changes in structure or amount of the abovementioned molecules. In Raman spectra the band at 2955 cm_1 is assigned to the in-phase mode of C–H stretching of methine CH groups associated with hydroxyl groups and thus the levels of 2-deoxy-D-ribose.65 On the other hand, 2-deoxy-D-ribose is known to have highly reduced properties, and this molecule increases the intracellular levels of ROS and stimulates the formation of advanced glycation end products.66 ROS levels are elevated in PBMCs from T1D patients,67 therefore, large molecules of cells are damaged, including phospholipids and the DNA molecule.68 The fluctuations at a wavelength of 2956 cm_1, detected in our study, may also indicate changes in lipid molecules including phospholipids, which in turn is also indicated by changes of the peak at 1070 cm_1 (ref. 69 and 70) (Table 3). Changes in this region could be more related to the presence of autoimmunity than to the consequences of diabetes mellitus, since, excluding individuals with autoimmunity from the control group, the peak at 1070 cm_1 does not appear in the healthy control group (Table 3). Spectrum changes of in the region 1040–1280 cm_1 could also indicate an extensive DNA rearrangement or even fragmentation.71 We previously showed that patients with T1D have a higher level of DNA breaks in PBMC than individuals without impaired glucose tolerance,68 probably related not only to glucose-induced processes but to autoimmune processes as well, as DNA damage is also registered in patients with another autoimmune diseases.72–75 Therefore, a shift of absorption band at 1070 cm_1, indicates changes between the symmetric –PO2 vibrational absorption peak in nucleic acids or phospholipids,69,70 but could also indicate bonds in carbohydrates.76 It is assumed that the increase of the peak in this region is due to the intensification of the glycolysis process in certain subpopulations of PBMC, which is characteristic of autoimmune processes,77,78 and this could be indicated directly by the peak absence at 1070 cm_1 in the control group without any autoimmune diseases (Table 3). Fructose 6-phosphate, an intermediate of glycolysis, has been reported to absorb at 1068–1070 cm_1.79–81 Therefore, our obtained data could indicate the proinflammatory spectrum of PBMCs of T1D patients and individuals without impaired glucose tolerance, but with autoimmunity, compared to individuals without autoimmune diseases.

We also detected statistically significant differences in the presence of peaks between the T1D patients and individuals without diabetes at a wavelength of 1417 cm_1 (Table 3), which indicates –CH2 symmetric scissoring vibrations.82 One might think that these changes are also more characteristic of an autoimmune disease per se, however if autoimmune individuals are excluded from a healthy control group, the p-value decreases, and when FTIR data from autoimmune individuals are added to data from T1D patients and analysed with healthy controls, the statistical significance remained and even increased (Table 3).

On the other hand, the amide II band at 1545 cm_1 (_N–H bond) shows small but statistically reliable shifts, thus indicating the changes in protein content and secondary structure64 specifically in spectra of patients with T1D.83 These changes persist even when individuals with self-reported autoimmunity are removed from the group with individuals without impaired glucose tolerance, while statistical reliability was lost when data from T1D patients and individuals without impaired glucose tolerance were combined and compared with healthy individuals. This reveals that the intensity increase of the peak at 1545 cm_1 is specific to T1D."
57,"Prediction of type 2 diabetes mellitus using hematological factors based on machine learning approaches: a cohort study analysis. Type 2 Diabetes Mellitus (T2DM) is a significant public health problem globally. The diagnosis and management of diabetes are critical to reduce the diabetes complications including cardiovascular disease and cancer. This study was designed to assess the potential association between T2DM and routinely measured hematological parameters. This study was a subsample of 9000 adults aged 35-65 years recruited as part of Mashhad stroke and heart atherosclerotic disorder (MASHAD) cohort study. Machine learning techniques including logistic regression (LR), decision tree (DT) and bootstrap forest (BF) algorithms were applied to analyze data. All data analyses were performed using SPSS version 22 and SAS JMP Pro version 13 at a significant level of 0.05. Based on the performance indices, the BF model gave high accuracy, precision, specificity, and AUC. Previous studies suggested the positive relationship of triglyceride-glucose (TyG) index with T2DM, so we considered the association of TyG index with hematological factors. We found this association was aligned with their results regarding T2DM, except MCHC. The most effective factors in the BF model were age and WBC (white blood cell). The BF model represented a better performance to predict T2DM. Our model provides valuable information to predict T2DM like age and WBC. In this study, a large number of biological and hematological factors like age, WBC, PDW, RDW, RBC, Sex, PLT, MCHC, and HCT had a significant relationship with T2DM. As we mentioned previously, we considered the association of the TyG index with hematological factors because of its positive relationship with T2DM presence. We found that the association of hematological factors with the TyG index was aligned with their results regarding T2DM, except MCHC. Therefore, we will continue the discussion based on the results of the T2DM and hematological factors. The most important and effective factors associated with T2DM presence were found to be age (as the most important and significant factors in the first line of DT) and WBC (as the second factor).

We found that in people over age of 47, the risk of diabetes increased dramatically. In line with our study, one study conducted in western Algeria on a sample of 1852 subjects, get these results with age 5025. In another study, the researchers indirectly found that the prevalence of T2DM was higher in middle-aged patients than in younger patients26. Contrary to our findings, a study on 307 diabetics showed that age had no significant relationship with the incidence or prevalence of diabetes1.

Our findings show that the WBC may be associated with the presence of T2DM. In people with a WBC_≥_5.4, the prevalence of diabetics was 4 times more than of non-diabetics. Similarly, Lindsay et al. found that high WBC has the protentional to be considered as T2DM after adjusting for age and sex31. Another study conducted in 2018 showed that high WBC count, a marker of subclinical inflammation, can be used as an indicator of T2DM due to obesity32.

One of the most important difficulties for diabetics is the increased risk of thrombotic events and coagulation problems33. Platelets, are the main cellular element of coagulation, and play an important role in this process, and disruption in their number, shape, and activation pathways (measured by PT and MPV criteria) can lead to coagulation problems. The results of our study indicated a direct association between PLT count and the risk of diabetes. Conversely to our findings, the results obtained from a study of 1852 Algerian subjects with 1059 type 2 diabetic patients showed negative effect of PLT on the onset of T2DM25 and Some studies just showed that PLT levels are not involved in the development of diabetes pathology34,35. The association between PLT and MPV and their effects on each other has been investigated and confirmed in other studies, but surprisingly we could not find any significant association between MPV and the incidence of diabetes. Similarly, a number of studies could not find any association36,35,36,39, but some have found conflicting results with showing positive effects40,39,42.

The association of RDW and diabetes are stated in different studies. G. Smith and et al. found that Low RDW is associated with increased incidence of T2DM43 .Nevertheless, a study conducted in China in 2018 shows a direct link between RDW and the incidence of diabetes44.

We also found that HCT was negatively associated with the presence of diabetes, and a 2020 study in Northwest Ethiopia confirmed this inverse relationship45. But in another study, they could not find a significant link between HCT and diabetes46.

We also found that like high WBC, high RBC and MCHC can also increase the risk of diabetes. As shown in the decision tree, it can be inferred that a decrease in RBC, lower than 4.73, can greatly decrease the risk of diabetes.

Similar to our results, a study of 87 bangal T2DM showed a correlation between high MCHC and RDW with T2DM47. However, the study carried out in Saudi Arabia on a population with T2DM showed a negative association between diabetes and MCHC48. And so, this factor needs to be further investigated to determine its exact link to diabetes.

According to the results of the study, we obtained that for each unit increase in RBC, the odds of having diabetes 1.64 times which indicates a strong effect of red blood cell count on the risk of T2DM. However, very few studies in the world have linked this factor, and most studies have only reported the effect of T2DM on changes in the appearance and properties of red blood cells49,48,51. Even a 2013 study by Zhan-Sheng Wang and Zhan-Chun Song, which examined the relationship between red blood cell count and its effect on microvascular complications in Chinese patients with T2DM, yielded conflicting results. It was found that the proportion of patients with microvascular complications increases with decreasing red blood cell count (p value below 0.001)52. Another study in India in 2019 examined the association and role of hematological factors in diabetes mellitus reported that poorly controlled diabetics were more likely to develop anemia53.

One of the most important strengths of our study is the large sample size used. The second strength is the wide age range used in the study, which easily includes the three age groups of young, middle-aged, and elderly, and examines this relationship in them. Also, in this study we examined a relatively large number of hematological factors and for some of these factors not many studies have been done globally yet.

One of the limitations of this study is that we did not measure HbA1c in participants of the MASHAD cohort study. Moreover, it would have been much better if we could have enriched the target community in terms of cultural diversity because our study population was adults in the Mashhad cohort who all live in a common geographical area with relatively similar customs and lifestyles. This makes it impossible to generalize the results of this study to the other countries or even the total population of Iran.

The results of this study can help health authorities in early diagnosis and prevention of diabetes by examining only a few simple hematological criteria."
58,"Predictive models for diabetes mellitus using machine learning techniques Background: Diabetes Mellitus is an increasingly prevalent chronic disease characterized by the body's inability to metabolize glucose. The objective of this study was to build an effective predictive model with high sensitivity and selectivity to better identify Canadian patients at risk of having Diabetes Mellitus based on patient demographic data and the laboratory results during their visits to medical facilities.

Methods: Using the most recent records of 13,309 Canadian patients aged between 18 and 90 years, along with their laboratory information (age, sex, fasting blood glucose, body mass index, high-density lipoprotein, triglycerides, blood pressure, and low-density lipoprotein), we built predictive models using Logistic Regression and Gradient Boosting Machine (GBM) techniques. The area under the receiver operating characteristic curve (AROC) was used to evaluate the discriminatory capability of these models. We used the adjusted threshold method and the class weight method to improve sensitivity - the proportion of Diabetes Mellitus patients correctly predicted by the model. We also compared these models to other learning machine techniques such as Decision Tree and Random Forest.

Results: The AROC for the proposed GBM model is 84.7% with a sensitivity of 71.6% and the AROC for the proposed Logistic Regression model is 84.0% with a sensitivity of 73.4%. The GBM and Logistic Regression models perform better than the Random Forest and Decision Tree models.

Conclusions: The ability of our model to predict patients with Diabetes using some commonly used lab results is high with satisfactory sensitivity. These models can be built into an online computer program to help physicians in predicting patients with future occurrence of diabetes and providing necessary preventive interventions. The model is developed and validated on the Canadian population which is more specific and powerful to apply on Canadian patients than existing models developed from US or other populations. Fasting blood glucose, body mass index, high-density lipoprotein, and triglycerides were the most important predictors in these models. In this research study, we used the Logistic Regression and GBM machine learning techniques to build a model to predict the probability that a patient develops DM based on their personal information and recent laboratory results. We also compared these models to other machine learning models to see that the Logistic Regression and GBM models perform best and give highest AROC values.

During the analysis, we also used the class weight method for our imbalanced dataset. We first tuned the class weight for the DM class to find the optimal class weight that minimized the average classification cost. We found that the optimal class weight for the GBM model is 3 and the optimal class weight for the Logistic Regression is 3.5. These optimal class weights are then incorporated into the model during the training process. We obtained similar results for GBM, Logistic Regression, and Random Forest model. However, the Decision Tree Rpart model gives a higher AROC at 81.8% compared to 78.2% when the threshold adjustment method was used (Additional file 1: Table S6). We also applied a natural logarithmic transformation on the continuous variables, however, this did not improve AROC and sensitivity.

Compared to the simple clinical model presented by Wilson et al. [18], the AROC value from our GBM model was very similar. The AROC value of our Logistic Regression model was lower, given the fact that the parental history of the disease was not available in our sample data. We also note that the characteristics of the sample data used in this study were not the same as the ones used by Wilson et al. [18]. For example, the age of the patients in our dataset ranges from 18 to 90, while the patients studied by Wilson et al. [18] ranges from 45 to 64. Schmid et al. [16] conducted a study on Swiss patients to compare different score systems used to estimate the risk of developing type 2 diabetes such as the 9-year risk score from Balkau et al. [1], the Finnish Diabetes Risk Score (FINDRISC) [13], the prevalent undiagnosed diabetes risk score from Griffin et al. [4], 10-year-risk scores from Kahn et al. [9], 8-year risk score from Wilson et al. [18], and the risk score from the Swiss Diabetes Association. Their results indicated that the risk for developing type 2 diabetes varies considerably among the scoring systems studied. They also recommended that different risk-scoring systems should be validated for each population considered to adequately prevent type 2 diabetes. These scoring systems all include the parental history of diabetes factor and the AROC values reported in these scoring systems range from 71 to 86%. Mashayekhi et al. [11] had previously applied Wilson’s simple clinical model to the Canadian population. Comparing our results to the results reported by Mashayekhi et al., the AROC values suggest that our GBM and Logistic Regression models perform better with respect to predictive ability. Using the same continuous predictors from the simple clinical model with the exception of parental history of diabetes, we also obtained an AROC of 83.8% for the Logistic Regression model on the test dataset."
59,"Prognostic Modeling and Prevention of Diabetes Using Machine Learning Technique Stratifying individuals at risk for developing diabetes could enable targeted delivery of interventional programs to those at highest risk, while avoiding the effort and costs of prevention and treatment in those at low risk. The objective of this study was to explore the potential role of a Hidden Markov Model (HMM), a machine learning technique, in validating the performance of the Framingham Diabetes Risk Scoring Model (FDRSM), a well-respected prognostic model. Can HMM predict 8-year risk of developing diabetes in an individual effectively? To our knowledge, no study has attempted use of HMM to validate the performance of FDRSM. We used Electronic Medical Record (EMR) data, of 172,168 primary care patients to derive the 8-year risk of developing diabetes in an individual using HMM. The Area Under Receiver Operating Characteristic Curve (AROC) in our study sample of 911 individuals for whom all risk factors and follow up data were available is 86.9% compared to AROCs of 78.6% and 85% reported in a previously conducted validation study of FDRSM in the same Canadian population and the Framingham study respectively. These results demonstrate that the discrimination capability of our proposed HMM is superior to the validation study conducted using the FDRSM in a Canadian population and in the Framingham population. We conclude that HMM is capable of identifying patients at increased risk of developing diabetes within the next 8-years. The increase in T2DM incidence is the main reason for increased diabetes prevalence around the world. It has a prolonged latent phase particularly in its early period and is thus poorly controlled29. Several meta analyses and clinical trials convincingly suggest that early interventions can postpone or prevent T2DM30,31. Early identification of high risk patients even when they are in a normoglycemic state is highly desirable, since interventions to prevent diabetes take time to implement. From a clinician and payor prospective, the development of such risk assessment techniques could enable optimal allocation of resources and healthcare services with greater confidence32. Although traditional risk factors for diabetes offer general guidance, they are ineffective for individual risk assessment33.

Several risk scoring models have been widely investigated to identify patients at high risk for developing T2DM as well as to communicate risk estimated effectively. Among them, the FDRSM is a well-known and widely used diabetes risk scoring model. This model was proposed to predict the 8-year risk of developing diabetes risk in middle-aged adults using 6 risk factors, including BMI, FBG, positive parental history of diabetes HDL, blood pressure and TG14. The FDRSM is primarily based on the data obtained from Framingham heart study. Technical details about the FDRSM and interactive risk calculator can be found on the Framingham heart study website (https://www.framinghamheartstudy.org).

The Framingham Heart Study is the first, most comprehensively characterized multigenerational and ongoing study of its kind. It continues to provide an effective platform for the primary prevention of chronic diseases. It has contributed to a paradigm shift in the history of medicine through its community-based approach. Despite of major contributions, this observational study is consuming a lot of resources and time. In such scenarios, special-purpose techniques are required. In line with the suggestion of the original paper, the Framingham offspring study, the FDRSM risk scoring model should be tested in various populations in order to ensure its validity in local population, Mashayekhi et al.15 proposed a study to validate the performance of the FDRSM in a Canadian population. The reported AROC was 78.6%, which is fair, given that parental history of diabetes was omitted because it was not available in the CPCSSN database. However, in the present study an effort has been made (1) to develop a HMM based diagnostic predictive model for leveraging EMRs data by utilizing temporal evolution of diabetes progression captured in repeated clinical measurements obtained from a longitudinal sample of patients (2) to validate the performance of the FDRSM as well as avoid some of the above mentioned limitations in order to assist health care professionals/physicians in investigating the 8-year risk of developing T2DM in an individual with the objective to control and manage the downstream consequences of diabetes. Unlike traditional machine learning techniques, the proposed HMM model has the ability to provide explicit information about prognosis, while utilizing the inherent temporal dependencies present in the data, and which is required to characterize disease risk and progression over time.

Our comparative analysis using a dataset with and without age, demonstrates that age does exhibit a significant association with diabetes risk, as slight under performance does occur when age is excluded from the dataset.

Unfortunately, this finding does not provide much guidance for T2DM prevention as age, along with sex, are non-modifiable risk factors. The remaining risk factors included in risk stratification are meaningful for the implementation of preventive and interventional measures in order to decrease the incidence of diabetes. Existing literature also highlights that modifiable risk factors contribute significantly to reduced risk of developing T2DM34. The Framingham study determined odds ratios of 1.00 and 1.15 for triglycerides and fasting blood sugar for predicting the 8-year risk of developing T2DM. The results of our study are consistent with the results of the Framingham study with respect to triglycerides (odds ratio 1.076 [95% CI, 0.862–1.343], p_<_0.005). However, in our study sample fasting blood sugar demonstrated an overly strong association with diabetes onset (9.936 [95% CI, 7.638–12.925], p_<_0.005). All other risk factors included in this study were also significantly associated with the incidence of diabetes. Comparative analysis of the percentage of people with low HDL levels in the Framingham research sample in Table 4 implies that the cut-off values for HDL should be revisited.

Validation of a risk-score model often involves plotting observed cases verses estimated probability35. We found an overlap between observed incidence and estimated probability in our analysis. Thus, estimated risk has a certain accuracy, however discrimination is the ability of the model to differentiate between individuals who have the disease from those who do not. We included the AROC analysis to evaluate the discriminatory capability of our proposed model to identify the 8-year risk of developing T2DM. The reported AROC for the proposed study is 86.9%, which is comparatively good, given that diabetic parental history is omitted due to its unavailability in the dataset. Experimental results also demonstrated that our proposed model has the potential to effectively predict the 8-year risk of developing T2DM in an individual.

These results are significant because in addition to identifying a-priori T2DM risk, this is the first study to evaluate the performance of the Framingham diabetes risk scoring model using a state of the art HMM. We believe this will motivate future investigations to apply ML methods to EMR data to assist in identifying the risk of developing various other diseases. The proposed method can be used easily by healthcare providers to identify high risk patients who may benefit from intensified prevention and intervention measures and as a result, halt or delay the onset of diabetes with reduced healthcare expenditure and improved healthcare services delivery.

It is estimated that people with diabetes are 2.6 times more likely to be hospitalized in the past year than people without diabetes (21% vs. 8%)36,37. The approximate healthcare expenditure for an individual with diabetes in the US is _$16,750 per year, of which _$9,600 is the direct cost of diabetes38. Economic costs and social burden of diabetes estimated by the American Diabetes Association demonstrates that the costs of diabetes increased by approximately 200% from 2002 to 201238. Given the newly predicted high risk individuals, a substantial fraction of healthcare cost and individual disease burden in our baseline dataset could be saved if clinicians and healthcare providers manage those high risk individuals promptly.

Despite the promising results, our study has several limitations. First, parental history of diabetes is missing in our model. This affects the internal validity of our proposed model. In addition, as our study sample only contains information related to those risk factors that were addressed by the FDRSM14,39,40, other risk factors incorporated in various risk scoring models are ignored (like, diet, physical activity, smoking, alcohol consumption and ethnicity). Second, the dataset used in this research is mainly obtained from a Canadian population; caution is required when generalizing these findings to other populations."
60,"Recent applications of machine learning and deep learning models in the prediction, diagnosis, and management of diabetes: a comprehensive review Diabetes as a metabolic illness can be characterized by increased amounts of blood glucose. This abnormal increase can lead to critical detriment to the other organs such as the kidneys, eyes, heart, nerves, and blood vessels. Therefore, its prediction, prognosis, and management are essential to prevent harmful effects and also recommend more useful treatments. For these goals, machine learning algorithms have found considerable attention and have been developed successfully. This review surveys the recently proposed machine learning (ML) and deep learning (DL) models for the objectives mentioned earlier. The reported results disclose that the ML and DL algorithms are promising approaches for controlling blood glucose and diabetes. However, they should be improved and employed in large datasets to affirm their applicability.
 The overview of the published papers discloses that machine learning models can effectively improve the prediction and management of blood glucose and diabetes. However, they should be improved and surveyed in large datasets. A significant advantage of these models is that they can be used as an application on mobile or any other diabetes management device. This can help for ensuring the health of diabetic patients and also for preventing further complications. Moreover, finding the potential risk factors can help to assess healthy subjects to prevent getting diabetes. It is predicted that the future prognosis of diseases and therapy lines depends on developing powerful machine learning methods.
"
61,"Reviewing Federated Machine Learning and Its Use in Diseases Prediction Machine learning (ML) has succeeded in improving our daily routines by enabling automation and improved decision making in a variety of industries such as healthcare, finance, and transportation, resulting in increased efficiency and production. However, the development and widespread use of this technology has been significantly hampered by concerns about data privacy, confidentiality, and sensitivity, particularly in healthcare and finance. The ""data hunger"" of ML describes how additional data can increase performance and accuracy, which is why this question arises. Federated learning (FL) has emerged as a technology that helps solve the privacy problem by eliminating the need to send data to a primary server and collect it where it is processed and the model is trained. To maintain privacy and improve model performance, FL shares parameters rather than data during training, in contrast to the typical ML practice of sending user data during model development. Although FL is still in its infancy, there are already applications in various industries such as healthcare, finance, transportation, and others. In addition, 32% of companies have implemented or plan to implement federated learning in the next 12-24 months, according to the latest figures from KPMG, which forecasts an increase in investment in this area from USD 107 million in 2020 to USD 538 million in 2025. In this context, this article reviews federated learning, describes it technically, differentiates it from other technologies, and discusses current FL aggregation algorithms. It also discusses the use of FL in the diagnosis of cardiovascular disease, diabetes, and cancer. Finally, the problems hindering progress in this area and future strategies to overcome these limitations are discussed in detail. Federated learning is a method for training ML models using decentralized data residing on different devices or systems as opposed to a central server. In the field of disease diagnosis, FL could be used to train models on a huge, distributed dataset of patient data from different hospitals or clinics. This method allows information and knowledge to be shared between facilities while protecting the privacy and security of patient data. Using a larger, more diverse dataset also allows for more accurate and robust models. However, implementations of federated learning for disease prediction, particularly cardiovascular disease, diabetes, and cancer, can be discussed from several perspectives, which are discussed in more detail in this section.

3.3.1. Models Performance: Competition between FL and ML
In classical ML, data collection is the first step in the execution of the known pipeline. It is also known that the accuracy of a trained ML model can be improved by collecting additional data. Therefore, it is agreed in theory that the accuracy of FL models will surpass that of traditional ML models because FL can access more data due to its nature.

In this context, the prediction results presented in Table 4 using FL show the high feasibility and accuracy. For example, the models in [110] achieved 98.9% accuracy in detecting cardiac arrhythmias, whereas the models in [108] had 87.85% accuracy. In addition, both models in [103,112] had area under the curve values of 0.78 and 0.89, respectively. However, these results are not better than any classical ML models used to predict CVDs. Even though the results of [110] are relatively high, a comparison between other implementations and classical implementations shows that the accuracy of the classical ML is higher. For example, the machine learning models proposed in [102] achieved over 91% accuracy in predicting CVDs 12 months before their onset. These results outperform all FL implementations in Table 4 except [110].

On the other hand, the FL implementations in diabetes diagnosis showed relatively high performance values, with the authors in [118] recording an accuracy of 99.24%, which is better than the traditional ML models used in this field, as explained by the authors. Moreover, in [116,117], the authors stated that the results obtained were comparable to those obtained with traditional DL models. However, the results in [119] are not as high as those obtained with other implementations, with an accuracy of 72%, which is lower than the results obtained with conventional ML models, as shown in [35].

Furthermore, the results presented in Table 6 were inconsistent in comparing the performance between FL and the classical ML and DL models. In this regard, the results obtained in [132,134,136,136] proved that the FL and ML models (including the classical ML and DL) have the same performance. However, the results obtained in [130,138,139,141] proved that the FL models outperform the earlier implementations of ML. In contrast, the authors of the results in [144] clearly stated that the models of DL outperform the models of FL, in contrast to the results in [143] where the authors stated that the models of FL have higher generalizability than the models of ML, but not higher accuracy.

In summary, although FL may theoretically have higher performance in machine learning, the results obtained are not yet sufficient to prove this hypothesis in the field of disease prediction. The FL implementations in this field are very accurate and feasible, but in some cases, the models of ML are still able to provide higher accuracy even if privacy is not preserved.

3.3.2. Real World vs. Research Implementations
Federated ML was proposed by Google in 2016 [30]. Although FL is still in its infancy, it has found widespread application in research, particularly in disease prediction.

However, most of the implementations performed, whether these were for cardiovascular diseases, diabetes, or cancer prediction, have been implemented as research studies rather than production methods. Moreover, most of these implementations are performed with publicly available data rather than using clinical or real-world data. For example, in the case of cardiovascular disease prediction, only [103] used real-world data from healthcare institutions and in the study in [104], real-world data from 10 individuals were used, whereas the others used either publicly available datasets or unspecified private data. In addition, none of these implementations were carried through to production readiness, but were conducted only as research studies.

In addition, the models for diabetes detection based on FL only used [121] data from a laboratory, whereas [118] used a dataset generated from a simulator and used other publicly available datasets. In addition, none of these implementations were taken to production maturity; all were conducted as research studies only. In contrast, for cancer detection, the studies in [139,142,143,144,145] used data from laboratories, whereas others used publicly available datasets, with the exception of [135,141], which used their own data without explaining their source. Similarly to the cardiovascular disease and diabetes cases, all studies were only research studies that were not production projects and were not made commercially available for further use. These findings support the fact that FL is still in its infancy and further efforts are needed to move into production phases with FL.

3.3.3. Dedication to Disease Diagnosis
The implementations of federated machine learning that have been performed in the field of predicting diseases such as cardiovascular disease, diabetes, and cancer have not all directly been for diagnosing diseases. For example, in the prediction of cardiovascular diseases, all of the studies listed in Table 4 were aimed at proving privacy-preservation concepts. In addition, the studies in [103,104] attempted to solve scalability problems using CVDs, while [108] attempted to solve personalization nodes using FL, and [110] addressed explainability, where reducing communication costs contributed to both privacy and personalization. In this context, only [109] addressed the disease itself, without targeting other FL-related topics, because it used a dataset from a clinical laboratory.

In contrast, the diabetes FL-based implementations summarized in Table 3 were all devoted to the disease itself, without targeting other FL-related topics. The same is true for the studies listed in Table 4, as this table only includes FL-based models dealing with cancer, whereas the authors in [129] mentioned dozens of articles proposing some FL-based models trained with cancers but focusing on FL-related topics.

FL-based models are therefore able to analyze data from different institutions that are not connected or related in the real world, using specific disease datasets while targeting other FL-related ideas such as scalability, communication costs, personalization, and so on. This may potentially help increase the efficiency and accuracy of intelligent models in predicting disease by giving them access to more data, while also helping to advance the field itself, clearly a win–win scenario for machine learning and health scientists.

3.3.4. Use of Smart Wearables
Smart wearables are known to provide people with continuous, long-term, and real-time monitoring. For example, fitness trackers and smartwatches have the potential to play an important role in the early detection and management of various diseases such as cardiovascular disease [34], diabetes [35], or even fatigue detection in the workplace, as shown in [36]. These tools can continuously monitor health data, such as the heart rate, and provide data that can help identify potential health problems. They also allow data to be collected outside of traditional healthcare settings, such as doctors’ offices and hospitals, so that a larger number of patients can be cared for over longer periods of time. Overall, the use of smart wearables can lead to the earlier diagnosis and treatment of diseases, improving outcomes and reducing healthcare costs.

The importance of smart wearables stems from their specifications, which have resulted from improvements in information and communication technologies (ICTs), the Internet of Things (IoT), and artificial intelligence. Smart wearables, as seen in [34,35,36], can be known as:

Non-invasive: do not penetrate the skin to collect data;
Compact: should not be bulky or large so as not to interfere with life activities;
Affordable: to increase its acceptance;
Rugged: to withstand harsh operating conditions such as light scratches or shocks;
Easy to use: should have an intuitive interface;
Durable power source: able to operate for a long period of time.
Despite the potential and usefulness of using smart wearables for disease detection using federated machine learning models, only one study ([104]) has employed a smart wearable to predict the onset of cardiovascular disease using data collected from a smartwatch for continuous, long-term, and real-time monitoring. In the other studies on cardiovascular disease, diabetes, or cancer, the use of smartwatches was not considered in the research. Therefore, there is still a great opportunity to merge smart wearables with the field of federated machine learning to enable private and secure model training without sharing confidential data.

3.3.5. Limitations in the Use of FL for Disease Prediction
In this sense, the use of federated machine learning in the detection and prediction of CVDs, diabetes and cancer is still in its early stages. In addition to the fact that not all FL implementations beat classical ML models, very rare real-world examples in this context can be obtained. In addition, it is also rarely seen that FL researchers used smart wearables in their experiments. All these details are mentioned in Table 7 below, which summarizes the results discussed in this section to provide a complete overview of how implementations based on FL have contributed to different concepts. Moreover, other limitations and challenges that are obtained in the field of FL and its implementations in disease prediction are mentioned in Section 4.1 below."
62,"Risk Stratification for Early Detection of Diabetes and Hypertension in Resource-Limited Settings: Machine Learning Analysis Background: The impending scale up of noncommunicable disease screening programs in low- and middle-income countries coupled with limited health resources require that such programs be as accurate as possible at identifying patients at high risk.

Objective: The aim of this study was to develop machine learning-based risk stratification algorithms for diabetes and hypertension that are tailored for the at-risk population served by community-based screening programs in low-resource settings.

Methods: We trained and tested our models by using data from 2278 patients collected by community health workers through door-to-door and camp-based screenings in the urban slums of Hyderabad, India between July 14, 2015 and April 21, 2018. We determined the best models for predicting short-term (2-month) risk of diabetes and hypertension (a model for diabetes and a model for hypertension) and compared these models to previously developed risk scores from the United States and the United Kingdom by using prediction accuracy as characterized by the area under the receiver operating characteristic curve (AUC) and the number of false negatives.

Results: We found that models based on random forest had the highest prediction accuracy for both diseases and were able to outperform the US and UK risk scores in terms of AUC by 35.5% for diabetes (improvement of 0.239 from 0.671 to 0.910) and 13.5% for hypertension (improvement of 0.094 from 0.698 to 0.792). For a fixed screening specificity of 0.9, the random forest model was able to reduce the expected number of false negatives by 620 patients per 1000 screenings for diabetes and 220 patients per 1000 screenings for hypertension. This improvement reduces the cost of incorrect risk stratification by US $1.99 (or 35%) per screening for diabetes and US $1.60 (or 21%) per screening for hypertension.

Conclusions: In the next decade, health systems in many countries are planning to spend significant resources on noncommunicable disease screening programs and our study demonstrates that machine learning models can be leveraged by these programs to effectively utilize limited resources by improving risk stratification. This study developed risk stratification models to predict the short-term (2-month) risk in a resource-limited setting for both diabetes and hypertension. Our primary analysis demonstrated that models from high-income countries do not generalize well to the low- and middle-income countries. In particular, the random forest model had the highest prediction accuracy for both diseases and was able to outperform the best baseline approach in terms of AUC by 35.5% for diabetes and 13.5% for hypertension. Our secondary analysis found that risk stratification can be accurately performed with limited data. A random forest model with access to only questionnaire-type features was able to capture 87% of the AUC obtained from a model with access to all features, suggesting that diabetes and hypertension risk stratification can be accurately conducted in extremely resource-limited settings. Although there are circumstances where advanced measurements may be required, eliminating the need for the corresponding tools means that community health workers require less training and can travel with fewer devices.

The observed performance difference between the baseline approaches and our models can be attributed to 3 improvements. First, our models were designed for short-term risk prediction, while the baseline models were designed for long-term prediction. Even though we retrained the baseline models with our data, the features included in the models were selected for long-term prediction. For example, none of the baseline models included self-reported symptoms (eg, dry tongue, urination), which may be more suitable for short-term prediction. Second, our models include additional features not used by the baseline approaches that may provide additional insight into the social, lifestyle, and genetic differences in the population. For example, none of the risk scores from high-income countries use self-reported symptoms or random blood glucose. Although random blood glucose is not typically used in high-income settings where HbA1c is preferred, it is often captured by community-based screening programs due to its operational simplicity (eg, no fasting required). For diabetes, random blood glucose was the most important feature and increased the AUC by 0.13, while for hypertension, random blood glucose was the second most important feature (see Figure S4 of Multimedia Appendix 1) and also led to an AUC increase. Third, we believe that the advanced machine learning models allowed us to extract maximum value from the small sample size and simple features available to us, whereas simple models with advanced features and large data sets may be equally effective in high-income settings.

As a by-product of our analysis, we externally validated the previously developed baseline approaches by using India-specific data. Although many of these models have been externally validated in a variety of settings, they have not been compared using India-specific data [10,11]. For example, the Framingham model for hypertensive risk has been validated in 7 countries with an average AUC of 0.80 (range 0.73-0.84) [11]. Our results show that the model is not as effective in India, where it had an average AUC of 0.70 after being retrained using local data. It is challenging to determine why the model performed poorly, but we believe that it may be due to subtle differences in the at-risk population, which manifest in the features selected by the model. Overall, our validation and comparison of baseline models highlights the importance of developing risk prediction models specifically for the low- and middle-income countries of interest.

The translation of our findings to the design and implementation of nation-wide screening programs must carefully consider costs, field accessibility, and disease management. The results of our secondary analysis indicate that the most impactful features (blood glucose, blood pressure, and heart rate) are measured using the most expensive field equipment (glucose monitor and blood pressure/heart rate cuffs). Even though these devices are more expensive, we find that including glucose monitors for diabetes screening and heart rate/blood pressure cuffs for hypertension screening can reduce the expected cost of incorrect risk stratification by US $1.35 and US $0.70, respectively (see Figure S7 of Multimedia Appendix 1). A formal cost-effectiveness analysis is needed to determine whether the gain in accuracy (and subsequent reduction is risk stratification cost) is worth the capital investment required to purchase glucose monitors and heart rate/blood pressure cuffs in low-resource settings.

There is also an important cost-tradeoff between a high false positive rate and a high false negative rate, which is determined by the discriminant threshold used to stratify patients into risk categories. Research suggests that the financial cost of a false positive is minimal (US $7 for diabetes and US $15 for hypertension) compared to that of a false negative (US $288 for diabetes and US $45 for hypertension) [22]. Our results demonstrated that the random forest model can reduce the number of false negatives by 620 patients per 1000 screenings for diabetes and 220 patients per 1000 screenings for hypertension. Extrapolating these results to a nationwide screening program in India that screens 600 million people [23] could save approximately US $1.19 billion for diabetes and US $960 million for hypertension by reducing the false negatives. In the next decade, the central government of India is planning to spend significant resources on noncommunicable disease screening programs [8] and our models can be leveraged by these screening programs to effectively utilize limited resources by improving risk stratification accuracy.

Despite the complex nature of our models, they can be easily implemented and computed into handheld tablets (or other mobile health devices) carried by community health workers without the need for a simplified, hand-computable risk score, which means we can provide the most accurate prediction without any extra effort or calculations by the community health workers. Furthermore, mobile health applications have demonstrated the ability to increase access to health care for low-income populations and to improve the capacity of the existing health systems [24]. Future research is needed to understand how to best integrate and present the risk stratification results into the community health worker workflow.

It is important to note that screening is only the first step to reducing the burden of noncommunicable diseases. Once high-risk patients are identified, they need to be linked to appropriate care and put on a disease management plan [25]. Linking patients to care and initiating disease management is a nontrivial process and governments need to carefully design nationwide disease management plans because otherwise, screening programs are unlikely to have the desired impact. Therefore, an important direction for future research includes studying the effect of screening programs on population health outcomes in the presence of current and enhanced linkages to care and disease management plans.

Our work has several limitations. First, we did not have access to an external validation set from a different study population (eg, rural slums, different state or country) to test our models. Second, our data displays a clear selection bias toward sicker patients visiting a physician within 2 months (see Table S1 of Multimedia Appendix 1). From a risk stratification perspective, the selection bias toward sicker individuals makes the problem more difficult because the model must discriminate between similar individuals. In other words, we need to identify those who actually have diabetes or hypertension from a pool of individuals who all appear to be at high risk. Finally, the differences in disease prevalence and overall health between our sample and the National Family Health Survey, Hyderabad suggest that, if applied broadly, our model may experience data shifting, which occurs when the training data differs from the application data [26]. See Table S3 of Multimedia Appendix 1 for a comparison of our data sample with the urban sample from India’s National Family Health Survey. Data shifting can negatively impact accuracy (similar to how the models from the United States and the United Kingdom performed poorly in India) and future research is needed to test our models in other settings.

In conclusion, this study found that a machine learning–based risk stratification model tailored to data collected by community-based screening programs can significantly improve risk stratification accuracy for both diabetes and hypertension in low-resource settings. Researchers and international organizations have proposed machine learning as a game changer in global health, [27-29] but there is limited documented evidence that machine learning can be effectively utilized in the resource-limited settings indicative of global health projects [30]. This study adds evidence to support machine learning in global health by quantitatively demonstrating the benefit of using these models in a novel resource-limited context."
63,"The importance of interpreting machine learning models for blood glucose prediction in diabetes: an analysis using SHAP. Machine learning has become a popular tool for learning models of complex dynamics from biomedical data. In Type 1 Diabetes (T1D) management, these models are increasingly been integrated in decision support systems (DSS) to forecast glucose levels and provide preventive therapeutic suggestions, like corrective insulin boluses (CIB), accordingly. Typically, models are chosen based on their prediction accuracy. However, since patient safety is a concern in this application, the algorithm should also be physiologically sound and its outcome should be explainable. This paper aims to discuss the importance of using tools to interpret the output of black-box models in T1D management by presenting a case-of-study on the selection of the best prediction algorithm to integrate in a DSS for CIB suggestion. By retrospectively “replaying” real patient data, we show that two long-short term memory neural networks (LSTM) (named p-LSTM and np-LSTM) with similar prediction accuracy could lead to different therapeutic decisions. An analysis with SHAP—a tool for explaining black-box models’ output—unambiguously shows that only p-LSTM learnt the physiological relationship between inputs and glucose prediction, and should therefore be preferred. This is verified by showing that, when embedded in the DSS, only p-LSTM can improve patients’ glycemic control. In this work we show that not interpreting black-box models could be potentially dangerous in some clinical and practical applications, i.e., when models are actively used to suggest therapeutic actions. We decided to consider as case-of-study the development of a simple LSTM-based DSS that suggests CIBs to mitigate the duration of post-prandial hyperglycemia. Corrective actions are suggested by exploiting the BG predictions provided by the LSTMs. To this end, we designed two ad-hoc LSTM models, np-LSTM and p-LSTM, which rely on the same input features and the same structure. The only difference between the two is a non-learnable, pre-processing layer in p-LSTM, which is placed between the input layer and the hidden LSTM layer. Training machine and deep learning models for accurate glucose prediction typically requires large amount of data. While glucose data can be easily recorded by CGM devices and insulin data by the infusion pump, a common challenge arises with patient’s manually reported information, such as CHO intake. The burden of logging data through a dedicated device or an electronic diary often leads to inconsistency in patients providing their daily diet and treatment information. As an example, subject 567 announced only 31 of the (at least) 141 meals expected during the monitoring period. In fact, when no or few instances of meal intake are provided, the algorithm may be unable to learn any contribution of CHO on future glucose levels. Instead, it may associate the combined effect of insulin and CHO only on the insulin input. Therefore, poor data recording can negatively impact the learning of any model.

To address this issue, we have carefully selected subject ID 588 for conducting our analysis. Among the 12 individuals of the dataset, this patient proved to report the largest number of meal and insulin information during the whole monitoring period (accurately and consistently).

In this context, the key parameter for choosing one among many competing predictive models is prediction accuracy. Therefore, as described in Table 1, we evaluated the models ability to accurately forecast glucose ahead in time in terms of RMSE, MAE and TG. Considering these numbers, it is not completely clear which model to prefer. Both networks provide similar results in terms of accuracy, with the np-LSTM network performing slightly better than p-LSTM.

Nevertheless, when looking at the summary plots, it is clear that the two LSTM models work differently, even though they are fed by the same input features. Glucose levels are expected to rise after CHO intake; vice versa, glucose should decrease after an insulin bolus. The SHAP values in Fig. 1 reveal that p-LSTM has learned this physiological explanation. The other model, np-LSTM, has instead learned an incorrect explanation of insulin and CHO. The SHAP values in Fig. 1 show that insulin positively contributes to the model output in np-LSTM for both PH=30 min and PH = 60 min. It means that np-LSTM will forecast a glucose rise after any insulin bolus, even when CHO are not consumed. This happens because of the collinearity between insulin and CHO in the dataset, which makes it difficult for the learning algorithm to discriminate their individual effect on the output. In conclusion, by looking at the summary plots, the most suitable model for any decision-making application seems to be the p-LSTM. To better understand how p-LSTM relates the input to the prediction, we analyzed some specific instances of the dataset. Firstly, we considered those instances from Fig. 1b for which the SHAP values of insulin and CHO are 0. These instances usually happen during nighttime or during the fasting periods between two consecutive meals, when no CHO are consumed and insulin is infused at basal rate. For these instances, CHO and insulin do not contribute to the model output, and the prediction relies exclusively on CGM. A representative example of these instances is shown in Fig. 3a which reports a portion of the training set during night. CGM data (top panel, blue dotted line) is within the normoglycemic range (black dashed lines) and slowly rising. During nighttime, there are no insulin boluses and the insulin input equals the constant basal rate (middle panel, green line). Also, no meals (bottom panel) have been consumed. The 30-minute ahead in time prediction provided by p-LSTM (top panel, black cross) is close to the target concentration and SHAP attributes a null contribution to insulin and CHO, while a nonnegative contribution is assigned to CGM.

By looking at Fig. 1b, it could also be interesting to focus on the opposite scenario, i.e., by considering all the instances in which both CHO and insulin have a SHAP value significantly different from 0. This condition can happen at meal time, when patients ingest CHO and an insulin bolus is administered. Figure 3b shows a representative example. A meal, most likely a dinner, of about 90 g of carbohydrates was ingested by the patient at 18:45. To avoid glucose exceeding the normoglycemic levels, an insulin bolus of more than 5 U has been administered. Also in this case the 30-minute ahead prediction provided by p-LSTM is close to the target glucose concentration. For this instance, SHAP reveals that: (i) CHO is the most important feature and has a positive contribution to the prediction (SHAP value = 0.76), which is in line with the physiological expectation that CHO ingestion increases glucose levels; (ii) insulin is the second important feature and has a negative contribution (SHAP value = _ 0.27); (iii) CGM only marginally contributes to the model prediction (SHAP value = _ 0.1).

To enforce previous considerations, thanks to a simulation tool (ReplayBG) we applied both p-LSTM and np-LSTM models to suggest CIBs based on LSTM predictions. Table 2 supports the finding drawn by summary plots, indicating that the most appropriate model for this particular case-study is the one aligned with the physiological meaning, although the p-LSTM model provides slightly lower prediction performance compared to the np-LSTM model.

Limitations of the SHAP analysis
Despite the interesting results provided by the summary plot, we acknowledge some limitations of SHAP that can lead to erroneous interpretations for particular instances of the dataset. Let’s assume, for instance, that a model is systematically misinterpreting the action of carbohydrates ingestion on future glucose levels because of confounding factors like physical exercise that increases insulin sensitivity and lowers glucose concentration. In this context, the summary plots provided by SHAP would show that a high value of the feature CHO is associated with a low (or even negative) SHAP value. From a physiological viewpoint, this is not in line with the expectations. Similar situations may also happen because of exogenous disturbances affecting the glucose-insulin system, like stress, alcohol intake, illness and menstrual cycle. We acknowledge that SHAP can only grant an explanation about how much each feature relates to the model prediction and it is not to be used for providing causal inference, i.e., for finding the true causes that lead to a specific event. On the other hand, biases in the learning process can only be identified by combining the interpretation provided by SHAP with physiological knowledge.
"
64,"Understanding Type 2 Diabetes Mellitus Risk Parameters through Intermittent Fasting: A Machine Learning Approach. Type 2 diabetes mellitus (T2DM) is a chronic metabolic disorder characterized by elevated blood glucose levels. Despite the availability of pharmacological treatments, dietary plans, and exercise regimens, T2DM remains a significant global cause of mortality. As a result, there is an increasing interest in exploring lifestyle interventions, such as intermittent fasting (IF). This study aims to identify underlying patterns and principles for effectively improving T2DM risk parameters through IF. By analyzing data from multiple randomized clinical trials investigating various IF interventions in humans, a machine learning algorithm was employed to develop a personalized recommendation system. This system offers guidance tailored to pre-diabetic and diabetic individuals, suggesting the most suitable IF interventions to improve T2DM risk parameters. With a success rate of 95%, this recommendation system provides highly individualized advice, optimizing the benefits of IF for diverse population subgroups. The outcomes of this study lead us to conclude that weight is a crucial feature for females, while age plays a determining role for males in reducing glucose levels in blood. By revealing patterns in diabetes risk parameters among individuals, this study not only offers practical guidance but also sheds light on the underlying mechanisms of T2DM, contributing to a deeper understanding of this complex metabolic disorder. Over the past decade, the landscape of T2DM care has witnessed remarkable progress, ushering in a new era of personalized and holistic approaches.
Utilizing cutting-edge methods, treating T2DM has taken innovative paths that hold promising potential. Stem cell therapy, for instance, represents a forward-looking approach that aims to harness the regenerative capabilities of stem cells to address the underlying factors of T2DM [47,48,49]. This method targets the restoration of damaged pancreatic beta cells responsible for insulin production, thereby enhancing the body’s glucose regulation. Stem cells sourced from various origins, such as adipose tissue or bone marrow, offer a way to replenish beta cell populations and mitigate the inflammatory response associated with T2DM. While stem cell therapy is in its early stages, preliminary clinical trials and preclinical studies have shown encouraging results, with improved glycemic control noted in certain patients. However, challenges like selecting optimal cell sources and ensuring long-term efficacy remain [50].
In addition to stem cell therapy, another emerging approach gaining traction is the use of CRISPR-Cas9 gene editing technology to modify genes related to T2DM [51]. This innovative method aims to target the root causes of the disorder by directly manipulating key genes involved in insulin production and glucose regulation. Although still in its infancy, precision gene editing offers the potential for more focused and durable interventions. This strategy directly tackles genetic factors contributing to T2DM and has the potential not only to manage but also to potentially reverse the condition’s progression. However, thorough research and clinical trials are essential to fully understand its safety, efficacy, and long-term impacts [52].
Among the various avenues explored, IF has garnered considerable attention as an alternative approach to conventional T2DM treatments [10,18,19,21,22,53]. IF entails cyclic patterns of controlled eating and fasting, demonstrating the potential to enhance insulin sensitivity, enhance glucose metabolism, and reduce inflammation. Unlike strict diets or exercise regimens, IF is adaptable to individuals’ lifestyles and embraces a natural, holistic methodology without significant side effects.
IF primary mechanisms to improve T2DM risk parameters involve metabolic changes that enhance overall metabolism and trigger tissue-specific metabolic adaptations. These adaptations include modifications in the gut microbiota, remodeling of adipose tissue, restoration of circadian rhythm balance, and increased autophagy in peripheral tissues [19,53].
IF offers a promising approach for treating T2DM, though individual responses can vary based on factors such as age, metabolic profile, and overall health. While some experience significant improvements, others may observe minimal changes, highlighting the need for personalized diabetes management. To address this, a recommendation system powered by machine learning analyzes individual characteristics to tailor IF guidance, maximizing its benefits. Age, weight, and BMI also play crucial roles, influencing outcomes as metabolic conditions differ. Recognizing these factors is vital for understanding conflicting study results and comprehensively evaluating IF’s potential benefits and limitations for T2DM intervention.
IF, however, does not exist in isolation. Rather, it complements and enriches the existing arsenal of treatments. Traditional approaches continue to hold value in managing T2DM, especially when tailored to each patient’s needs. Combining IF with pharmacological interventions and exercise can create a comprehensive regimen that addresses the multifaceted nature of T2DM. Moreover, the synergy of IF with advancements in precision medicine further refines treatment strategies. Utilizing machine learning algorithms to recommend personalized IF plans [54] aligns with the broader trend of precision medicine, where interventions are customized to individuals based on genetic, metabolic, and lifestyle factors.
The results of this study allow us to conclude that weight is the crucial feature for females, while age is the determining factor for males to reduce glucose levels in blood. Furthermore, the results reveal the substantial role of BMI in determining the appropriateness of IF for reducing HOMA-IR in individuals below 59 years old. In contrast, for individuals aged 59 and above, gender, specifically for women, appears to have a higher likelihood of benefiting from the IF approach in reducing HOMA-IR. Leveraging advanced machine learning techniques, such a recommendation system holds the potential to provide highly personalized and customized recommendations, thereby optimizing the advantages of intermittent fasting for various subgroups within the population. Moreover, the development of such a recommendation system will contribute to our understanding of the underlying mechanisms behind T2DM and explore potential clinical applications of intermittent fasting in a more precise and individualized manner."
65,"Use and performance of machine learning models for type 2 diabetes prediction in community settings: A systematic review and meta-analysis Objective: We aimed to identify machine learning (ML) models for type 2 diabetes (T2DM) prediction in community settings and determine their predictive performance.

Method: Systematic review of ML predictive modelling studies in 13 databases since 2009 was conducted. Primary outcomes included metrics of discrimination, calibration, and classification. Secondary outcomes included important variables, level of validation, and intended use of models. Meta-analysis of c-indices, subgroup analyses, meta-regression, publication bias assessments and sensitivity analyses were conducted.

Results: Twenty-three studies (40 prediction models) were included. Studies with high-, moderate-, and low- risk of bias were 3, 14, and 6 respectively. All studies conducted internal validation whereas none conducted external validation of their models. Twenty studies provided classification metrics to varying extents whereas only 7 studies performed model calibration. Eighteen studies reported information on both the variables used for model development and the feature importance. Twelve studies highlighted potential applicability of their models for T2DM screening. Meta-analysis produced a good pooled c-index (0.812). Sources of heterogeneity were identified through subgroup analyses and meta-regression. Issues pertaining to methodological quality and reporting were observed.

Conclusions: We found evidence of good performance of ML models for T2DM prediction in the community. Improvements to methodology, reporting and validation are needed before they can be used at scale. Meta-analysis demonstrated good discrimination ability of ML models for T2DM prediction in community settings, suggesting ML could enhance T2DM detection in the general population since hospital and primary care data were excluded. Nevertheless, findings should be considered cautiously given the heterogeneity, publication bias and other shortcomings. Limitations in current ML studies for T2DM prediction (Supplementary Table S21) and recommendations to overcome them (Supplementary Table S22) are discussed below.

Artificial neural networks performed best, closely followed by logistic regression, decision trees and random forests. This is in tandem with recent findings by Lai et al. [50] whose prediction models built with gradient boosting and logistic regression outperformed random forest and decision trees. Moreover, linear and non-linear classifiers excelled over ensembles, confirming logistic regression could be as robust as other algorithms for certain classification tasks.

A number of studies reported sparingly on study design. Studies which developed prognostic models did not always consider time-varying covariates whilst analyses were conducted cross-sectionally. This is crucial when temporal decay in predictors occur and for judging causality of associations. Also, none of the studies used algorithms amenable to longitudinal designs such as mixed-effects machine learning [51], RE-EM trees [52], or random survival forests [53]. Cross-sectional studies by default do not support causal inference but over 60 % of the studies (14/23) were cross-sectional. Although it would be prudent to restrict the sampling of a cross-sectional study to undiagnosed cases, this was not universally followed. Furthermore, biases associated with observational designs were rarely accounted for. Since subgroup analyses revealed prospective cohort and interventional studies produced more robust models than cross-sectional studies, whilst studies with low risk of bias performed better than moderately biased ones, data quality seemingly plays an important role in achieving higher predictive performances.

Although thorough reporting of various dimensions of predictive performance is essential to determine the value of a prediction model, studies often lacked adequate information on predictive performance. Most studies described a single dimension of predictive performance; c-index or classification accuracy. Confusion matrices were presented sparingly. Moreover, uncalibrated models are likely to be of limited use in real-world scenarios [54]. Therefore, detailed reporting of model performance, consisting of information such as classification, discrimination, calibration measures, variable importance, applicability, and acceptability, is recommended.

Noteworthily, none of the studies conducted external validation, creating uncertainty about their generalizability. Multiple internal validation methods, though could be more efficient, were applied only by 1 study. Even when large samples feasible for random splitting were available, some studies resorted to cross-validation, which is likely to be less efficient than random partitioning, as the validation is done on alternating sub-samples of the same data used for training models. A recent study concluded cross-validation is insufficient for ML model validation [55]. Models devoid of stringent validation offer limited applicability. Therefore, we recommend at least a thorough internal validation is performed before ML prediction models are used at scale.

Methodological flaws were seen in several areas specific to ML approach. Only 8 studies conducted hyperparameter tuning and feature selection. Sample size was unduly small in some studies, the smallest being 234 and less than 1000 in 4 studies. Machine learning, by default, is effective with large samples as they account for multi-dimensionality [56]. Also, smaller studies are less likely to be of broad public health significance. Although class imbalance was prevalent, few studies attempted to address it with recommended techniques such as resampling [57] and cost-sensitive learning [58]. As this is encountered in many disease conditions, it is imperative that class imbalance handling is incorporated into future guidelines. Despite the ability of ML to tackle multidimensionality, majority of studies (n_=_16) used less than 20 features for modelling. As ML is evolving from its “black box” paradigm to more interpretable stages, a narrow focus on predictive performance per se is unwarranted and further insights into underlying dynamics are expected [59]. For example, most algorithms currently produce measures of variable importance in addition to predictive performance metrics [60,61]. However, 3 studies did not contain any information on important predictors.

Since ML is an iterative “trial and error” process, combined use of algorithms, preferably a mix of linear, non-linear, and ensemble learners would be best to generate an optimal model. However, such extensive modelling approaches were not frequently adopted. For instance, only 14 studies used multiple algorithms while 9 were developed using a single linear or non-linear algorithm. Unsupervised ML, which could discover underlying structures and patterns of multidimensional data [62], were sparingly used. Also, hybrid algorithms combining steps such as pre-processing, feature selection and modelling together, or with other techniques such as systems dynamics, although known to perform well [63,64], were not applied.

A range of biases was identified by a recent systematic review of ML clinical prediction models which highlighted the need for improvements in methodology and reporting [65]. The current review demonstrated that these drawbacks prevailed in community-based studies as well. Inverse associations between predictive performance and ethically-approved studies and those conducted in low- or middle-income countries are intriguing. Presumably, due to the nature of data sources, ethics approval may not have been required from the countries within which the studies were conducted. Regardless, these findings point to the importance of conducting a comprehensive evaluation of the methods employed in predictive modelling without solely focusing on predictive performance.

Lack of reporting standards in ML modelling studies to assist in systematic reviews was a challenge. Related guidelines such as CHARMS [34] and TRIPOD [35] have not incorporated unique features that should be assessed in ML models. Furthermore, PROBAST guidelines [37] also have not incorporated certain biases associated with ML. Also, guidelines are required to inform researchers on feature selection, as this was altogether omitted or conducted in an ad hoc manner using diverse methods. Perhaps due to lack of reporting guidelines, we observed variations of the extent and style of reporting of studies across journals. Therefore, a set of guidelines for conducting, reporting, and evaluating ML predictive modelling studies in healthcare, is recommended.

While high discrimination ability was achieved by majority of models, issues of applicability, feasibility, and translatability were not sufficiently discussed by respective studies. Two studies had no clear presentation of the developed models as formulas or graphically whilst eleven studies did not develop any prediction tools. This perhaps indicates lack of emphasis on translating ML models into scalable products. Moreover, 5 studies did not present clear information about intended use of their models. Therefore, translation of these prediction models into broader use such as population-wide screening is likely to face obstacles."
66,"Use of Machine Learning Approaches in Clinical Epidemiological Research of Diabetes Purpose of review: Machine learning approaches-which seek to predict outcomes or classify patient features by recognizing patterns in large datasets-are increasingly applied to clinical epidemiology research on diabetes. Given its novelty and emergence in fields outside of biomedical research, machine learning terminology, techniques, and research findings may be unfamiliar to diabetes researchers. Our aim was to present the use of machine learning approaches in an approachable way, drawing from clinical epidemiological research in diabetes published from 1 Jan 2017 to 1 June 2020.

Recent findings: Machine learning approaches using tree-based learners-which produce decision trees to help guide clinical interventions-frequently have higher sensitivity and specificity than traditional regression models for risk prediction. Machine learning approaches using neural networking and ""deep learning"" can be applied to medical image data, particularly for the identification and staging of diabetic retinopathy and skin ulcers. Among the machine learning approaches reviewed, researchers identified new strategies to develop standard datasets for rigorous comparisons across older and newer approaches, methods to illustrate how a machine learner was treating underlying data, and approaches to improve the transparency of the machine learning process. Machine learning approaches have the potential to improve risk stratification and outcome prediction for clinical epidemiology applications. Achieving this potential would be facilitated by use of universal open-source datasets for fair comparisons. More work remains in the application of strategies to communicate how the machine learners are generating their predictions. In our review of machine learning approaches for the study of diabetes clinical epidemiology, we found three common themes. First, it is increasingly common to generate risk scores for diabetes complications, including both macrovascular and microvascular outcomes, as well as progression from pre-diabetes to diabetes, using tree-based machine learning methods such as random forest or gradient boosting machines. The literature has consistently found that these tree-based approaches may have key advantages over standard logistic or survival methods for designing a risk score. In particular, the tree-based methods have the advantage of being able to capture nonlinear interactions with greater ease in a data-driven manner and have the ability to rigorously identify subgroups with uniquely higher or low risk than the mean patient. The success of the XGBoost algorithm, in particular, demonstrates that tree-based methods can provide substantial improvements over standard logistic regression with some complex risk prediction problems. In addition, the increasing popularity of ensemble methods highlights the fact that the ideal learner for any given problem may not be known a priori, but a combination of learners can often address complex problems empirically. Effectively, this means that researchers do not have to choose a single learner; one can apply logistic regression, random forest, gradient boosting machines, and other approaches to the same problem. Additionally, the review found the usefulness of neural networking methods applied to image data analysis, particularly for the study of diabetes retinopathy and diabetic skin ulcers. The key limitation of this field is the assembly and widespread agreement on reference datasets that are judged by experts and can be used to fairly compare and train newer machine learners. Finally, our review found a common struggle among researchers to clearly explain and illustrate what variables, or relationships among variables, were causing machine learning models to improve prediction or classification of the outcomes of interest, and how to identify and communicate the internal workings of the machine learner.

The three themes also relate to key insights for future research. First, the increasing popularity of machine learning model explanation methods, such as variable important ranking methods and partial dependence plots to illustrate how machine learners are using different variables, will be vital to ensuring that the machine learners can be explained and communicated. Even if such learners are being used purely for “Black Box” predictions, clinicians will inevitably want to understand how the learners are working, even if such learners cannot be interpreted as causal inference tools or as providing a mechanistic explanation for an underlying relationship. Machine learning experts have increasingly used a method called locally interpretable model explanations (LIME), to provide examples of how different types of patients would be treated by a learner, effectively as a sophisticated subgroup analysis tool [128]. The LIME method may be increasingly helpful for diabetes researchers generating risk prediction, stratification, or clustering tools. Additionally, the wide application of neural networks to image analysis has shown potential but requires extensive translation into practice. In particular, due to the structure of clinical practice, it remains unclear when the clinician would benefit from consulting a machine learning model before, after, or instead of an expert peer such as an ophthalmologist or dermatologist. The risk in terms of harm to the patient and liability structures of modern medicine may make it challenging to initially adopt such technologies. Hence, it is important to plan future randomized clinical trials to evaluate the true effectiveness of these novel methods, similarly to how we would evaluate any other intervention designed to improve patient care.

In the future, we anticipate that machine learners will be increasingly built into electronic health record systems, lowering the threshold and barriers to utilization, similarly to how prognostic risk models are currently built into such record systems. It therefore serves the clinician to understand the benefits and limitations of these modeling methods. In particular, although such learners increasingly have predictive power over their traditional alternatives, the most important caveat we have from reviewing the literature is that it is critical for the learners to be tested on data independent from the datasets on which they were trained. It is quite common, in our view, for machine learning authors to claim that their learners are superior to the status quo but not to test the learners on broad independent datasets from diverse populations. A related problem is that of implicit bias. Many recent machine learning commentaries have indicated that if the datasets against which the machine learners are trained have implicit biases—such as confusing lower diagnostic rates among minorities as being indicative of lower risk among those minority groups—then the learners will propagate bias and inequality in the system [129]. It is imperative to sample and train learners in ways that help to detect such biases and actively reverse them, rather than assuming our most convenient datasets are necessarily accurate ones.

As these problems are addressed in ongoing research studies, our review finds that machine learning approaches have the potential to improve risk stratification and outcome prediction for clinical epidemiology applications. Achieving this potential would be facilitated by using universal open-source datasets for fair comparisons. More work remains in the application of strategies to communicate how the machine learners are generating their predictions."
67,"Using machine learning to predict severe hypoglycaemia in hospital Aim: To predict the risk of hypoglycaemia using machine-learning techniques in hospitalized patients.

Methods: We conducted a retrospective cohort study of patients hospitalized under general internal medicine (GIM) and cardiovascular surgery (CV) at a tertiary care teaching hospital in Toronto, Ontario. Three models were generated using supervised machine learning: least absolute shrinkage and selection operator (LASSO) logistic regression; gradient-boosted trees; and a recurrent neural network. Each model included baseline patient data and time-varying data. Natural-language processing was used to incorporate text data from physician and nursing notes.

Results: We included 8492 GIM admissions and 8044 CV admissions. Hypoglycaemia occurred in 16% of GIM admissions and 13% of CV admissions. The area under the curve for the models in the held-out validation set was approximately 0.80 on the GIM ward and 0.82 on the CV ward. When the threshold for hypoglycaemia was lowered to 2.9 mmol/L (52 mg/dL), similar results were observed. Among the patients at the highest decile of risk, the positive predictive value was approximately 50% and the sensitivity was 99%.

Conclusion: Machine-learning approaches can accurately identify patients at high risk of hypoglycaemia in hospital. Future work will involve evaluating whether implementing this model with targeted clinical interventions can improve clinical outcomes. In this study of hospitalized patients on a GIM or CV ward, we were able to utilize natural-language processing and machine learning to identify patients at an increased risk of hypoglycaemia. Our model's area under the curve of approximately 80% suggests it can accurately identify these higher-risk patients. However, the rate of hypoglycaemia in a 24-hour window was rare and thus our model's positive predictive value was low (ie, less than 15%).

The low positive predictive value indicates that our model would lead to a high number of false alarms if implemented indiscriminately into clinical practice.21, 22 Specifically, although approximately 15% of hospitalizations involved hypoglycaemia, our model was generating predictions for the subsequent 24_hours and this necessarily diluted the rate of hypoglycaemia. If instead we generated predictions for the subsequent 7_days, our model's true positive rate would have been higher. However, a 7-day prediction window may be of limited clinical utility because most patients' length of hospitalization is 4_days and because much can change during 7_days that could increase or decrease a person's risk of hypoglycaemia. This observation highlights the trade-off between selecting a prediction window that is both clinically relevant and has adequate model performance.

To avoid the resultant alert fatigue from our model's low positive predictive value, a more effective approach might be applying the model to identify patients in the highest decile of risk.21, 22 Specifically, our model could be implemented to provide an attending physician with a daily list of patients who are in the highest decile of predicted risk for hypoglycaemia in the next 24_hours. A daily list, as opposed to an electronic medical record prompt for each high-risk patient, would, at least in theory, be less bothersome for clinicians because it would not lead to multiple alerts throughout the day. Focusing on patients in the highest decile of risk would necessarily mean that some patients with hypoglycaemia may be missed, and this is an important trade-off to avoid a high false-positive rate. Based on our study, focusing on the highest decile of risk has the potential to reduce hypoglycaemia by approximately 50% if all hypoglycaemic events could be prevented in this risk group. However, this is speculative, based on projections from our model's performance and requires evaluation in a prospective study to understand how the model performed when implemented in clinical practice.

Our study applied multiple modelling techniques, which naturally leads to the question of which modelling approach was “best”. However, there is no single model characteristic that can be used to make this determination. Instead, this is a subjective determination depending on what the clinician and patient value the most. For example, if the most important aspect is avoiding severe hypoglycaemia then the model with the highest sensitivity might be considered the “best”. If instead the most important aspect is to avoid false alarms, then a model with a higher threshold would be preferred.

A strength of the present study was that we utilized both structured (eg, laboratory values) and unstructured (eg, free-text nursing notes and electronic admission notes) data, neither of which would require manual input by the treating physician. While the former are commonly used in clinical prediction,12, 23, 24 the latter are less often used because of the intrinsic challenges of working with unstructured data. However, analytical packages freely available in R and Python have made it easier to transform unstructured data into a format that can be seamlessly incorporated into a prediction model. Previous studies have demonstrated that including unstructured data decreases the amount of missing data, provides potentially rich clinical information, and can improve model performance.25, 26 Outside of individual studies, free-text data are seldom used to predict clinical outcomes in healthcare organizations and this probably relates to the fact that free-text data can be challenging to work with and the use of natural-language processing in clinical prediction is still relatively new.

The most commonly used model to identify patients at an increased risk of hypoglycaemia had an area under the curve of approximately 0.81 (sensitivity and positive predictive value were not reported).12 However, since that model focused on outpatients, rather than inpatients, it is challenging to make a direct comparison to our model. Furthermore, unlike our model it did not use free-text notes or time-varying covariates (eg, insulin dose) and applied logistic regression. Another study of hospitalized adults also applied logistic regression to identify patients at an increased risk of hypoglycaemia in the subsequent 7_days using similar glucose thresholds to those used in the present study.13 The one important difference is that they only included patients who received insulin during their hospitalization, which is the strongest risk factor for hypoglycaemia. Their model also approximated oral intake, albeit using diet orders, and included similar variables to those included in our model. In their validation set using a glucose threshold of 3.9_mmol/L (70_mg/dL), they identified an area under the curve of 0.77 and a positive predictive value of 12.4%.13 These results are similar to our modelling using penalized logistic regression (“LASSO regression”), and lower than our model using extreme gradient boosting or a recurrent neural network.

Another interesting finding of the present study was that up to 30% of patients were transferred to the intensive care unit in the subsequent 24_hours after experiencing hypoglycaemia. It is unknown what proportion of these patients were transferred because of severe hypoglycaemia or because of another underlying process that caused hypoglycaemia (eg, acute renal failure, sepsis). Regardless, it underscores the importance of identifying patients who are at high risk of hypoglycaemia so they can be quickly assessed and closely monitored during the high-risk time period. Of course, a prospective study is necessary to know whether identifying patients at high risk of hypoglycaemia can prevent its occurrence and the related clinical sequelae.

There are a number of additional important limitations to our study. First, while we used natural-language processing to estimate oral intake by reviewing nursing notes, we lacked precise estimates of caloric intake and the quantity of carbohydrates consumed with each meal. Second, there were other important sources of missing data that we were unable to include because they were either not available electronically (eg, handwritten daily progress notes, intraoperative notes) or incompletely captured electronically (eg, daily weight, physical activity). Third, prospective data are necessary to understand the utility of implementing the model. Without these data it is unknown what the clinical implications will be when the model is deployed in real time into clinical care. Specifically, while we were able to estimate the potential reduction in hypoglycaemia, this does not account for potential changes in practice (eg, reduction in insulin dose for patients with renal failure or poor oral intake) in response to the model's output that could potentially benefit not only the patients in the highest risk group but also patients in the lower risk groups. Fourth, further research would be needed for model interpretability to identify why a given patient was specifically identified as high risk. Fifth, another important limitation of our study was missing data, which is common in healthcare databases. Because LASSO regression requires complete data, we used the approach of last observation carried forward but it is important to note that there are other methods available for handling missing data (eg, restricting the cohort to all those with complete data, multiple imputation). Finally, our study was conducted on a medical ward and surgical ward at one academic hospital, and thus it is unknown how results will generalize to other settings (eg, a community hospital).

In conclusion, in this 6-year study of hospitalized patients, we were able to use machine learning and natural-language processing to identify those at highest risk of hypoglycaemia. Further studies are needed to determine if implementing this model will result in a meaningful reduction in hypoglycaemic events and improve clinical outcomes."
68,"Big data and machine learning to tackle diabetes management Background: Type 2 Diabetes (T2D) diagnosis is based solely on glycaemia, even though it is an endpoint of numerous dysmetabolic pathways. Type 2 Diabetes complexity is challenging in a real-world scenario; thus, dissecting T2D heterogeneity is a priority. Cluster analysis, which identifies natural clusters within multidimensional data based on similarity measures, poses a promising tool to unravel Diabetes complexity.

Methods: In this review, we scrutinize and integrate the results obtained in most of the works up to date on cluster analysis and T2D.

Results: To correctly stratify subjects and to differentiate and individualize a preventive or therapeutic approach to Diabetes management, cluster analysis should be informed with more parameters than the traditional ones, such as etiological factors, pathophysiological mechanisms, other dysmetabolic co-morbidities, and biochemical factors, that is the millieu. Ultimately, the above-mentioned factors may impact on Diabetes and its complications. Lastly, we propose another theoretical model, which we named the Integrative Model. We differentiate three types of components: etiological factors, mechanisms and millieu. Each component encompasses several factors to be projected in separate 2D planes allowing an holistic interpretation of the individual pathology.

Conclusion: Fully profiling the individuals, considering genomic and environmental factors, and exposure time, will allow the drive to precision medicine and prevention of complications. Precision medicine allows tailoring an approach or treatment to different individuals. In other words, a population is stratified into similar groups, considering relevant characteristics to the condition (e.g. T2D). Doing so for each group an appropriate therapeutic approach is defined. Although precision medicine approaches can make use of genetic data, they can also be based on many other types of clinical data. Observed complexity is solved with the help of mathematical algorithms that stratify individuals into groups by similarity.

In the era of omics and digital health, in which we can extract and deal with thousands of features and use them to tailor care to diabetes, it is not prudent to limit cluster analysis to a few already preestablished common mechanisms. Furthermore, these new strategies allow us to deal with blood glucose levels as a continuum, together with the overall milieu, surpassing the artificial glycaemia_based cut_off approach. By fully profiling subjects regarding genomics, environmental factors and time exposition, we will be able to know which mechanism(s) is(are) affected and is(are) responsible for a dysmetabolic condition. This enables the use of drugs in a precise manner and the discovery of new ones. Additionally, the prevention of complications, such as cardiovascular events, may be earlier and more effective. The great big challenge will be identifying which features are relevant to consider precise care and gather the data to perform these analyses. In a global village such as our world, we should gather robust clinical data working in a worldwide consortium."
69,"Use and performance of machine learning models for type 2 diabetes prediction in clinical and community care settings: Protocol for a systematic review and meta-analysis of predictive modeling studies Objective: Machine learning involves the use of algorithms without explicit instructions. Of late, machine learning models have been widely applied for the prediction of type 2 diabetes. However, no evidence synthesis of the performance of these prediction models of type 2 diabetes is available. We aim to identify machine learning prediction models for type 2 diabetes in clinical and community care settings and determine their predictive performance.

Methods: The systematic review of English language machine learning predictive modeling studies in 12 databases will be conducted. Studies predicting type 2 diabetes in predefined clinical or community settings are eligible. Standard CHARMS and TRIPOD guidelines will guide data extraction. Methodological quality will be assessed using a predefined risk of bias assessment tool. The extent of validation will be categorized by Reilly-Evans levels. Primary outcomes include model performance metrics of discrimination ability, calibration, and classification accuracy. Secondary outcomes include candidate predictors, algorithms used, level of validation, and intended use of models. The random-effects meta-analysis of c-indices will be performed to evaluate discrimination abilities. The c-indices will be pooled per prediction model, per model type, and per algorithm. Publication bias will be assessed through funnel plots and regression tests. Sensitivity analysis will be conducted to estimate the effects of study quality and missing data on primary outcome. The sources of heterogeneity will be assessed through meta-regression. Subgroup analyses will be performed for primary outcomes.

Ethics and dissemination: No ethics approval is required, as no primary or personal data are collected. Findings will be disseminated through scientific sessions and peer-reviewed journals. This is the first systematic review assessing the use and predictive performance of ML models for T2DM prediction and capturing a wide scope of prediction models. For example, models at varying levels of validation, from those at development stage to those that have been fully externally validated and updated, developed in and aimed at using in different clinical and community settings as well as both diagnostic and prognostic tools are considered. The findings may be reproducible to a broader context due to the inclusion of a range of studies emanating from both clinical and non-clinical settings. This review will cover a large number of databases as well. The use of only English language articles is a limitation of the review. Poor quality studies and between-study heterogeneity will be of concern, as they may impact on the validity and interpretability findings. Insights into the sources of heterogeneity, however, will be provided by meta-regression and sub-group analyses.

It is anticipated that the findings of this review will be relevant to many stakeholders. First, the review will present a comprehensive overview of features of ML models for T2DM prediction and will highlight any potential gaps in the current literature on this topic. Second, it will produce high-level evidence from peer-reviewed literature on the predictive performance, feasibility, and acceptability of ML prediction models of T2DM. Third, the review could provide information regarding valuable and robust ML models for T2DM prediction, which may guide clinicians on their evidence-based use as a diagnostic tool for the detection of patients with T2DM. Models applicable for community settings may be of use as screening tools. Fourth, the review may be useful for funders to better understand the applications of ML for T2DM prediction, which could assist priority setting in funding allocations. Finally, the findings may support clinicians, researchers, and health policy-makers to design future ML studies to improve T2DM prediction."
70,"Artificial Intelligence Applications in Type 2 Diabetes Mellitus Care: Focus on Machine Learning Methods Objectives: The incidence of type 2 diabetes mellitus has increased significantly in recent years. With the development of artificial intelligence applications in healthcare, they are used for diagnosis, therapeutic decision making, and outcome prediction, especially in type 2 diabetes mellitus. This study aimed to identify the artificial intelligence (AI) applications for type 2 diabetes mellitus care.

Methods: This is a review conducted in 2018. We searched the PubMed, Web of Science, and Embase scientific databases, based on a combination of related mesh terms. The article selection process was based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Finally, 31 articles were selected after inclusion and exclusion criteria were applied. Data gathering was done by using a data extraction form. Data were summarized and reported based on the study objectives.

Results: The main applications of AI for type 2 diabetes mellitus care were screening and diagnosis in different stages. Among all of the reviewed AI methods, machine learning methods with 71% (n = 22) were the most commonly applied techniques. Many applications were in multi method forms (23%). Among the machine learning algorithms applications, support vector machine (21%) and naive Bayesian (19%) were the most commonly used methods. The most important variables that were used in the selected studies were body mass index, fasting blood sugar, blood pressure, HbA1c, triglycerides, low-density lipoprotein, high-density lipoprotein, and demographic variables.

Conclusions: It is recommended to select optimal algorithms by testing various techniques. Support vector machine and naive Bayesian might achieve better performance than other applications due to the type of variables and targets in diabetes-related outcomes classification. According to the results, the main AI applications are related to ML use for knowledge production and model development, which has been widely used in many healthcare applications [9]. It is a key technology to transform biomedical datasets into actionable knowledge that is useful for the advancement of clinical practice and healthcare through rules developed by medical experts, statistical methods, and ML algorithms with the ability of self-improvement [9,30,61]. Actually, ML methods often achieve high accuracy due to looser assumptions regarding data distribution in comparison to other methods [8,10,11,17,30]. The results of this study indicate that main clinical variables in running and designing ML models and systems for type 2 diabetes care, were body mass index (BMI), fasting blood sugar (FBS), blood pressure (systolic and diastolic), HbA1c, triglycerides (TG), low-density lipoprotein (LDL), high-density lipoprotein (HDL), family history, and demographic variables (Table 2). This study supports the evidence from previous reports that have identified some important clinical variables that are used in data mining methods in diabetes research [29].

According to the results of this study, models based on ML algorithms in T2DM care have been mainly focused on pre-diabetes screening and diagnostic outcomes, risk factor analysis, treatment, and complication categorization. That is, ML algorithms have been mainly used to classify diabetic prone cases for pre-diabetes, diabetes, and advanced diabetes based on the patients' HbA1c level. They are used to analyze T2DM risk factors in various populations to determine which categories of patients may require more attention to prevent (1) disease occurrence, (2) progress to worse stages, and (3) advancing to complications [29,51]. This study revealed that there has been limited research on estimating the probability of these outcomes rather than classifying patients based on their disease outcomes in the given three categories. According to previous classification experiments, SVM, which has been widely used for diabetic data analysis has outperformed other algorithms. This may be attributed to its capability to apply hyperlinks to separate classes in a three-dimensional space [61,62]. Also, NB, which has been used in several cases for the purpose of determining diabetes-related outcomes is usually applied for prediction task only for small datasets [29]. Using this algorithm, the probability of specific outcome occurrence, such as pre-diabetes occurrence probability, can be calculated; while by using the algorithm of NB the probability of specific outcome occurrence such as pre-diabetes occurrence can be calculated, there is no report of conducting this work in practice. Generally, ANN, and DT in the frame of C5, CART, and RF have been applied for both classification and prediction purposes [63,64]; however, there has been no report of applying these algorithms for diabetes-related outcome prediction using big data, even with a satisfactory and generalization level.

The performance of AI techniques and its models applied to T2DM care are shown in the eighth and ninth columns of Table 2. Regarding the best performance of ML techniques, it is obvious that there are no particular best techniques for every condition. For example, decision trees that classify cases by sorting them based on feature values show varying performance in different studies. It has been reported that there is a relationship between the performance of applied techniques and the following subjects, including the type of issues analyzed, the type of input data (discrete or continuous), and finally, the emerging overlapping in outcome classes [65,66,67].

In the area of AI application for T2DM care, there have been some scattered observational studies which need more trials to be applied in routine care. For example, a study presented a patient-level sequential modeling approach to implementing personalized prescription. In this approach, previous records of a patient were applied to the prediction of future prescriptions to improve accuracy. The effectiveness of this method was tested by implementing prediction models based on recurrent neural networks (RNN) [68]. Another study performed a literature review of efforts to use artificial intelligence techniques for diabetes management. The results demonstrate that AI methods are not only suitable for use in clinical practice but also self-management of diabetes. Also, these methods have the potential for improving patients' quality of life [24]. The present study focused on ML techniques to predict T2DM outcomes. The frequencies of various types of applications, as well as health aspects of T2DM care, were studied. Other AI methods may provide additional powerful tools to support diabetes care.

Furthermore, an experiment used the Q-learning algorithm in the area of reinforcement learning of AI to develop personalized treatment plans based on glucose level to provide different basal dose levels automatically for the treatment of each diabetes patient. This approach is a model free of reinforcement learning technique, which is used to determine the optimal treatment policy from a patient's treatment history and related laboratory results. The results reveal that this approach is an effective tool for personalized diabetes management [69].

As seen in Table 2, there have been several reports of multiple-method applications in T2DM care. Most of the ML algorithms have been used with FL, KB, and ES to create multiple-method-based systems for users in the area of diabetes control, including patients, physicians/nurses, managers, and policymakers. In the frame of these systems, ML applications for T2DM data analysis are used for knowledge production to enrich the system's knowledge base incrementally. They use diabetes patients' real data collected in a data repository over time to create a rich knowledge base. The knowledge base might also include experts' knowledge. The fuzzy method can either use real data or experts' knowledge to enrich the knowledge base [70].

Similarly, robotics technologies learn from humans and from the environment [71]. Despite many applications of ML in the framework of an indirect ES or direct ES, according to our inclusion criteria, there have been no reports of studies using robotics and NLP technology to support T2DM care that have evaluated model performance at the same time.

There have been reports on other areas of DM research using robotics, such as using a robot for pancreas transplantation, pancreatectomy surgery, and monitoring and training to improve the care of elderly with dementia [72,73,74]. Also, the management of type 1 diabetes in children is improved by robots. Robots can keep track of individuals' performance and can offer tailored lessons to enhance learning [75].

One of the most interesting findings of this study was that AI has often shown success in the prediction of related issues in T2DM. For example, in treatment, a study predicted DPP-IV inhibitors with ML approaches with 87.2% accuracy [31]. In screening and diagnosis, another investigation predicted hypoglycemia using ML models with 97% accuracy [40]. In the diagnosis of complications (nephropathy), a study predicted microalbuminuria using multiple AI methods (ML, FL, ES) with 92% accuracy [45]. In risk factor analysis, another study predicted the risk of type 2 diabetes, hypertension, and comorbidity using ML models with 85% accuracy [58].

The results of this study may support researchers and developers of AI-based systems and models in the care of patients with T2DM in choosing methods, models, algorithms, and efficient and optimal systems. It is suggested that ML, specifically SVM and NB, algorithms are considered by designers and developers of patterns and systems. The present study also identified the most important clinical variables used in the design and development of artificial intelligence systems and models for the care of patients with T2DM. This can provide insights for choosing key variables in T2DM, and in data analysis and system development using AI-based methods.

One of the limitations of this study was the number of databases that were reviewed. In this work, PubMed, Embase, and Web of Science were reviewed. In fact, the focus of this review was clinical databases; therefore, more technical databases, such as IEEE and Scopus should be considered for further reviews. Future investigation should focus on the effect of AI on clinical outcomes and its impact."